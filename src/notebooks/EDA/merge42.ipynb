{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "from trackml.dataset import load_event\n",
    "from trackml.randomize import shuffle_hits\n",
    "from trackml.score import score_event\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import collections as coll\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "import merge as merge\n",
    "import extension as ext\n",
    "import zroutlier as zro\n",
    "import free_hits as free\n",
    "import track_score as score2\n",
    "import straight_tracks as strt\n",
    "import eda_utils as eda\n",
    "import r0outlier as r0o\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../../../input/train_1'\n",
    "event_id = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event000001000 memory usage 18.46 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "event_prefix = 'event00000' + str(event_id)\n",
    "hits, cells, particles, truth = load_event(os.path.join(TRAIN_PATH, event_prefix))\n",
    "\n",
    "mem_bytes = (hits.memory_usage(index=True).sum() \n",
    "             + cells.memory_usage(index=True).sum() \n",
    "             + particles.memory_usage(index=True).sum() \n",
    "             + truth.memory_usage(index=True).sum())\n",
    "print('{} memory usage {:.2f} MB'.format(event_prefix, mem_bytes / 2**20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "helix_root_path = '../../1000_r0_exp3/event_' + str(event_id) + '_labels_train_helix'\n",
    "labels_helix1 = pd.read_csv(helix_root_path + '1.csv').label.values\n",
    "labels_helix2 = pd.read_csv(helix_root_path + '2.csv').label.values\n",
    "labels_helix3 = pd.read_csv(helix_root_path + '3.csv').label.values\n",
    "labels_helix4 = pd.read_csv(helix_root_path + '4.csv').label.values\n",
    "labels_helix5 = pd.read_csv(helix_root_path + '5.csv').label.values\n",
    "labels_helix6 = pd.read_csv(helix_root_path + '6.csv').label.values\n",
    "labels_helix7 = pd.read_csv(helix_root_path + '7.csv').label.values\n",
    "labels_helix8 = pd.read_csv(helix_root_path + '8.csv').label.values\n",
    "labels_helix9 = pd.read_csv(helix_root_path + '9.csv').label.values\n",
    "labels_helix10 = pd.read_csv(helix_root_path + '10.csv').label.values\n",
    "labels_helix11 = pd.read_csv(helix_root_path + '11.csv').label.values\n",
    "labels_helix12 = pd.read_csv(helix_root_path + '12.csv').label.values\n",
    "labels_helix13 = pd.read_csv(helix_root_path + '13.csv').label.values\n",
    "labels_helix14 = pd.read_csv(helix_root_path + '14.csv').label.values\n",
    "labels_helix15 = pd.read_csv(helix_root_path + '15.csv').label.values\n",
    "labels_helix16 = pd.read_csv(helix_root_path + '16.csv').label.values\n",
    "labels_helix17 = pd.read_csv(helix_root_path + '17.csv').label.values\n",
    "labels_helix18 = pd.read_csv(helix_root_path + '18.csv').label.values\n",
    "labels_helix19 = pd.read_csv(helix_root_path + '19.csv').label.values\n",
    "labels_helix20 = pd.read_csv(helix_root_path + '20.csv').label.values\n",
    "labels_helix21 = pd.read_csv(helix_root_path + '21.csv').label.values\n",
    "labels_helix22 = pd.read_csv(helix_root_path + '22.csv').label.values\n",
    "labels_helix23 = pd.read_csv(helix_root_path + '23.csv').label.values\n",
    "labels_helix24 = pd.read_csv(helix_root_path + '24.csv').label.values\n",
    "labels_helix25 = pd.read_csv(helix_root_path + '25.csv').label.values\n",
    "labels_helix26 = pd.read_csv(helix_root_path + '26.csv').label.values\n",
    "labels_helix27 = pd.read_csv(helix_root_path + '27.csv').label.values\n",
    "labels_helix28 = pd.read_csv(helix_root_path + '28.csv').label.values\n",
    "labels_helix29 = pd.read_csv(helix_root_path + '29.csv').label.values\n",
    "labels_helix42 = pd.read_csv(helix_root_path + '42.csv').label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_event_submission(event_id, hits, labels):\n",
    "    sub_data = np.column_stack(([event_id]*len(hits), hits.hit_id.values, labels))\n",
    "    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n",
    "    return submission\n",
    "\n",
    "def score_one_submission(event_id, hits, labels, truth):\n",
    "    submission = create_one_event_submission(event_id, hits, labels)\n",
    "    score = score_event(truth, submission)\n",
    "    print(\"Score for event %d: %.8f\" % (event_id, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block if you want to see the distribution of perfect vs short vs imperfect vs horrible\n",
    "# tracks, by comparing to ground truth\n",
    "helix6 = np.copy(labels_helix9)\n",
    "#helix6 = merge.remove_outliers(helix6, hits, smallest_track_size=6, aggressive=False, print_counts=False)\n",
    "(helix6, small_count) = merge.remove_small_tracks(helix6, smallest_track_size=6)\n",
    "#helix6 = r0o.remove_badr0_tracks(helix6, hits)\n",
    "helix6 = merge.renumber_labels(helix6)\n",
    "tracks = np.unique(helix6)\n",
    "short_tracks = []\n",
    "perfect_tracks = []\n",
    "imperfect_tracks = []\n",
    "horrible_tracks = []\n",
    "for track in tracks:\n",
    "    if track == 0: continue\n",
    "    tix = np.where(helix6 == track)[0]\n",
    "    if len(tix) < 6: continue\n",
    "    else:\n",
    "        (is_match, correct,incorrect) = eda.track_distance_from_truth(track, helix6, hits, truth)\n",
    "        if is_match:\n",
    "            perfect_tracks.append(track)\n",
    "        elif incorrect == 0:\n",
    "            short_tracks.append(track)\n",
    "        elif incorrect <= 4 and correct >= incorrect:\n",
    "            imperfect_tracks.append(track)\n",
    "        else:\n",
    "            horrible_tracks.append(track)\n",
    "\n",
    "print('Total tracks:     ' + str(len(tracks)))\n",
    "print('Perfect tracks:   ' + str(len(perfect_tracks)))\n",
    "print('Short tracks:     ' + str(len(short_tracks)))\n",
    "print('Imperfect tracks: ' + str(len(imperfect_tracks)))\n",
    "print('Horrible tracks:  ' + str(len(horrible_tracks)))\n",
    "# Helix1: 8143, perfect 1972, short 1364, imperfect 3547, horrible 1259\n",
    "# Helix2: 7666, perfect 2020, short 1170, imperfect 3522, horrible 953\n",
    "# Helix3: 6717, perfect 1783, short 1589, imperfect 2696, horrible 648\n",
    "# Helix4: 6688, perfect 1547, short 1649, imperfect 2780, horrible 711\n",
    "# Helix5: 6645, perfect 1600, short 1763, imperfect 2781, horrible 500\n",
    "# Helix6: 7436, perfect 1755, short 1716, imperfect 3361, horrible 603\n",
    "# Helix7: 7451, perfect 1795, short 1795, imperfect 3299, horrible 589\n",
    "# Helix8: 7421, perfect 1734, short 1638, imperfect 3302, horrible 746\n",
    "# Helix9: 7414, perfect 1807, short 1699, imperfect 3276, horrible 631"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_track_quality(labels, hits, truth):\n",
    "    tracks, counts = np.unique(labels, return_counts=True)\n",
    "    short_tracks = 0\n",
    "    perfect_tracks = 0\n",
    "    imperfect_tracks = 0\n",
    "    horrible_tracks = 0\n",
    "    for ix, track in enumerate(tracks):\n",
    "        if track == 0: continue\n",
    "        if counts[ix] < 6: continue\n",
    "\n",
    "        (is_match, correct,incorrect) = eda.track_distance_from_truth(track, labels, hits, truth)\n",
    "        if is_match:\n",
    "            perfect_tracks = perfect_tracks + 1\n",
    "        elif incorrect == 0:\n",
    "            short_tracks = short_tracks + 1\n",
    "        elif incorrect <= 4 and correct >= incorrect:\n",
    "            imperfect_tracks = imperfect_tracks + 1\n",
    "        else:\n",
    "            horrible_tracks = horrible_tracks + 1\n",
    "\n",
    "    print('Total tracks:     ' + str(len(tracks)))\n",
    "    print('Perfect tracks:   ' + str(perfect_tracks))\n",
    "    print('Short tracks:     ' + str(short_tracks))\n",
    "    print('Imperfect tracks: ' + str(imperfect_tracks))\n",
    "    print('Horrible tracks:  ' + str(horrible_tracks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.copy(labels_helix6)\n",
    "score_one_submission(event_id, hits, labels, truth) # 0.63580502\n",
    "(strong, medium, weak) = r0o.split_tracks_based_on_quality(labels, hits)\n",
    "score_one_submission(event_id, hits, strong, truth)\n",
    "score_one_submission(event_id, hits, medium, truth)\n",
    "score_one_submission(event_id, hits, weak, truth)\n",
    "# (0.5 weak, 0.1 strong): 0.28793004 (1776/3737), 0.31400817 (1582/5075), 0.03201160 (28/646)\n",
    "# (0.5 weak, 0.15 strong): 0.35599925 (2247/4484), 0.24593896 (1111/4328), 0.03201160 (28/646)\n",
    "# (0.5 weak, 0.2 strong): 0.40582813 (2604/5035), 0.19611008 (754/3777), 0.03201160 (28/646)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_track_quality(strong, hits, truth)\n",
    "display_track_quality(medium, hits, truth)\n",
    "display_track_quality(weak, hits, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_helix1 = merge.remove_outliers(labels_helix1, hits, cells, aggressive=True, print_counts=False)\n",
    "labels_helix2 = merge.remove_outliers(labels_helix2, hits, cells, aggressive=True, print_counts=False)\n",
    "#labels_helix3 = merge.remove_outliers(labels_helix3, hits, cells, aggressive=True, print_counts=False)\n",
    "labels_helix4 = merge.remove_outliers(labels_helix4, hits, cells, aggressive=True, print_counts=False)\n",
    "labels_helix5 = merge.remove_outliers(labels_helix5, hits, cells, aggressive=True, print_counts=False)\n",
    "labels_helix1 = r0o.remove_badr0_tracks(labels_helix1, hits)\n",
    "labels_helix2 = r0o.remove_badr0_tracks(labels_helix2, hits)\n",
    "#labels_helix3 = r0o.remove_badr0_tracks(labels_helix3, hits)\n",
    "labels_helix4 = r0o.remove_badr0_tracks(labels_helix4, hits)\n",
    "labels_helix5 = r0o.remove_badr0_tracks(labels_helix5, hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = []\n",
    "all_labels.append(labels_helix1)\n",
    "all_labels.append(labels_helix2)\n",
    "all_labels.append(labels_helix6)\n",
    "all_labels.append(labels_helix7)\n",
    "all_labels.append(labels_helix8)\n",
    "all_labels.append(labels_helix9)\n",
    "all_labels.append(labels_helix10)\n",
    "all_labels.append(labels_helix11)\n",
    "all_labels.append(labels_helix12)\n",
    "all_labels.append(labels_helix13)\n",
    "all_labels.append(labels_helix14)\n",
    "#all_labels.append(labels_helix15) #\n",
    "all_labels.append(labels_helix16)\n",
    "#all_labels.append(labels_helix17) #\n",
    "#all_labels.append(labels_helix18) #\n",
    "#all_labels.append(labels_helix19) #\n",
    "all_labels.append(labels_helix20)\n",
    "#all_labels.append(labels_helix21) # 21 helps slightly, 22 hurts\n",
    "all_labels.append(labels_helix23) # 23 helped 0.001, but combined with 26 had only slight impr. over 26 alone\n",
    "all_labels.append(labels_helix26) # 26 helped almost 0.002\n",
    "all_labels.append(labels_helix42)\n",
    "all_labels.append(labels_helix5)\n",
    "#all_labels.append(labels_helix1) # merging with 1 again at the end helps\n",
    "#all_labels.append(labels_helix3)\n",
    "all_labels.append(labels_helix4)\n",
    "strong_labels = []\n",
    "medium_labels = []\n",
    "weak_labels = []\n",
    "for label in all_labels:\n",
    "    (strong, medium, weak) = r0o.split_tracks_based_on_quality(label, hits)\n",
    "    strong_labels.append(strong)\n",
    "    medium_labels.append(medium)\n",
    "    weak_labels.append(weak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_labels(all_labels, hits, truth):\n",
    "    merge_count = 0\n",
    "    labels_merged = np.copy(all_labels[0])\n",
    "    for i in range(len(all_labels)):\n",
    "        if i == 0: continue\n",
    "        labels_merged = merge.heuristic_merge_tracks(labels_merged, all_labels[i], hits, overwrite_limit=6, print_summary=False)\n",
    "        merge_count = merge_count + 1\n",
    "        #message = 'Merged loop 1-' + str(i+1) + ' score for event '\n",
    "        #display_score(event_id, hits, labels_merged, truth, message)\n",
    "        score_one_submission(event_id, hits, labels_merged, truth)\n",
    "    return labels_merged\n",
    "\n",
    "def merge_all_strong_labels(all_labels, hits, truth):\n",
    "    merge_count = 0\n",
    "    labels_merged = np.copy(all_labels[0])\n",
    "    for i in range(len(all_labels)):\n",
    "        if i == 0: continue\n",
    "        labels_merged = merge.heuristic_merge_tracks(labels_merged, all_labels[i], hits, overwrite_limit=6, print_summary=False)\n",
    "        merge_count = merge_count + 1\n",
    "        # Periodically remove small tracks/noise to help merge performance.\n",
    "        # If we're only dealing with unmatched hits from a previous round, don't filter though,\n",
    "        # since we have relatively few tracks already, and removing even small tracks hurts.\n",
    "        if merge_count % 4 == 0:\n",
    "            (labels_merged, _) = merge.remove_small_tracks(labels_merged, smallest_track_size=3)\n",
    "        #message = 'Merged loop 1-' + str(i+1) + ' score for event '\n",
    "        #display_score(event_id, hits, labels_merged, truth, message)\n",
    "        score_one_submission(event_id, hits, labels_merged, truth)\n",
    "    return labels_merged\n",
    "\n",
    "def merge_all_medium_labels(all_labels, hits, truth):\n",
    "    merge_count = 0\n",
    "    labels_merged = np.copy(all_labels[0])\n",
    "    for i in range(len(all_labels)):\n",
    "        if i == 0: continue\n",
    "        labels_merged = merge.heuristic_merge_tracks(labels_merged, all_labels[i], hits, overwrite_limit=6, weak_tracks=True, print_summary=False)\n",
    "        merge_count = merge_count + 1\n",
    "        # Periodically remove small tracks/noise to help merge performance.\n",
    "        # If we're only dealing with unmatched hits from a previous round, don't filter though,\n",
    "        # since we have relatively few tracks already, and removing even small tracks hurts.\n",
    "        if merge_count % 4 == 0:\n",
    "            (labels_merged, _) = merge.remove_small_tracks(labels_merged, smallest_track_size=3)\n",
    "        #message = 'Merged loop 1-' + str(i+1) + ' score for event '\n",
    "        #display_score(event_id, hits, labels_merged, truth, message)\n",
    "        score_one_submission(event_id, hits, labels_merged, truth)\n",
    "    return labels_merged\n",
    "\n",
    "def merge_all_weak_labels(all_labels, hits, truth):\n",
    "    merge_count = 0\n",
    "    labels_merged = np.copy(all_labels[0])\n",
    "    for i in range(len(all_labels)):\n",
    "        if i == 0: continue\n",
    "        labels_merged = merge.heuristic_merge_tracks(labels_merged, all_labels[i], hits, overwrite_limit=3, weak_tracks=True, print_summary=False)\n",
    "        merge_count = merge_count + 1\n",
    "        # Periodically remove small tracks/noise to help merge performance.\n",
    "        # If we're only dealing with unmatched hits from a previous round, don't filter though,\n",
    "        # since we have relatively few tracks already, and removing even small tracks hurts.\n",
    "        if merge_count % 4 == 0:\n",
    "            (labels_merged, _) = merge.remove_small_tracks(labels_merged, smallest_track_size=3)\n",
    "        #message = 'Merged loop 1-' + str(i+1) + ' score for event '\n",
    "        #display_score(event_id, hits, labels_merged, truth, message)\n",
    "        score_one_submission(event_id, hits, labels_merged, truth)\n",
    "    return labels_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_merged = merge_all_labels(all_labels, hits, truth)\n",
    "# No outlier removal, order 1-9: 0.70948133\n",
    "# outlier removal, 1-2,6-9,5,3,4: 0.71563907\n",
    "# + r0 outlier rem: 0.71652579\n",
    "# + more models(10-20):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for event 1000: 0.51293564\n",
      "Score for event 1000: 0.55977605\n",
      "Score for event 1000: 0.57273489\n",
      "Score for event 1000: 0.58420738\n",
      "Score for event 1000: 0.58765860\n",
      "Score for event 1000: 0.59718572\n",
      "Score for event 1000: 0.60141234\n",
      "Score for event 1000: 0.60528105\n",
      "Score for event 1000: 0.60770404\n",
      "Score for event 1000: 0.61064412\n",
      "Score for event 1000: 0.61160560\n",
      "Score for event 1000: 0.61303118\n",
      "Score for event 1000: 0.61403313\n",
      "Score for event 1000: 0.61691591\n",
      "Score for event 1000: 0.61985516\n",
      "Score for event 1000: 0.62612279\n",
      "Score for event 1000: 0.62895606\n"
     ]
    }
   ],
   "source": [
    "strong_merged = merge_all_strong_labels(strong_labels, hits, truth)\n",
    "# 0.1 cutoff: 0.47028638\n",
    "# 0.15 cutoff: 0.54866460\n",
    "# 0.2 cutoff: 0.59883933\n",
    "# outlier rem, 0.2 cutoff: 0.60684391\n",
    "# +r0 out. rem: 0.60669428\n",
    "# + models 10-20: 0.62883099\n",
    "# NEW: non-aggressive outlier, models 1-20: 0.62464468\n",
    "# aggr. removal: 0.62883099\n",
    "# over. limit 8: 0.62338910\n",
    "# over. limit 3: 0.62573651\n",
    "# lim 6, periodically clean <3 tracks: 0.62919876\n",
    "# ++ remove 17,19: 0.62821628\n",
    "# ++ remove 17,18,19,3,4: 0.62257023\n",
    "# ordered 1-20: 0.63031451\n",
    "# ordered 1-20, rem 3,4: 0.62557659\n",
    "# --> with 3-trk cleanse: 0.62573526\n",
    "# --> with 4-trk cleanse: 0.62589822\n",
    "# --> with 5-trk cleanse: 0.62569532\n",
    "# ordered 1-20, rem 3,4,15: 0.62405548\n",
    "# remove 15,17,18,19,3,4: 0.62262955\n",
    "# --> weak: 0.61891542"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#track_extension_limits = [0.02, 0.04, 0.06, 0.08, 0.10]\n",
    "#sm2 = np.copy(strong_merged)\n",
    "#sm2 = ext.do_all_track_extensions(sm2, hits, track_extension_limits, num_neighbours=15, use_scoring=True)\n",
    "strong_merged = strt.extend_straight_tracks(strong_merged, hits)\n",
    "score_one_submission(event_id, hits, strong_merged, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for event 1000: 0.22744578\n",
      "Score for event 1000: 0.28685333\n",
      "Score for event 1000: 0.31105647\n",
      "Score for event 1000: 0.33316696\n",
      "Score for event 1000: 0.34087531\n",
      "Score for event 1000: 0.38542046\n",
      "Score for event 1000: 0.40943704\n",
      "Score for event 1000: 0.41715433\n",
      "Score for event 1000: 0.42547329\n",
      "Score for event 1000: 0.43005594\n",
      "Score for event 1000: 0.43347721\n",
      "Score for event 1000: 0.43586714\n",
      "Score for event 1000: 0.43778796\n",
      "Score for event 1000: 0.44390561\n",
      "Score for event 1000: 0.44616844\n",
      "Score for event 1000: 0.45111056\n",
      "Score for event 1000: 0.45512096\n"
     ]
    }
   ],
   "source": [
    "#m2_lbl = medium_labels[0:13]\n",
    "#m2_lbl.append(medium_labels[-3])\n",
    "#m2_lbl.append(medium_labels[-2])\n",
    "#m2_lbl.append(medium_labels[-1])\n",
    "#medium_merged = merge_all_labels(m2_lbl, hits, truth)\n",
    "medium_merged = merge_all_medium_labels(medium_labels, hits, truth)\n",
    "# 0.1 cutoff: 0.50709876\n",
    "# 0.15 cutoff: 0.43838218\n",
    "# 0.2 cutoff: 0.38796599\n",
    "# outlier rem, 0.2 cutoff: 0.36414178\n",
    "# +r0 out. rem: 0.36216201\n",
    "# + models 10-20: 0.45574288\n",
    "# + models 1,2,6,7,8,9,10,5,3,4: 0.42359094\n",
    "# + models 1,2,6-15,5,3,4: 0.44986680\n",
    "# + models 1-20: 0.45574288\n",
    "# overwrite limit 4: 0.45255829\n",
    "# overwrite limit 8: 0.45429087\n",
    "# overwrite limit 6: 0.45574288\n",
    "# lim 6, periodically clean <3 tracks: 0.45601010\n",
    "# ++ remove 17,19: 0.44087093\n",
    "# ++ remove 17,18,19,3,4: 0.44612346\n",
    "# ordered: 0.46101420\n",
    "# weak: 0.45405540\n",
    "# ordered 1-20, rem 3,4: 0.45618824\n",
    "# --> weak: 0.45612816\n",
    "# ordered 1-20, rem 3,4,5, weak: 0.45192894\n",
    "# remove 15,17,18,19,3,4: 0.44360005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#track_extension_limits = [0.02, 0.04, 0.06, 0.08, 0.10]\n",
    "#sm2 = np.copy(medium_merged)\n",
    "medium_merged = strt.extend_straight_tracks(medium_merged, hits)\n",
    "score_one_submission(event_id, hits, medium_merged, truth)\n",
    "#medium_merged = ext.do_all_track_extensions(medium_merged, hits, track_extension_limits, num_neighbours=15, use_scoring=True)\n",
    "# 0.44986680->0.45397096\n",
    "#score_one_submission(event_id, hits, sm2, truth)\n",
    "#sm2 = strt.extend_straight_tracks(sm2, hits)\n",
    "# 0.45397096->0.45556476\n",
    "#score_one_submission(event_id, hits, sm2, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for event 1000: 0.04506415\n",
      "Score for event 1000: 0.06575363\n",
      "Score for event 1000: 0.07006864\n",
      "Score for event 1000: 0.07853070\n",
      "Score for event 1000: 0.08255572\n",
      "Score for event 1000: 0.09500363\n",
      "Score for event 1000: 0.10233660\n",
      "Score for event 1000: 0.10574703\n",
      "Score for event 1000: 0.10763563\n",
      "Score for event 1000: 0.10868704\n",
      "Score for event 1000: 0.10904090\n",
      "Score for event 1000: 0.11027754\n",
      "Score for event 1000: 0.11112603\n",
      "Score for event 1000: 0.11526440\n",
      "Score for event 1000: 0.11686153\n",
      "Score for event 1000: 0.11891198\n",
      "Score for event 1000: 0.11970010\n"
     ]
    }
   ],
   "source": [
    "#w2_lbl = weak_labels[0:8]\n",
    "#w2_lbl.append(weak_labels[-3])\n",
    "#w2_lbl.append(weak_labels[-2])\n",
    "#w2_lbl.append(weak_labels[-1])\n",
    "#weak_merged = merge_all_labels(w2_lbl, hits, truth)\n",
    "weak_merged = merge_all_weak_labels(weak_labels, hits, truth)\n",
    "# 0.5 cutoff: 0.09220529\n",
    "# 0.5 cutoff + outlier rem: 0.08541034\n",
    "# + r0 out. rem: 0.08497520\n",
    "# + models 10-20: 0.11515280\n",
    "# + models 1,2,6,7,8,9,10,5,3,4: 0.10249580\n",
    "# + models 1-20: 0.11515280\n",
    "# overwrite limit 2: 0.11508325\n",
    "# overwrite limit 8: 0.11279941\n",
    "# overwrite limit 3, weak: 0.11976585\n",
    "# lim 3, weak, periodically clean <3 tracks: 0.11968905\n",
    "# ++ remove 17,19: 0.11787688\n",
    "# ++ remove 17,18,19,3,4: 0.11552246\n",
    "# ordered: 0.11851233\n",
    "# weak: 0.12264231\n",
    "# ordered 1-20, rem 3,4: 0.12094372\n",
    "# ordered 1-20, rem 3,4,5, weak: 0.11752672\n",
    "# remove 15,17,18,19,3,4: 0.11401350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for event 1000: 0.73651231\n"
     ]
    }
   ],
   "source": [
    "#labels_merged = merge.heuristic_merge_tracks(strong_merged, medium_merged, hits, overwrite_limit=3, print_summary=False)\n",
    "#mm1 = np.copy(medium_merged)\n",
    "#mm1 = merge.remove_outliers(mm1, hits, cells, aggressive=False, print_counts=False)\n",
    "#score_one_submission(event_id, hits, mm1, truth)\n",
    "\n",
    "#labels_merged = merge.heuristic_merge_tracks(strong_merged, mm1, hits, weak_tracks=True, overwrite_limit=3)\n",
    "labels_merged = merge.heuristic_merge_tracks(strong_merged, medium_merged, hits, weak_tracks=True, overwrite_limit=3)\n",
    "score_one_submission(event_id, hits, labels_merged, truth)\n",
    "# strong=0.1 cutoff: 0.7186 (overwrite=3)\n",
    "# strong=0.15 cutoff: 0.7192 (overwrite=3)\n",
    "# strong=0.2 cutoff: 0.7196\n",
    "# outlier rem., 0.2 cutoff: 0.7246\n",
    "# + r0 out. rem: 0.7260\n",
    "# + models 10-20: 0.73364939\n",
    "# only 10 medium models: 0.73336881\n",
    "# only 15 medium models: 0.73148404\n",
    "# 15 med, 10 weak: 0.73329531\n",
    "# last: 0.73367862 ((2) - 0.73377616)\n",
    "# models 1-20: 0.73361873\n",
    "# chg overwrite: 0.73196386\n",
    "# overwrite limit 8: 0.72996687\n",
    "# strong 3, med 6, weak 3: 0.73041769\n",
    "# ++ remove 17,19: 0.73443782\n",
    "# --> + med. out. rem: 0.73407980\n",
    "# ++ remove 17,18,19,3,4: 0.73393060\n",
    "# --> + med. out. rem: 0.73396006\n",
    "# ordered: 0.73258941\n",
    "# --> out rem: 0.73207804\n",
    "# weak: 0.73214341\n",
    "# --> out rem: 0.73189557\n",
    "# ordered 1-20, rem 3,4: 0.73250120\n",
    "# --> medium-weak: 0.73288691\n",
    "# --> strong-cleanse: 0.73237699\n",
    "# --> strong-cleanse5: 0.73258272\n",
    "# --> strong-cleanse4: 0.73267125\n",
    "# ordered 1-20, rem 3,4,15: 0.73255419\n",
    "# remove 15,17,18,19,3,4: 0.73534231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for event 1000: 0.11790143\n",
      "Score for event 1000: 0.74107931\n"
     ]
    }
   ],
   "source": [
    "#labels_merged2 = merge.heuristic_merge_tracks(labels_merged, weak_merged, hits, overwrite_limit=1, print_summary=False)\n",
    "\n",
    "wm1 = np.copy(weak_merged)\n",
    "wm1 = merge.remove_outliers(wm1, hits, cells, aggressive=True, print_counts=False)\n",
    "score_one_submission(event_id, hits, wm1, truth)\n",
    "\n",
    "labels_merged2 = merge.heuristic_merge_tracks(labels_merged, wm1, hits, weak_tracks=True, overwrite_limit=1)\n",
    "#labels_merged2 = merge.heuristic_merge_tracks(labels_merged, weak_merged, hits, weak_tracks=True, overwrite_limit=1)\n",
    "score_one_submission(event_id, hits, labels_merged2, truth)\n",
    "# strong=0.1 cutoff: 0.7228 (overwrite=1)\n",
    "# strong=0.15 cutoff: 0.7237 (overwrite=1)\n",
    "# strong=0.2 cutoff: 0.7241 (overwrite=1)\n",
    "# outlier rem. strong=0.2: 0.7293\n",
    "# +r0 out. rem: 0.7311\n",
    "# +models 10-14: 0.735?\n",
    "# + models 10-20: 0.73648855\n",
    "# only 10 weak models: 0.73707780\n",
    "# only 15 medium models: 0.73500946\n",
    "# 15 med, 10 weak: 0.73671680\n",
    "# last: 0.73678249\n",
    "# new weak merge: 0.73742467, 0.73759009 (4), 0.73786259 (3)\n",
    "# models 1-20: 0.73793692\n",
    "# chg overwrite: 0.73704419\n",
    "# overwrite limit 8: 0.73361786\n",
    "# str 3, med 6, weak 3: 0.73516573\n",
    "# ++ remove 17,19: 0.73852283\n",
    "# --> + weak aggr. out. rem: 0.73800277\n",
    "# ++ remove 17,18,19,3,4: 0.73831033\n",
    "# --> + med. out. rem: 0.73851074\n",
    "# ordered: 0.73730571\n",
    "# --> + out. rem: 0.73672076\n",
    "# weak: 0.73665424\n",
    "# --> out rem: 0.73640193\n",
    "# ordered 1-20, rem 3,4: 0.73745883\n",
    "# --> out rem: 0.73743842\n",
    "# --> med-weak: 0.73756486\n",
    "# --> strong-cleanse: 0.73713546\n",
    "# --> strong-cleanse5: 0.73726067\n",
    "# --> strong-cleanse4: 0.73710238\n",
    "# ordered 1-20, rem 3,4,15: 0.73693037"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for event 1000: 0.74183549\n",
      "Score for event 1000: 0.74334149\n"
     ]
    }
   ],
   "source": [
    "labels = strt.extend_straight_tracks(labels_merged2, hits)\n",
    "score_one_submission(event_id, hits, labels, truth)\n",
    "labels = free.assign_free_hits(labels, hits)\n",
    "score_one_submission(event_id, hits, labels, truth)\n",
    "# scores: 0.73736782, 0.73874326\n",
    "# only 15 medium models: 0.73845154\n",
    "# 15 med, 10 weak: 0.73724001, 0.73845154\n",
    "# last: 0.73673186, 0.73799254\n",
    "# new weak merge: 0.73737404, 0.73856768\n",
    "# models 1-20: 0.73847834, 0.73981423\n",
    "# chg overwrite: 0.73763503, 0.73873021\n",
    "# overwrite limit 8: 0.73436074, 0.73554492\n",
    "# str 3, med 6, weak 3: 0.73581388, 0.73718725\n",
    "# ++ remove 17,19: 0.73914233, 0.74031220\n",
    "# --> + med/weak out rem: 0.73861231, 0.73985150\n",
    "# ++ remove 17,18,19,3,4: 0.73915103, 0.74047960\n",
    "# --> + med. out. rem: 0.73934148, 0.74065658\n",
    "# end score: 0.74030116\n",
    "# ordered: 0.73786483, 0.73927505\n",
    "# --> + out. rem.: 0.73730187, 0.73858468\n",
    "# weak: 0.73770705, 0.73915284\n",
    "# --> out rem: 0.73752564, 0.73913394\n",
    "# ordered 1-20, rem 3,4: 0.73809852, 0.73946622\n",
    "# --> out rem: 0.73810009, 0.73950646\n",
    "# --> med-weak: 0.73840189, 0.73963008\n",
    "# --> strong-cleanse: 0.73797404, 0.73912490\n",
    "# --> strong-cleanse5: 0.73809770, 0.73925513\n",
    "# --> strong-cleanse4: 0.73793941, 0.73908986\n",
    "# ordered 1-20, rem 3,4,15: 0.73780211, 0.73895136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_helix6f = remove_outliers2(labels_helix6, hits, cells, print_counts=True)\n",
    "score_one_submission(event_id, hits, labels_helix6, truth)\n",
    "score_one_submission(event_id, hits, labels_helix6f, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_merge_weak_tracks(labels1, labels2, hits, overwrite_limit=4, print_summary=False):\n",
    "    \"\"\" Merge tracks from two arrays of track labels.\n",
    "\n",
    "    Merges are handled as follows:\n",
    "     - tracks from labels2 are identified and searched\n",
    "     - for each track from labels2:\n",
    "       - use track directly if no conflict with any tracks from labels1\n",
    "       - skip if labels1 already contains the same track of equal (or longer) length\n",
    "       - otherwise, if there are potentially multiple conflicting tracks from labels1\n",
    "         - if labels1 only contains a single track ID, as well as un-classified (0) hits,\n",
    "           re-assign '0' track ID to labels1 track ID (i.e. lengthen the track)\n",
    "         - otherwise, labels1 contains multiple non-zero track IDs\n",
    "           - replace any track ID 0 occurrences with the longest labels1 track ID\n",
    "           - replace any occurrences of short (len <= 3) labels1 tracks with the longest labels1 track ID\n",
    "\n",
    "    Parameters:\n",
    "     - labels1: np array of labels, each entry represents a hit, the value represents the\n",
    "       track ID that hit is assigned to. This should be considered the 'higher-quality' of\n",
    "       the two input labels\n",
    "     - labels2: np array of secondary labels, whose tracks should be merged into labels1\n",
    "\n",
    "    Returns: The merged array of labeled tracks.\n",
    "    \"\"\"\n",
    "    labels_merged = np.copy(labels1)\n",
    "    labels_merged = merge.renumber_labels(labels_merged)\n",
    "    max_track = np.amax(labels_merged)\n",
    "    labels2[labels2 != 0] = labels2[labels2 != 0] + max_track\n",
    "    trks2 = np.unique(labels2)\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count3 = 0\n",
    "    count4 = 0\n",
    "    count4_len = []\n",
    "    count5 = 0\n",
    "    count6 = 0\n",
    "    count7 = 0\n",
    "    count8 = 0\n",
    "    count9 = 0\n",
    "    count10 = 0\n",
    "    count11 = 0\n",
    "    count12 = 0\n",
    "    count13 = 0\n",
    "    count14 = 0\n",
    "    count15 = 0\n",
    "    count16 = 0\n",
    "    for trk2 in trks2:\n",
    "        if trk2 == 0:\n",
    "            continue\n",
    "        trk2_ix = np.where(labels2 == trk2)[0]\n",
    "        trk2_length = len(trk2_ix)\n",
    "        if trk2_length < 2:\n",
    "            continue\n",
    "        trk1_val = labels_merged[trk2_ix]\n",
    "        #print('trk2: ' + str(trk2) + ', label1: ' + str(trk1_val))\n",
    "        trk1_uniq = np.unique(trk1_val)\n",
    "        # Now we know which tracks from the 1st label overlap with the tracks from the 2nd label\n",
    "        if len(trk1_uniq) == 1:\n",
    "            if trk1_uniq[0] == 0:\n",
    "                #print('Good candidate to replace!')\n",
    "                # This track was not found by labels1, just directly use the\n",
    "                # track from labels2.\n",
    "                count1 = count1 + 1\n",
    "                labels_merged[trk2_ix] = trk2\n",
    "            else:\n",
    "                # We found a track that is at least as long as the current\n",
    "                # track in labels1. Nothing more needed, at least for now.\n",
    "                # We could consider scenarios where the labels1 track contains\n",
    "                # hits from 2 different tracks, where labels2 only has a\n",
    "                # shorter single track. In this case, it may be good to split\n",
    "                # the labels1 track into two pieces. However, this condition\n",
    "                # would be very hard to detect, for now we want to favour\n",
    "                # longer tracks whenever possible.\n",
    "                #print('Same track found, skipping...')\n",
    "                count2 = count2 + 1\n",
    "        else:\n",
    "            found_tracks = 0\n",
    "            # Get counts for all identified tracks from labels1 that match trk2\n",
    "            trk1_counts = coll.Counter(trk1_val).most_common(len(trk1_uniq))\n",
    "            longest_track_id = trk1_counts[0][0]\n",
    "            longest_track_count = trk1_counts[0][1]\n",
    "            second_track_id = trk1_counts[1][0]\n",
    "            second_track_count = trk1_counts[1][1]\n",
    "            # If longest track in labels1 was 0, create a new track, but only\n",
    "            # from free hits, or from small tracks. Also, if there is not\n",
    "            # enough overlap (less than half the hits overlap), also create\n",
    "            # a new track.\n",
    "            if longest_track_id == 0:\n",
    "                count5 = count5 + 1\n",
    "                longest_track_id = trk2\n",
    "                # See if we should instead lengthen the longest non-zero track\n",
    "                if len(trk1_uniq) == 2:\n",
    "                    test_track_ix = np.where(labels_merged == second_track_id)[0]\n",
    "                    if len(test_track_ix) <= (second_track_count + 3):\n",
    "                        outliers1 = zro.find_track_outliers_zr(second_track_id, labels_merged, hits, find_all=True)\n",
    "                        labelx = np.copy(labels_merged)\n",
    "                        labelx[trk2_ix] = second_track_id\n",
    "                        outliers2 = zro.find_track_outliers_zr(second_track_id, labelx, hits, find_all=True)\n",
    "                        if len(outliers2) <= len(outliers1):\n",
    "                            longest_track_id = second_track_id\n",
    "                            longest_track_count = second_track_count\n",
    "            elif (trk2_length > 20) or (longest_track_count > 20):\n",
    "                count9 = count9 + 1\n",
    "                longest_track_id = trk2\n",
    "            elif (trk2_length > 6) and (longest_track_count < int(trk2_length/2)) and second_track_id != 0:\n",
    "                # Try to avoid creating crossed tracks, do not lengthen existing track if not\n",
    "                # enough overlap.\n",
    "                count10 = count10 + 1\n",
    "                trk1a = np.where(labels_merged == longest_track_id)[0]\n",
    "                if longest_track_count + 3 >= len(trk1a):\n",
    "                    #print('Top 2 tracks, new: ' + str(trk2_length) + ', len1: '  + str(longest_track_count) + ', len1a: ' + str(len(trk1a)) + ', len2: ' + str(second_track_count) + ', len2a: ' + str(len(trk2a)))\n",
    "                    # Lengthen the longest track, it's fully contained by our new/proposed track.\n",
    "                    # Reset 2nd longest track if mostly contained in new/proposed track.\n",
    "                    count14 = count14 + 1\n",
    "                    trk2a = np.where(labels_merged == second_track_id)[0]\n",
    "                    if second_track_count + 1 >= len(trk2a):\n",
    "                        count15 = count15 + 1\n",
    "                        labels_merged[trk2a] = longest_track_id\n",
    "                else:\n",
    "                    # Not much overlap, start a new track to avoid hurting existing tracks.\n",
    "                    longest_track_id = trk2\n",
    "            else:\n",
    "                # If the old track had too many hits not part of the new/proposed track, do\n",
    "                # not lengthen it - that may lose majority. Better to start a new track.\n",
    "                trk1a = np.where(labels_merged == longest_track_id)[0]\n",
    "                if longest_track_count + 3 < len(trk1a): # change from 6\n",
    "                    count16 = count16 + 1\n",
    "                    longest_track_id = trk2\n",
    "\n",
    "            for trk1 in trk1_uniq:\n",
    "                if trk1 == 0:\n",
    "                    continue\n",
    "                trk1_ix = np.where(labels_merged == trk1)[0]\n",
    "                if len(trk1_ix) > 1:\n",
    "                    found_tracks = found_tracks + 1\n",
    "            if found_tracks > 1:\n",
    "                #print('Found ' + str(found_tracks) + ' non-trivial tracks.')\n",
    "                count3 = count3 + 1\n",
    "                # If there are un-classified hits, assign those to the track\n",
    "                # ID with the most hits.\n",
    "                for label_ix in trk2_ix:\n",
    "                    if labels_merged[label_ix] == 0:\n",
    "                        labels_merged[label_ix] = longest_track_id\n",
    "                        count6 = count6 + 1\n",
    "\n",
    "                # If there are tracks of length 2 or less, and one or both\n",
    "                # of those hits are included in the target track, re-assign\n",
    "                # those matching the labels2 track to the most common\n",
    "                # original track ID.\n",
    "                for trk1_count in trk1_counts:\n",
    "                    trk1_count_ix = np.where(labels_merged == trk1_count[0])[0]\n",
    "                    if len(trk1_count_ix) <= overwrite_limit:\n",
    "                        outliers = zro.find_track_outliers_zr(trk2, labels2, hits, find_all=True)\n",
    "                        for label_ix in trk2_ix:\n",
    "                            if labels_merged[label_ix] == trk1_count[0] and label_ix in outliers:\n",
    "                                count13 = count13 + 1\n",
    "                            elif labels_merged[label_ix] == trk1_count[0]:# and label_ix not in outliers:\n",
    "                                labels_merged[label_ix] = longest_track_id\n",
    "                                count7 = count7 + 1\n",
    "                    #else:\n",
    "                    #    outliers = zro.find_track_outliers_zr(trk1_count[0], labels_merged, hits, find_all=True)\n",
    "                    #    for label_ix in trk2_ix:\n",
    "                    #        if labels_merged[label_ix] == trk1_count[0] and label_ix in outliers:\n",
    "                    #            labels_merged[label_ix] = longest_track_id\n",
    "                    #            count12 = count12 + 1\n",
    "\n",
    "            else:\n",
    "                # Only the track ID, as well as track ID 0, were found in labels1.\n",
    "                # Replace any occurrences of ID 0 with the labels1 track ID.\n",
    "                count4 = count4 + 1\n",
    "                count4_len.append(len(trk2_ix))\n",
    "                # If there are un-classified hits, assign those to the track\n",
    "                # ID with the most hits (lengthens the track).\n",
    "                for label_ix in trk2_ix:\n",
    "                    if labels_merged[label_ix] == 0:\n",
    "                        labels_merged[label_ix] = longest_track_id\n",
    "                        count8 = count8 + 1\n",
    "\n",
    "    if print_summary:\n",
    "        print('Simple replacement of unclassified hits: ' + str(count1))\n",
    "        print('Similar tracks (no-op): ' + str(count2))\n",
    "        print('New track creations from little overlap(0): ' + str(count5))\n",
    "        print('New track creations from huge tracks(>20): ' + str(count9))\n",
    "        print('Test for new track creations from little overlap(non-0): ' + str(count10))\n",
    "        print('--> Lengthen longest overlap instead: ' + str(count14))\n",
    "        print('  --> And clear 2nd longest track: ' + str(count15))\n",
    "        print('Skip extension due to too little overlap: ' + str(count16))\n",
    "        print('Multiple non-trivial tracks: ' + str(count3))\n",
    "        print('--> of which partial track ID 0 hits were updated: ' + str(count6))\n",
    "        print('--> of which partial track ID non-0 hits were updated: ' + str(count7))\n",
    "        print('--> of which partial track ID non-0 hits were skipped: ' + str(count13))\n",
    "        print('--> of which outliers were overwritten: ' + str(count12))\n",
    "        print('Tracks to be lengthened: ' + str(count4))\n",
    "        print('--> of which track ID 0 hits were updated: ' + str(count8))\n",
    "        print('--> from which new tracks were created instead: ' + str(count11))\n",
    "        noises = np.unique(np.asarray(count4_len))\n",
    "        print('--> of which labels2 unique track lengths were: ' + str(noises))\n",
    "\n",
    "    return labels_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_track_outliers2(track, labels, hits, cells, aggressive):\n",
    "    labels = np.copy(labels)\n",
    "    found_bad_volume = 0\n",
    "    found_bad_cell = 0\n",
    "    found_bad_dimension = 0\n",
    "    found_bad_slope = 0\n",
    "    found_bad_z = 0\n",
    "    found_bad_zr = 0\n",
    "\n",
    "    if True:\n",
    "        outlier_zr = zro.find_track_outliers_zr(track, labels, hits)\n",
    "        if len(outlier_zr) > 0:\n",
    "            #print('track ' + str(track) + ' zr outliers: ' + str(outlier_zr))\n",
    "            found_bad_zr = found_bad_zr + len(outlier_zr)\n",
    "            for oix in outlier_zr:\n",
    "                labels[oix] = 0\n",
    "\n",
    "    if True:\n",
    "        # Check if the sorted hits (on z-axis) go through the volumes\n",
    "        # and layers in the expected order\n",
    "        duplicatez_ix = merge.find_duplicate_z_using_zr(track, labels, hits)\n",
    "        if len(duplicatez_ix) > 0:\n",
    "            #print('track ' + str(track) + ' duplicate z: ' + str(duplicatez_ix))\n",
    "            found_bad_z = found_bad_z + len(duplicatez_ix)\n",
    "            for bzix in duplicatez_ix:\n",
    "                labels[bzix] = 0\n",
    "\n",
    "    if False:#True:\n",
    "        # Check the helix slope, discard hits that do not match\n",
    "        outlier_slope_ix = merge.remove_track_outliers_slope(track, labels, hits)\n",
    "        if len(outlier_slope_ix) > 0:\n",
    "            #print('track ' + str(track) + ' slope outliers: ' + str(outlier_slope_ix))\n",
    "            found_bad_slope = found_bad_slope + len(outlier_slope_ix)\n",
    "            for oix in outlier_slope_ix:\n",
    "                labels[oix] = 0\n",
    "\n",
    "    return (labels, found_bad_volume, found_bad_dimension, found_bad_z, found_bad_slope, found_bad_zr, found_bad_cell)\n",
    "\n",
    "\n",
    "def remove_outliers2(labels, hits, cells, smallest_track_size=2, aggressive=False, print_counts=True):\n",
    "    tracks = np.unique(labels)\n",
    "    hits['z_abs'] = hits.z.abs()\n",
    "    hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "    hits['a0'] = np.arctan2(hits.y,hits.x)\n",
    "    hits['zr'] = hits['z'] / hits['r']\n",
    "    count_rem_volume = 0\n",
    "    count_rem_dimension = 0\n",
    "    count_duplicatez = 0\n",
    "    count_rem_slope = 0\n",
    "    count_small_tracks = 0\n",
    "    count_zr = 0\n",
    "    count_cell = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 3:\n",
    "            (labels, c1, c2, c3, c4, c5, c6) = remove_track_outliers2(track, labels, hits, cells, aggressive)\n",
    "            count_rem_volume = count_rem_volume + c1\n",
    "            count_rem_dimension = count_rem_dimension + c2\n",
    "            count_duplicatez = count_duplicatez + c3\n",
    "            count_rem_slope = count_rem_slope + c4\n",
    "            count_zr = count_zr + c5\n",
    "            count_cell = count_cell + c6\n",
    "\n",
    "    # Remove small tracks, we do not get any score for those. This is done\n",
    "    # last, in case removing the outliers (above) removed enough hits\n",
    "    # from a track to make them smaller than the threshold.\n",
    "    (labels, count_small_tracks) = merge.remove_small_tracks(labels, smallest_track_size=smallest_track_size)\n",
    "\n",
    "    if print_counts:\n",
    "        print('Total removed due to bad cells: ' + str(count_cell))\n",
    "        print('Total removed due to bad volumes: ' + str(count_rem_volume))\n",
    "        print('Total removed due to bad zr values: ' + str(count_zr))\n",
    "        print('Total removed due to bad dimensions: ' + str(count_rem_dimension))\n",
    "        print('Total removed due to duplicate zs: ' + str(count_duplicatez))\n",
    "        print('Total removed due to bad slopes: ' + str(count_rem_slope))\n",
    "        print('Total removed small tracks (<' + str(smallest_track_size) + ') hits: ' + str(count_small_tracks))\n",
    "\n",
    "    return labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
