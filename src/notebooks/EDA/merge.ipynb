{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from trackml.dataset import load_event\n",
    "from trackml.randomize import shuffle_hits\n",
    "from trackml.score import score_event\n",
    "\n",
    "import collections as coll\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "import merge as merge\n",
    "import extension as ext\n",
    "import track_score as scor\n",
    "import zroutlier as zro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../../../input/train_1'\n",
    "event_id = 1029"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = '../../'\n",
    "#event_1003_labels_train_helix1_phase1_dbscan1_premerge.csv\n",
    "CSV_FILENAME_PREFIX = 'event_' + str(event_id) + '_labels_train_'\n",
    "#CSV_HELIX1 = CSV_PATH + 'first_2_event_1003_labels_train_helix1_shift_5_6_m6_m9.csv'\n",
    "#CSV_HELIX2 = CSV_PATH + 'second_event_1003_labels_train_helix1.csv'\n",
    "#CSV_HELIX3 = CSV_PATH + 'third_2_event_1003_labels_train_helix4.csv'\n",
    "#CSV_HELIX4 = CSV_PATH + 'fourth_event_1003_labels_train_helix2.csv'\n",
    "CSV_FILENAME_PREFIX = 'event_' + str(event_id) + '_labels_train_helix1_phase1_dbscan'\n",
    "CSV_HELIX1 = CSV_PATH + CSV_FILENAME_PREFIX + '1_processed.csv'\n",
    "CSV_HELIX2 = CSV_PATH + CSV_FILENAME_PREFIX + '2_processed.csv'\n",
    "CSV_HELIX3 = CSV_PATH + CSV_FILENAME_PREFIX + '3_processed.csv'\n",
    "CSV_HELIX4 = CSV_PATH + CSV_FILENAME_PREFIX + '4_processed.csv'\n",
    "CSV_HELIX5 = CSV_PATH + CSV_FILENAME_PREFIX + '5_processed.csv'\n",
    "CSV_HELIX6 = CSV_PATH + CSV_FILENAME_PREFIX + '6_processed.csv'\n",
    "CSV_HELIX7 = CSV_PATH + CSV_FILENAME_PREFIX + '7_processed.csv'\n",
    "CSV_HELIX8 = CSV_PATH + CSV_FILENAME_PREFIX + '8_processed.csv'\n",
    "CSV_HELIX9 = CSV_PATH + CSV_FILENAME_PREFIX + '9_processed.csv'\n",
    "CSV_HELIX10 = CSV_PATH + CSV_FILENAME_PREFIX + '10_processed.csv'\n",
    "CSV_HELIX11 = CSV_PATH + CSV_FILENAME_PREFIX + '11_processed.csv'\n",
    "CSV_HELIX12 = CSV_PATH + CSV_FILENAME_PREFIX + '12_processed.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = '../../best_csvs12/'\n",
    "CSV_PATH2 = '../../'\n",
    "#event_1003_labels_train_helix1_phase1_dbscan1_premerge.csv\n",
    "CSV_FILENAME_PREFIX = 'event_' + str(event_id) + '_labels_train_helix1_phase2_dbscan'\n",
    "CSV_HELIX1 = CSV_PATH + CSV_FILENAME_PREFIX + '1_premerge.csv'\n",
    "CSV_HELIX2 = CSV_PATH + CSV_FILENAME_PREFIX + '2_premerge.csv'\n",
    "CSV_HELIX3 = CSV_PATH + CSV_FILENAME_PREFIX + '3_premerge.csv'\n",
    "CSV_HELIX4 = CSV_PATH + CSV_FILENAME_PREFIX + '4_premerge.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event000001029 memory usage 16.61 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "event_prefix = 'event00000' + str(event_id)\n",
    "hits, cells, particles, truth = load_event(os.path.join(TRAIN_PATH, event_prefix))\n",
    "\n",
    "mem_bytes = (hits.memory_usage(index=True).sum() \n",
    "             + cells.memory_usage(index=True).sum() \n",
    "             + particles.memory_usage(index=True).sum() \n",
    "             + truth.memory_usage(index=True).sum())\n",
    "print('{} memory usage {:.2f} MB'.format(event_prefix, mem_bytes / 2**20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_hough = pd.read_csv(CSV_HOUGH).label.values\n",
    "labels_helix1 = pd.read_csv(CSV_HELIX1).label.values\n",
    "labels_helix2 = pd.read_csv(CSV_HELIX2).label.values\n",
    "labels_helix3 = pd.read_csv(CSV_HELIX3).label.values\n",
    "labels_helix4 = pd.read_csv(CSV_HELIX4).label.values\n",
    "labels_helix5 = pd.read_csv(CSV_HELIX5).label.values\n",
    "labels_helix6 = pd.read_csv(CSV_HELIX6).label.values\n",
    "labels_helix7 = pd.read_csv(CSV_HELIX7).label.values\n",
    "labels_helix8 = pd.read_csv(CSV_HELIX8).label.values\n",
    "labels_helix9 = pd.read_csv(CSV_HELIX9).label.values\n",
    "labels_helix10 = pd.read_csv(CSV_HELIX10).label.values\n",
    "labels_helix11 = pd.read_csv(CSV_HELIX11).label.values\n",
    "labels_helix12 = pd.read_csv(CSV_HELIX12).label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_event_submission(event_id, hits, labels):\n",
    "    sub_data = np.column_stack(([event_id]*len(hits), hits.hit_id.values, labels))\n",
    "    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_score(message, event_id, labels, hits):\n",
    "    submission = create_one_event_submission(event_id, hits, labels)\n",
    "    score = score_event(truth, submission)\n",
    "    print(message + ': ' + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_hough_filter = merge.remove_outliers(labels_hough, hits, print_counts=True)\n",
    "labels_helix1_filter = merge.remove_outliers(labels_helix1, hits, print_counts=True)\n",
    "labels_helix2_filter = merge.remove_outliers(labels_helix2, hits, print_counts=True)\n",
    "#labels_helix3_filter = merge.remove_outliers(labels_helix3, hits, print_counts=True)\n",
    "#labels_helix4_filter = merge.remove_outliers(labels_helix4, hits, print_counts=True)\n",
    "#labels_helix5_filter = merge.remove_outliers(labels_helix5, hits, print_counts=True)\n",
    "#labels_helix6_filter = merge.remove_outliers(labels_helix6, hits, print_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "hits['zr'] = hits['z'] / hits['r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple replacement of unclassified hits: 209\n",
      "Similar tracks (no-op): 4010\n",
      "New track creations from little overlap(0): 432\n",
      "New track creations from huge tracks(>20): 8\n",
      "Test for new track creations from little overlap(non-0): 16\n",
      "--> Lengthen longest overlap instead: 10\n",
      "  --> And clear 2nd longest track: 4\n",
      "Skip extension due to too little overlap: 111\n",
      "Multiple non-trivial tracks: 1199\n",
      "--> of which partial track ID 0 hits were updated: 1601\n",
      "--> of which partial track ID non-0 hits were updated: 0\n",
      "--> of which partial track ID non-0 hits were skipped: 0\n",
      "--> of which outliers were overwritten: 145\n",
      "Tracks to be lengthened: 1343\n",
      "--> of which track ID 0 hits were updated: 3199\n",
      "--> from which new tracks were created instead: 0\n",
      "--> of which labels2 unique track lengths were: [ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 25]\n",
      "Merged1 score: 0.644889382557\n",
      "Merged2 score: 0.651335948918\n",
      "Merged3 score: 0.658322110111\n",
      "Merged4 score: 0.661137585052\n",
      "Merged5 score: 0.66315586132\n",
      "Merged6 score: 0.675132424244\n",
      "Merged7 score: 0.675729488958\n",
      "Merged8 score: 0.676632712216\n",
      "Merged9 score: 0.678548944175\n",
      "Merged10 score: 0.679002641814\n",
      "Merged11 score: 0.681056401359\n"
     ]
    }
   ],
   "source": [
    "labels_merged = heuristic_merge_tracks2(labels_helix1, labels_helix2, overwrite_limit=4, print_summary=True)\n",
    "display_score('Merged1 score', event_id, labels_merged, hits)\n",
    "#(labels_merged, _) = merge.remove_small_tracks(labels_merged, smallest_track_size=2)\n",
    "labels_merged = heuristic_merge_tracks2(labels_merged, labels_helix3, overwrite_limit=4, print_summary=False)\n",
    "display_score('Merged2 score', event_id, labels_merged, hits)\n",
    "#(labels_merged, _) = merge.remove_small_tracks(labels_merged, smallest_track_size=2)\n",
    "labels_merged = heuristic_merge_tracks2(labels_merged, labels_helix4, overwrite_limit=4, print_summary=False)\n",
    "display_score('Merged3 score', event_id, labels_merged, hits)\n",
    "(labels_merged, _) = merge.remove_small_tracks(labels_merged, smallest_track_size=3)\n",
    "labels_merged = heuristic_merge_tracks2(labels_merged, labels_helix5, overwrite_limit=4, print_summary=False)\n",
    "display_score('Merged4 score', event_id, labels_merged, hits)\n",
    "#(labels_merged, _) = merge.remove_small_tracks(labels_merged, smallest_track_size=2)\n",
    "labels_merged = heuristic_merge_tracks2(labels_merged, labels_helix6, overwrite_limit=4, print_summary=False)\n",
    "display_score('Merged5 score', event_id, labels_merged, hits)\n",
    "#(labels_merged, _) = merge.remove_small_tracks(labels_merged, smallest_track_size=3)\n",
    "#(labels_merged, _) = merge.remove_small_tracks(labels_merged, smallest_track_size=2)\n",
    "labels_merged = heuristic_merge_tracks2(labels_merged, labels_helix7, overwrite_limit=4, print_summary=False)\n",
    "display_score('Merged6 score', event_id, labels_merged, hits)\n",
    "(labels_merged, _) = merge.remove_small_tracks(labels_merged, smallest_track_size=3)\n",
    "labels_merged = heuristic_merge_tracks2(labels_merged, labels_helix8, overwrite_limit=4, print_summary=False)\n",
    "display_score('Merged7 score', event_id, labels_merged, hits)\n",
    "#(labels_merged, _) = merge.remove_small_tracks(labels_merged, smallest_track_size=2)\n",
    "labels_merged = heuristic_merge_tracks2(labels_merged, labels_helix9, overwrite_limit=4, print_summary=False)\n",
    "display_score('Merged8 score', event_id, labels_merged, hits)\n",
    "#(labels_merged, _) = merge.remove_small_tracks(labels_merged, smallest_track_size=2)\n",
    "labels_merged = heuristic_merge_tracks2(labels_merged, labels_helix10, overwrite_limit=4, print_summary=False)\n",
    "display_score('Merged9 score', event_id, labels_merged, hits)\n",
    "(labels_merged, _) = merge.remove_small_tracks(labels_merged, smallest_track_size=3)\n",
    "labels_merged = heuristic_merge_tracks2(labels_merged, labels_helix11, overwrite_limit=4, print_summary=False)\n",
    "display_score('Merged10 score', event_id, labels_merged, hits)\n",
    "#(labels_merged, _) = merge.remove_small_tracks(labels_merged, smallest_track_size=3)\n",
    "#(labels_merged, _) = merge.remove_small_tracks(labels_merged, smallest_track_size=2)\n",
    "labels_merged = heuristic_merge_tracks2(labels_merged, labels_helix12, overwrite_limit=4, print_summary=False)\n",
    "display_score('Merged11 score', event_id, labels_merged, hits)\n",
    "#(labels_merged, _) = merge.remove_small_tracks(labels_merged, smallest_track_size=2)\n",
    "# Default: 0.64340, 0.64919, 0.65539, 0.65766, 0.65902, 0.66876, 0.66927, 0.66985, 0.67088, 0.67057, 0.67092\n",
    "# overw_3: 0.64341, 0.64908, 0.65535, 0.65735, 0.65830, 0.66793, 0.66850, 0.66918, 0.67038, 0.67022, 0.67051\n",
    "# longest: 0.63425, 0.63635, 0.63783, 0.63932, 0.63884, 0.65100, 0.64945, 0.64728, 0.65008, 0.64839, 0.65360\n",
    "# longst2: 0.64025, 0.64245, 0.64172, 0.64383, 0.64109, 0.65684, 0.65461, 0.65163, 0.65430, 0.65111, 0.65831\n",
    "# nosplit: 0.64298, 0.64868, 0.65475, 0.65720, 0.65837, 0.66846, 0.66895, 0.66982, 0.67101, 0.67064, 0.67167\n",
    "# nosplt2: 0.64293, 0.64866, 0.65473, 0.65725, 0.65841, 0.66851, 0.66901, 0.66982, 0.67101, 0.67064, 0.67173\n",
    "# scor1:   0.64083, 0.64277, 0.64301, 0.64526, 0.64166, 0.65782, 0.65691, 0.64359, 0.65646, 0.65365, 0.66034\n",
    "# scor2:   BAD, 0.64800 final score\n",
    "# Mknew:   0.64279, 0.64853, 0.65459, 0.65711, 0.65828, 0.66837, 0.66887, 0.66968, 0.67087, 0.67050, 0.67159\n",
    "# mkovw:   ---> end is 0.67079\n",
    "# mkovw+2: 0.64303, 0.64877, 0.65483, 0.65735, 0.65852, 0.66861, 0.66911, 0.66992, 0.67111, 0.67074, 0.67183\n",
    "# current: 0.64293, 0.64866, 0.65473, 0.65725, 0.65841, 0.66851, 0.66901, 0.66982, 0.67101, 0.67064, 0.67173\n",
    "# ovw_out: 0.64329, 0.64943, 0.65561, 0.65847, 0.65970, 0.67055, 0.67106, 0.67192, 0.67311, 0.67310, 0.67468\n",
    "# outlr2:  0.64329, 0.64943, 0.65561, 0.65847, 0.65970, 0.67051, 0.67102, 0.67188, 0.67303, 0.67303, 0.67461\n",
    "# --> look at next: new track creations from little overlap(0) (most frequent), non-0 (next), >20 (least frequent)\n",
    "# +0trkeq: 0.64302, 0.64916, 0.65538, 0.65834, 0.65956, 0.67059, 0.67110, 0.67199, 0.67314, 0.67315, 0.67472\n",
    "# +0trk+1: 0.64293, 0.64914, 0.65530, 0.65838, 0.65961, 0.67057, 0.67105, 0.67194, 0.67315, 0.67316, 0.67473\n",
    "# +0trk+2: 0.64286, 0.64908, 0.65522, 0.65837, 0.65953, 0.67059, 0.67107, 0.67196, 0.67317, 0.67318, 0.67475\n",
    "# +0trk+3: 0.64279, 0.64905, 0.65517, 0.65846, 0.65967, 0.67076, 0.67128, 0.67217, 0.67311, 0.67312, 0.67456\n",
    "# +0trk++5: 0.64254, 0.64880, 0.65486, 0.65801, 0.65924, 0.67045, 0.67097, 0.67198, 0.67321, 0.67322, 0.67466\n",
    "# +0trk++3: 0.64293, 0.64914, 0.65525, 0.65831, 0.65953, 0.67053, 0.67101, 0.67190, 0.67311, 0.67312, 0.67469\n",
    "# +0trk++2: 0.64293, 0.64914, 0.65530, 0.65838, 0.65961, 0.67057, 0.67105, 0.67194, 0.67315, 0.67316, 0.67473\n",
    "# +0trk++: 0.64302, 0.64916, 0.65538, 0.65834, 0.65956, 0.67059, 0.67110, 0.67199, 0.67314, 0.67315, 0.67472\n",
    "# ->+rem:  0.64302, 0.64916, 0.65538, 0.65834, 0.65956, 0.67059, 0.67110, 0.67199, 0.67314, 0.67315, 0.67472\n",
    "# ->+rem2: 0.64302, 0.64916, 0.65538, 0.65814, 0.65936, 0.67035, 0.67086, 0.67205, 0.67320, 0.67323, 0.67495\n",
    "# ->+rem2i: 0.64302, 0.64916, 0.65538, 0.65816, 0.65939, 0.67044, 0.67095, 0.67202, 0.67318, 0.67319, 0.67492\n",
    "# ->+rmhlf: 0.64302, 0.64916, 0.65538, 0.65834, 0.65956, 0.67062, 0.67113, 0.67202, 0.67318, 0.67319, 0.67492\n",
    "# ->fix0:   0.64378, 0.64992, 0.65610, 0.65897, 0.66019, 0.67109, 0.67160, 0.67245, 0.67362, 0.67363, 0.67535\n",
    "# ->fix0+1: 0.64410, 0.65024, 0.65643, 0.65929, 0.66052, 0.67143, 0.67190, 0.67276, 0.67393, 0.67394, 0.67567\n",
    "# ->fix0+2: 0.64437, 0.65057, 0.65676, 0.65962, 0.66080, 0.67170, 0.67217, 0.67304, 0.67420, 0.67422, 0.67590\n",
    "### ->fix0+3: 0.64429, 0.65050, 0.65669, 0.65962, 0.66080, 0.67179, 0.67226, 0.67312, 0.67434, 0.67435, 0.67597\n",
    "# ->fix0+4: 0.64421, 0.65032, 0.65652, 0.65942, 0.66053, 0.67144, 0.67192, 0.67278, 0.67395, 0.67384, 0.67553\n",
    "# +other0:  0.64436, 0.65058, 0.65672, 0.65977, 0.66124, 0.67180, 0.67235, 0.67307, 0.67436, 0.67433, 0.67589\n",
    "# +other1:  0.64354, 0.64974, 0..... 0.67569\n",
    "# +other2:  0.64318, 0.64942, 0.65548, 0.65869, 0.65994, 0.67096, 0.67150, 0.67233, 0.67350, 0.67354, 0.67535\n",
    "# +other3:  0.64300, 0.64926, 0.65536, 0.65897, 0.66003, 0.67087, 0.67142, 0.67223, 0.67339, 0.67342, 0.67536\n",
    "# +othrunl: 0.64237, 0.64872, 0.65521, 0.65909, ..., 0.67542\n",
    "# rem_dbl:  0.64486, 0.65110, 0.65737, 0.66036, 0.66181, 0.67233, 0.67280, 0.67360, 0.67475, 0.67474, 0.67588\n",
    "# dblnon0:  0.64466, 0.65086, 0.65708, 0.65992, 0.66134, 0.67209, 0.67264, 0.67333, 0.67463, 0.67460, 0.67614\n",
    "# 3waym1:   0.64466, 0.65091, 0.65717, 0.66001, 0.66145, 0.67220, 0.67276, 0.67344, 0.67474, 0.67471, 0.67623\n",
    "# 3waym2:   0.64481, 0.65106, 0.65739, 0.66023, 0.66167, 0.67239, 0.67294, 0.67363, 0.67493, 0.67490, 0.67642\n",
    "# 3waym2+1:   0.64487, 0.65113, 0.65746, 0.66036, 0.66186, 0.67266, 0.67315, 0.67384, 0.67508, 0.67505, 0.67649\n",
    "# 3waym2+2:   0.64487, 0.65113, 0.65746, 0.66036, 0.66186, 0.67298, 0.67320, 0.67390, 0.67513, 0.67511, 0.67659\n",
    "# 3waym2+3:   0.64503, 0.65129, 0.65764, 0.66055, 0.66204, 0.67308, 0.67331, 0.67401, 0.67529, 0.67526, 0.67680\n",
    "# 3waym2+4:   0.64509, 0.65126, 0.65760, 0.66055, 0.66204, 0.67285, 0.67308, 0.67377, 0.67499, 0.67497, 0.67652\n",
    "# 3waym5+1: 0.64505, 0.65135, 0.65770, 0.66079, 0.66225, 0.67329, 0.67358, 0.67421, 0.67555, 0.67553, 0.67715\n",
    "# 3waym5+2: 0.64505, 0.65135, 0.65771, 0.66069, 0.66192, 0.67296, 0.67325, 0.67382, 0.67516, 0.67513, 0.67678\n",
    "# 3waym5+3: 0.64458, 0.65091, 0.65726, 0.66025, 0.66147, 0.67246, 0.67275, 0.67331, 0.67466, 0.67458, 0.67623\n",
    "# 3waym5a1: 0.65605, 0.65136, 0.65771, 0.66073, 0.66222, 0.67327, 0.67352, 0.67415, 0.67549, 0.67546, 0.67700\n",
    "# 3waym5a2: 0.64505, 0.65136, 0.65777, 0.66075, 0.66201, 0.67305, 0.67330, 0.67388, 0.67521, 0.67519, 0.67675\n",
    "# 3waym5a3: 0.64478, 0.65092, 0.65733, 0.66031, 0.66157, 0.67256, 0.67281, 0.67339, 0.67472, 0.67460, 0.67617\n",
    "# 3waym510: 0.64505, 0.65137, 0.65772, 0.66080, 0.66226, 0.67330, 0.67359, 0.67422, 0.67555, 0.67552, 0.67714\n",
    "# 3waym511: 0.64512, 0.65140, 0.65776, 0.66084, 0.66230, 0.67334, 0.67363, 0.67426, 0.67559, 0.67556, 0.67719\n",
    "# 3wayx511: 0.64517, 0.65145, 0.65775, 0.66082, 0.66230, 0.67334, 0.67354, 0.67417, 0.67550, 0.67552, 0.67715\n",
    "# 3waym512: 0.64512, 0.65141, 0.65776, 0.66084, 0.66230, 0.67334, 0.67363, 0.67426, 0.67559, 0.67556, 0.67719\n",
    "# 3waym514: 0.64394, 0.65010, 0.65763, 0.66075, 0.66255, 0.67420, 0.67501, 0.67577, 0.67774, 0.67812, 0.68027\n",
    "# 3waym515: 0.64456, 0.65085, 0.65789, 0.66111, 0.66281, 0.67473, 0.67556, 0.67636, 0.67816, 0.67859, 0.68064\n",
    "### 3waym516: 0.64489, 0.65136, 0.65832, 0.66134, 0.66327, 0.67514, 0.67592, 0.67657, 0.67850, 0.67886, 0.68098\n",
    "# 3waym517: 0.64498, 0.65120, 0.65820, 0.66123, 0.66289, 0.67466, 0.67543, 0.67634, 0.67812, 0.67806, 0.68030\n",
    "## 3waym516, rem 2-track hits after each 3 merges, final score: 0.68106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_merge_tracks2(labels1, labels2, overwrite_limit=4, print_summary=False):\n",
    "    return heuristic_merge_tracks3(labels1, labels2, hits, overwrite_limit=overwrite_limit, print_summary=print_summary)\n",
    "\n",
    "def heuristic_merge_tracks3(labels1, labels2, hits, overwrite_limit=4, print_summary=True):\n",
    "    \"\"\" Merge tracks from two arrays of track labels.\n",
    "\n",
    "    Merges are handled as follows:\n",
    "     - tracks from labels2 are identified and searched\n",
    "     - for each track from labels2:\n",
    "       - use track directly if no conflict with any tracks from labels1\n",
    "       - skip if labels1 already contains the same track of equal (or longer) length\n",
    "       - otherwise, if there are potentially multiple conflicting tracks from labels1\n",
    "         - if labels1 only contains a single track ID, as well as un-classified (0) hits,\n",
    "           re-assign '0' track ID to labels1 track ID (i.e. lengthen the track)\n",
    "         - otherwise, labels1 contains multiple non-zero track IDs\n",
    "           - replace any track ID 0 occurrences with the longest labels1 track ID\n",
    "           - replace any occurrences of short (len <= 3) labels1 tracks with the longest labels1 track ID\n",
    "\n",
    "    Parameters:\n",
    "     - labels1: np array of labels, each entry represents a hit, the value represents the\n",
    "       track ID that hit is assigned to. This should be considered the 'higher-quality' of\n",
    "       the two input labels\n",
    "     - labels2: np array of secondary labels, whose tracks should be merged into labels1\n",
    "\n",
    "    Returns: The merged array of labeled tracks.\n",
    "    \"\"\"\n",
    "    labels_merged = np.copy(labels1)\n",
    "    labels_merged = merge.renumber_labels(labels_merged)\n",
    "    max_track = np.amax(labels_merged)\n",
    "    labels2[labels2 != 0] = labels2[labels2 != 0] + max_track\n",
    "    trks2 = np.unique(labels2)\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count3 = 0\n",
    "    count4 = 0\n",
    "    count4_len = []\n",
    "    count5 = 0\n",
    "    count6 = 0\n",
    "    count7 = 0\n",
    "    count8 = 0\n",
    "    count9 = 0\n",
    "    count10 = 0\n",
    "    count11 = 0\n",
    "    count12 = 0\n",
    "    count13 = 0\n",
    "    count14 = 0\n",
    "    count15 = 0\n",
    "    count16 = 0\n",
    "    for trk2 in trks2:\n",
    "        if trk2 == 0:\n",
    "            continue\n",
    "        trk2_ix = np.where(labels2 == trk2)[0]\n",
    "        trk2_length = len(trk2_ix)\n",
    "        if trk2_length < 2:\n",
    "            continue\n",
    "        trk1_val = labels_merged[trk2_ix]\n",
    "        #print('trk2: ' + str(trk2) + ', label1: ' + str(trk1_val))\n",
    "        trk1_uniq = np.unique(trk1_val)\n",
    "        # Now we know which tracks from the 1st label overlap with the tracks from the 2nd label\n",
    "        if len(trk1_uniq) == 1:\n",
    "            if trk1_uniq[0] == 0:\n",
    "                #print('Good candidate to replace!')\n",
    "                # This track was not found by labels1, just directly use the\n",
    "                # track from labels2.\n",
    "                count1 = count1 + 1\n",
    "                labels_merged[trk2_ix] = trk2\n",
    "            else:\n",
    "                # We found a track that is at least as long as the current\n",
    "                # track in labels1. Nothing more needed, at least for now.\n",
    "                # We could consider scenarios where the labels1 track contains\n",
    "                # hits from 2 different tracks, where labels2 only has a\n",
    "                # shorter single track. In this case, it may be good to split\n",
    "                # the labels1 track into two pieces. However, this condition\n",
    "                # would be very hard to detect, for now we want to favour\n",
    "                # longer tracks whenever possible.\n",
    "                #print('Same track found, skipping...')\n",
    "                count2 = count2 + 1\n",
    "        else:\n",
    "            found_tracks = 0\n",
    "            # Get counts for all identified tracks from labels1 that match trk2\n",
    "            trk1_counts = coll.Counter(trk1_val).most_common(len(trk1_uniq))\n",
    "            longest_track_id = trk1_counts[0][0]\n",
    "            longest_track_count = trk1_counts[0][1]\n",
    "            second_track_id = trk1_counts[1][0]\n",
    "            second_track_count = trk1_counts[1][1]\n",
    "            # If longest track in labels1 was 0, create a new track, but only\n",
    "            # from free hits, or from small tracks. Also, if there is not\n",
    "            # enough overlap (less than half the hits overlap), also create\n",
    "            # a new track.\n",
    "            if longest_track_id == 0:\n",
    "                count5 = count5 + 1\n",
    "                longest_track_id = trk2\n",
    "                # See if we should instead lengthen the longest non-zero track\n",
    "                if len(trk1_uniq) == 2:\n",
    "                    test_track_ix = np.where(labels_merged == second_track_id)[0]\n",
    "                    if len(test_track_ix) <= (second_track_count + 3):\n",
    "                        outliers1 = zro.find_track_outliers_zr(second_track_id, labels_merged, hits, find_all=True)\n",
    "                        labelx = np.copy(labels_merged)\n",
    "                        labelx[trk2_ix] = second_track_id\n",
    "                        outliers2 = zro.find_track_outliers_zr(second_track_id, labelx, hits, find_all=True)\n",
    "                        if len(outliers2) <= len(outliers1):\n",
    "                            longest_track_id = second_track_id\n",
    "                            longest_track_count = second_track_count\n",
    "            elif (trk2_length > 20) or (longest_track_count > 20):\n",
    "                count9 = count9 + 1\n",
    "                longest_track_id = trk2\n",
    "            elif (trk2_length > 6) and (longest_track_count < int(trk2_length/2)) and second_track_id != 0:\n",
    "                # Try to avoid creating crossed tracks, do not lengthen existing track if not\n",
    "                # enough overlap.\n",
    "                count10 = count10 + 1\n",
    "                trk1a = np.where(labels_merged == longest_track_id)[0]\n",
    "                if longest_track_count + 3 >= len(trk1a):\n",
    "                    #print('LIAM: Top 2 tracks, new: ' + str(trk2_length) + ', len1: '  + str(longest_track_count) + ', len1a: ' + str(len(trk1a)) + ', len2: ' + str(second_track_count) + ', len2a: ' + str(len(trk2a)))\n",
    "                    # Lengthen the longest track, it's fully contained by our new/proposed track.\n",
    "                    # Reset 2nd longest track if mostly contained in new/proposed track.\n",
    "                    count14 = count14 + 1\n",
    "                    trk2a = np.where(labels_merged == second_track_id)[0]\n",
    "                    if second_track_count + 1 >= len(trk2a):\n",
    "                        count15 = count15 + 1\n",
    "                        labels_merged[trk2a] = longest_track_id\n",
    "                else:\n",
    "                    # Not much overlap, start a new track to avoid hurting existing tracks.\n",
    "                    longest_track_id = trk2\n",
    "            else:\n",
    "                # If the old track had too many hits not part of the new/proposed track, do\n",
    "                # not lengthen it - that may lose majority. Better to start a new track.\n",
    "                trk1a = np.where(labels_merged == longest_track_id)[0]\n",
    "                if longest_track_count + 6 < len(trk1a):\n",
    "                    count16 = count16 + 1\n",
    "                    longest_track_id = trk2\n",
    "\n",
    "            for trk1 in trk1_uniq:\n",
    "                if trk1 == 0:\n",
    "                    continue\n",
    "                trk1_ix = np.where(labels_merged == trk1)[0]\n",
    "                if len(trk1_ix) > 1:\n",
    "                    found_tracks = found_tracks + 1\n",
    "            if found_tracks > 1:\n",
    "                #print('Found ' + str(found_tracks) + ' non-trivial tracks.')\n",
    "                count3 = count3 + 1\n",
    "                # If there are un-classified hits, assign those to the track\n",
    "                # ID with the most hits.\n",
    "                for label_ix in trk2_ix:\n",
    "                    if labels_merged[label_ix] == 0:\n",
    "                        labels_merged[label_ix] = longest_track_id\n",
    "                        count6 = count6 + 1\n",
    "\n",
    "                # If there are tracks of length 2 or less, and one or both\n",
    "                # of those hits are included in the target track, re-assign\n",
    "                # those matching the labels2 track to the most common\n",
    "                # original track ID.\n",
    "                for trk1_count in trk1_counts:\n",
    "                    #if trk1_count[1] <= overwrite_limit:\n",
    "                    trk1_count_ix = np.where(labels_merged == trk1_count[0])[0]\n",
    "                    if len(trk1_count_ix) <= overwrite_limit:\n",
    "                        outliers = zro.find_track_outliers_zr(trk2, labels2, hits, find_all=True)\n",
    "                        for label_ix in trk2_ix:\n",
    "                            if labels_merged[label_ix] == trk1_count[0] and label_ix in outliers:\n",
    "                                count13 = count13 + 1\n",
    "                            elif labels_merged[label_ix] == trk1_count[0]:# and label_ix not in outliers:\n",
    "                                labels_merged[label_ix] = longest_track_id\n",
    "                                count7 = count7 + 1\n",
    "                    else:\n",
    "                        outliers = zro.find_track_outliers_zr(trk1_count[0], labels_merged, hits, find_all=True)\n",
    "                        for label_ix in trk2_ix:\n",
    "                            if labels_merged[label_ix] == trk1_count[0] and label_ix in outliers:\n",
    "                                labels_merged[label_ix] = longest_track_id\n",
    "                                count12 = count12 + 1\n",
    "\n",
    "            else:\n",
    "                # Only the track ID, as well as track ID 0, were found in labels1.\n",
    "                # Replace any occurrences of ID 0 with the labels1 track ID.\n",
    "                count4 = count4 + 1\n",
    "                count4_len.append(len(trk2_ix))\n",
    "                # If there are un-classified hits, assign those to the track\n",
    "                # ID with the most hits (lengthens the track).\n",
    "                for label_ix in trk2_ix:\n",
    "                    if labels_merged[label_ix] == 0:\n",
    "                        labels_merged[label_ix] = longest_track_id\n",
    "                        count8 = count8 + 1\n",
    "\n",
    "    if print_summary:\n",
    "        print('Simple replacement of unclassified hits: ' + str(count1))\n",
    "        print('Similar tracks (no-op): ' + str(count2))\n",
    "        print('New track creations from little overlap(0): ' + str(count5))\n",
    "        print('New track creations from huge tracks(>20): ' + str(count9))\n",
    "        print('Test for new track creations from little overlap(non-0): ' + str(count10))\n",
    "        print('--> Lengthen longest overlap instead: ' + str(count14))\n",
    "        print('  --> And clear 2nd longest track: ' + str(count15))\n",
    "        print('Skip extension due to too little overlap: ' + str(count16))\n",
    "        print('Multiple non-trivial tracks: ' + str(count3))\n",
    "        print('--> of which partial track ID 0 hits were updated: ' + str(count6))\n",
    "        print('--> of which partial track ID non-0 hits were updated: ' + str(count7))\n",
    "        print('--> of which partial track ID non-0 hits were skipped: ' + str(count13))\n",
    "        print('--> of which outliers were overwritten: ' + str(count12))\n",
    "        print('Tracks to be lengthened: ' + str(count4))\n",
    "        print('--> of which track ID 0 hits were updated: ' + str(count8))\n",
    "        print('--> from which new tracks were created instead: ' + str(count11))\n",
    "        noises = np.unique(np.asarray(count4_len))\n",
    "        print('--> of which labels2 unique track lengths were: ' + str(noises))\n",
    "\n",
    "    return labels_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_7_extend = np.copy(labels_hough)\n",
    "print(len(labels_7_extend))\n",
    "one_submission = create_one_event_submission(event_id, hits, labels_7_extend)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Extension %d loop %d score for event %d: %.8f\" % (ix,i+1,event_id, score))\n",
    "track_extension_limits = [0.02, 0.04, 0.06, 0.08, 0.10]\n",
    "for ix, limit in enumerate(track_extension_limits):\n",
    "    labels_7_extend = extend_labels(ix, labels_7_extend, hits, do_swap=(ix%2==1), limit=(limit))\n",
    "    one_submission = create_one_event_submission(event_id, hits, labels_7_extend)\n",
    "    score = score_event(truth, one_submission)\n",
    "    print(\"Extension %d score for event %d: %.8f\" % (ix,event_id, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTENSION_ATTEMPT = 8\n",
    "EXTENSION_LIMIT_START = 0.03\n",
    "EXTENSION_LIMIT_INTERVAL = 0.005\n",
    "\n",
    "\n",
    "l1a = np.copy(labels_helix1)\n",
    "l2a = np.copy(labels_helix2)\n",
    "l3a = np.copy(labels_helix3)\n",
    "l4a = np.copy(labels_helix4)\n",
    "for i in range(EXTENSION_ATTEMPT):\n",
    "    limit = EXTENSION_LIMIT_START + EXTENSION_LIMIT_INTERVAL*i\n",
    "    l1a = ext.extend_labels(i, l1a, hits, do_swap=i%2==1, limit=(limit))\n",
    "    l2a = ext.extend_labels(i, l2a, hits, do_swap=i%2==1, limit=(limit))\n",
    "    l3a = ext.extend_labels(i, l3a, hits, do_swap=i%2==1, limit=(limit))\n",
    "    l4a = ext.extend_labels(i, l4a, hits, do_swap=i%2==1, limit=(limit))\n",
    "\n",
    "l1a = merge.renumber_labels(l1a)\n",
    "l2a = merge.renumber_labels(l2a)\n",
    "l3a = merge.renumber_labels(l3a)\n",
    "l4a = merge.renumber_labels(l4a)\n",
    "\n",
    "seed_length = 5\n",
    "my_volumes = [7, 8, 9]\n",
    "l1a = merge.filter_invalid_tracks(l1a, hits, my_volumes, seed_length)\n",
    "l1a = merge.renumber_labels(l1a)\n",
    "l2a = merge.filter_invalid_tracks(l2a, hits, my_volumes, seed_length)\n",
    "l2a = merge.renumber_labels(l2a)\n",
    "l3a = merge.filter_invalid_tracks(l3a, hits, my_volumes, seed_length)\n",
    "l3a = merge.renumber_labels(l3a)\n",
    "l4a = merge.filter_invalid_tracks(l4a, hits, my_volumes, seed_length)\n",
    "l4a = merge.renumber_labels(l4a)\n",
    "\n",
    "# Perform sophisticated outlier removal, duplicate-z removal, slope-based removal\n",
    "l1a = merge.remove_outliers(l1a, hits, print_counts=False)\n",
    "l2a = merge.remove_outliers(l2a, hits, print_counts=False)\n",
    "l3a = merge.remove_outliers(l3a, hits, print_counts=False)\n",
    "l4a = merge.remove_outliers(l4a, hits, print_counts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_merged = merge.heuristic_merge_tracks(l1a, l2a)\n",
    "#labels_merged = merge.heuristic_merge_tracks(labels_merged, l3a)\n",
    "#labels_merged = merge.heuristic_merge_tracks(labels_merged, l4a)\n",
    "labels_merged2 = merge.heuristic_merge_tracks(l3a, l4a)\n",
    "labels_merged = merge.heuristic_merge_tracks(labels_merged, labels_merged2)\n",
    "#labels_merged = merge.heuristic_merge_tracks(labels_merged, labels_helix4_filter)\n",
    "#labels_merged = merge.heuristic_merge_tracks(labels_merged, labels_helix5_filter)\n",
    "#labels_merged = merge.heuristic_merge_tracks(labels_merged, labels_helix6_filter)\n",
    "one_submission = create_one_event_submission(event_id, hits, labels_merged)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merged score for event %d: %.8f\" % (event_id, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hack_one_extend(labels, hits, truth):\n",
    "    angle=82\n",
    "    limit=0.04\n",
    "    df = hits.copy(deep=True)\n",
    "    df['track_id'] = labels.tolist()\n",
    "    df = df.assign(d = np.sqrt( df.x**2 + df.y**2 + df.z**2 ))\n",
    "    df = df.assign(r = np.sqrt( df.x**2 + df.y**2))\n",
    "    df = df.assign(arctan2 = np.arctan2(df.z, df.r))\n",
    "    df = _one_cone_slice(df, truth, angle, 1, limit)\n",
    "    return df.track_id.values\n",
    "\n",
    "def extend(iter, df, do_swap=False, limit=0.04):\n",
    "    if do_swap:\n",
    "        df = df.assign(x = -df.x)\n",
    "        df = df.assign(y = -df.y)\n",
    "\n",
    "    df = df.assign(d = np.sqrt( df.x**2 + df.y**2 + df.z**2 ))\n",
    "    df = df.assign(r = np.sqrt( df.x**2 + df.y**2))\n",
    "    df = df.assign(arctan2 = np.arctan2(df.z, df.r))\n",
    "\n",
    "\n",
    "    for angle in range(-90,90,1):\n",
    "\n",
    "        print ('\\r%d %f '%(iter,angle), end='',flush=True)\n",
    "        df1 = df.loc[(df.arctan2>(angle-1.0)/180*np.pi) & (df.arctan2<(angle+1.0)/180*np.pi)]\n",
    "\n",
    "        num_hits = len(df1)\n",
    "        # Dynamically adjust the delta based on how many hits are found\n",
    "        if num_hits > 2000:\n",
    "            df = _one_cone_slice(df, angle-0.6, 0.4, limit)\n",
    "            df = _one_cone_slice(df, angle-0.2, 0.4, limit)\n",
    "            df = _one_cone_slice(df, angle+0.2, 0.4, limit)\n",
    "            df = _one_cone_slice(df, angle+0.6, 0.4, limit)\n",
    "        else:\n",
    "            df = _one_cone_slice(df, angle, 1, limit)\n",
    "           \n",
    "    return df\n",
    "\n",
    "def extend_labels(iter, labels, hits, do_swap=False, limit=0.04):\n",
    "    df = hits.copy(deep=True)\n",
    "    df['track_id'] = labels.tolist()\n",
    "    return extend(iter, df, do_swap, limit).track_id.values\n",
    "\n",
    "def _one_cone_slice(df, truth, angle, delta_angle, limit=0.04, num_neighbours=18):\n",
    "\n",
    "    df1 = df.loc[(df.arctan2>(angle - delta_angle)/180*np.pi) & (df.arctan2<(angle + delta_angle)/180*np.pi)]\n",
    "\n",
    "    min_num_neighbours = len(df1)\n",
    "    if min_num_neighbours < 3: \n",
    "        return df\n",
    "\n",
    "    hit_ids = df1.hit_id.values\n",
    "    x,y,z = df1.as_matrix(columns=['x', 'y', 'z']).T\n",
    "    r  = (x**2 + y**2)**0.5\n",
    "    r  = r/1000\n",
    "    a  = np.arctan2(y,x)\n",
    "    c = np.cos(a)\n",
    "    s = np.sin(a)\n",
    "    tree = KDTree(np.column_stack([c,s,r]), metric='euclidean')\n",
    "\n",
    "    track_ids = list(df1.track_id.unique())\n",
    "    num_track_ids = len(track_ids)\n",
    "    min_length=3\n",
    "\n",
    "    \n",
    "    for i in range(num_track_ids):\n",
    "        p = track_ids[i]\n",
    "        if p==0: continue\n",
    "\n",
    "        idx = np.where(df1.track_id==p)[0]\n",
    "        cur_track_len = len(idx)\n",
    "        if cur_track_len<min_length: continue\n",
    "\n",
    "        truth_ix = []\n",
    "        for ii in idx:\n",
    "            truth_ix.append(hit_ids[ii] - 1)\n",
    "        tdf = truth.loc[truth_ix]\n",
    "        truth_count = coll.Counter(tdf.particle_id.values).most_common(1)\n",
    "        truth_particle_id = truth_count[0][0]\n",
    "        #print('track: ' + str(p) + ', len: ' + str(len(idx)) + ', idx: ' + str(idx))\n",
    "        #print('truth particle: ' + str(truth_particle_id) + ', count:' + str(truth_count[0][1]))\n",
    "            \n",
    "        if angle>0:\n",
    "            idx = idx[np.argsort( z[idx])]\n",
    "        else:\n",
    "            idx = idx[np.argsort(-z[idx])]\n",
    "\n",
    "## start and end points  ##\n",
    "        idx0,idx1 = idx[0],idx[-1]\n",
    "        a0 = a[idx0]\n",
    "        a1 = a[idx1]\n",
    "        r0 = r[idx0]\n",
    "        r1 = r[idx1]\n",
    "        c0 = c[idx0]\n",
    "        c1 = c[idx1]\n",
    "        s0 = s[idx0]\n",
    "        s1 = s[idx1]\n",
    "\n",
    "        da0 = a[idx[1]] - a[idx[0]]  #direction\n",
    "        dr0 = r[idx[1]] - r[idx[0]]\n",
    "        direction0 = np.arctan2(dr0,da0)\n",
    "\n",
    "        da1 = a[idx[-1]] - a[idx[-2]]\n",
    "        dr1 = r[idx[-1]] - r[idx[-2]]\n",
    "        direction1 = np.arctan2(dr1,da1)\n",
    "\n",
    "        ## extend start point\n",
    "        ns = tree.query([[c0, s0, r0]], k=min(num_neighbours, min_num_neighbours), return_distance=False)\n",
    "        ns = np.concatenate(ns)\n",
    "\n",
    "        direction = np.arctan2(r0 - r[ns], a0 - a[ns])\n",
    "        diff = 1 - np.cos(direction - direction0)\n",
    "        ns = ns[(r0 - r[ns] > 0.01) & (diff < (1 - np.cos(limit)))]\n",
    "        for n in ns:\n",
    "            df_ix = hit_ids[n] - 1\n",
    "            if truth.loc[df_ix, 'particle_id'] == truth_particle_id:\n",
    "                print('Found valid start extension!')\n",
    "            else:\n",
    "                print('Found WRONG start extension!')\n",
    "            old_track = df.loc[df_ix, 'track_id']\n",
    "            if old_track == 0:\n",
    "                df.loc[df_ix, 'track_id'] = p\n",
    "            elif old_track != 0:\n",
    "                # If the hit is already occupied by another track, only take ownership\n",
    "                # of the hit if our track is longer than the current-occupying track.\n",
    "                existing_track_len = len(np.where(df.track_id==old_track)[0])\n",
    "                if cur_track_len > existing_track_len:\n",
    "                    df.loc[df_ix, 'track_id'] = p\n",
    "    \n",
    "\n",
    "        ## extend end point\n",
    "        ns = tree.query([[c1, s1, r1]], k=min(num_neighbours, min_num_neighbours), return_distance=False)\n",
    "        ns = np.concatenate(ns)\n",
    "\n",
    "        direction = np.arctan2(r[ns] - r1, a[ns] - a1)\n",
    "        diff = 1 - np.cos(direction - direction1)\n",
    "  \n",
    "        ns = ns[(r[ns] - r1 > 0.01) & (diff < (1 - np.cos(limit)))]\n",
    "        for n in ns:  \n",
    "            df_ix = hit_ids[n] - 1\n",
    "            if truth.loc[df_ix, 'particle_id'] == truth_particle_id:\n",
    "                print('Found valid end extension!')\n",
    "            else:\n",
    "                print('Found WRONG end extension!')\n",
    "            old_track = df.loc[df_ix, 'track_id']\n",
    "            if old_track == 0:\n",
    "                df.loc[df_ix, 'track_id'] = p\n",
    "            elif old_track != 0:\n",
    "                # If the hit is already occupied by another track, only take ownership\n",
    "                # of the hit if our track is longer than the current-occupying track.\n",
    "                existing_track_len = len(np.where(df.track_id==old_track)[0])\n",
    "                if cur_track_len > existing_track_len:\n",
    "                    df.loc[df_ix, 'track_id'] = p\n",
    "      \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = hack_one_extend(labels_helix1, hits, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pre-merge\n",
    "#labels_helix1_filter = np.copy(labels_helix1)\n",
    "#labels_helix2_filter = np.copy(labels_helix2)\n",
    "#labels_helix3_filter = np.copy(labels_helix3)\n",
    "#labels_helix4_filter = np.copy(labels_helix4)\n",
    "\n",
    "labels_merged = merge.heuristic_merge_tracks(labels_helix1_filter, labels_helix2_filter)\n",
    "labels_merged2 = merge.heuristic_merge_tracks(labels_helix3_filter, labels_helix4_filter)\n",
    "labels_merged = merge.heuristic_merge_tracks(labels_merged, labels_merged2)\n",
    "#one_submission = create_one_event_submission(event_id, hits, labels_merged)\n",
    "#score = score_event(truth, one_submission)\n",
    "#print(\"H1/H2 Merged score for event %d: %.8f\" % (event_id, score))\n",
    "#one_submission = create_one_event_submission(event_id, hits, labels_merged2)\n",
    "#score = score_event(truth, one_submission)\n",
    "#print(\"H3/H4 Merged score for event %d: %.8f\" % (event_id, score))\n",
    "\n",
    "#labels_merged = merge.heuristic_merge_tracks(labels_helix1_filter, labels_helix2_filter)\n",
    "#labels_merged = merge.heuristic_merge_tracks(labels_merged, labels_helix3_filter)\n",
    "#labels_merged = merge.heuristic_merge_tracks(labels_merged, labels_helix4_filter)\n",
    "\n",
    "one_submission = create_one_event_submission(event_id, hits, labels_merged)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merged score for event %d: %.8f\" % (event_id, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
