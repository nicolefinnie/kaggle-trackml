{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from trackml.dataset import load_event\n",
    "from trackml.randomize import shuffle_hits\n",
    "from trackml.score import score_event\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import collections as coll\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "import merge as merge\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../../../input/train_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event000001003 memory usage 15.95 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "event_prefix = 'event000001003'\n",
    "hits, cells, particles, truth = load_event(os.path.join(TRAIN_PATH, event_prefix))\n",
    "\n",
    "mem_bytes = (hits.memory_usage(index=True).sum() \n",
    "             + cells.memory_usage(index=True).sum() \n",
    "             + particles.memory_usage(index=True).sum() \n",
    "             + truth.memory_usage(index=True).sum())\n",
    "print('{} memory usage {:.2f} MB'.format(event_prefix, mem_bytes / 2**20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_cone = pd.read_csv('../../event_1003_labels_train_cone.csv').label.values\n",
    "#labels_helix = pd.read_csv('../../event_1000_labels_train_helix1.csv').label.values\n",
    "labels_helix1 = pd.read_csv('../../loop_nofilter_noextend/event_1003_labels1_train_helix1.csv').label.values\n",
    "labels_helix2 = pd.read_csv('../../loop_nofilter_noextend/event_1003_labels2_train_helix1.csv').label.values\n",
    "labels_helix3 = pd.read_csv('../../loop_nofilter_noextend/event_1003_labels3_train_helix1.csv').label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_hh = pd.read_csv('event_1003_helix2.csv').label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniq_cone = np.unique(labels_cone)\n",
    "uniq_helix = np.unique(labels_helix)\n",
    "#print(uniq_cone)\n",
    "print(uniq_helix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_event_submission(event_id, hits, labels):\n",
    "    sub_data = np.column_stack(([event_id]*len(hits), hits.hit_id.values, labels))\n",
    "    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_submission = create_one_event_submission(1000, hits, labels_helix)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"helix score for event %d: %.8f\" % (1000, score))\n",
    "#one_submission = create_one_event_submission(1000, hits, labels_cone)\n",
    "#score = score_event(truth, one_submission)\n",
    "#print(\"cone score for event %d: %.8f\" % (1000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_simple_merge = merge.merge_tracks(labels_helix, labels_cone)\n",
    "one_submission = create_one_event_submission(1000, hits, labels_simple_merge)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Simple merged cone+helix score for event %d: %.8f\" % (1000, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Removing cone outliers')\n",
    "labels_c3 = np.copy(labels_cone)\n",
    "labels_c3 = remove_outliers(labels_c3, hits, print_counts=True)\n",
    "\n",
    "print('Removing helix outliers')\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "labels_h3 = remove_outliers(labels_h3, hits, print_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Merging tracks')\n",
    "tmp_labels = merge.heuristic_merge_tracks(labels_helix, labels_cone)\n",
    "one_submission = create_one_event_submission(1000, hits, tmp_labels)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"cone score for event %d: %.8f\" % (1000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Merging tracks')\n",
    "tmp_labels = merge.heuristic_merge_tracks(labels_h3, labels_c3)\n",
    "one_submission = create_one_event_submission(1000, hits, tmp_labels)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merged score for event %d: %.8f\" % (1000, score))\n",
    "# Baseline helix prior to outlier removal: 0.51217316\n",
    "# Score: 0.4766-->0.4799 after improved outlier removal\n",
    "# Score is 0.5037 if, after outlier removal,  we only add tracks from labels_c3 when no such track was recorded in labels_h3\n",
    "# Score is 0.51087490 if we only remove bad volumes, duplicatez, and singletons, and only add tracks from labels_c3 when no such track was recorded in labels_h3\n",
    "# Score is 0.51936460, remove badv, dupz, sings, selective track merging.\n",
    "# Score is 0.52234651, remove badv, dupz, sings, selective track merging.\n",
    "# Score is 0.52499622, remove badv, dupz, sings, selective track merging, overwrite smaller tracks of length <= 3.\n",
    "# Score is 0.52653574, same as above except overwrite tracks of length <= 4\n",
    "# Score is 0.52622554, same as above except overwrite tracks of length <= 5\n",
    "# Score is 0.52209245, full outlier removal, overwrite tracks of length <= 4\n",
    "# Score is 0.58658664 orig heuristic\n",
    "#  --> 0.58240360 with aggressive outlier removal\n",
    "#  --> 0.58621259 with aggressive cone removal, non-aggressive helix\n",
    "#  --> 0.58417970 with aggressive helix removal, non-aggressive cone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: Need to evaluate this better, seems to hurt!\n",
    "def find_invalid_volumes(track, labels, df):\n",
    "    invalid_ix = []\n",
    "\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "    df2 = df.loc[hit_ix]\n",
    "    df2 = df2.sort_values('z_abs')\n",
    "    hit_ix2 = df2.index.values\n",
    "    all_positive = np.all(df2.z.values >= 0)\n",
    "    all_negative = np.all(df2.z.values <= 0)\n",
    "    volumes = df2.volume_id.values\n",
    "    layers = df2.layer_id.values\n",
    "    last_volume = volumes[0]\n",
    "    last_layer = layers[0]\n",
    "    # Tracks with the first volume of 8, 13, and 17 are very odd, sometimes\n",
    "    # they hit in the negative way, sometimes the positive way,\n",
    "    # sometimes a mix of both. Ignore these.\n",
    "    if last_volume == 8 or last_volume == 13 or last_volume == 17:\n",
    "        all_negative = False\n",
    "        all_positive = False\n",
    "    for idx, cur_vol in enumerate(volumes):\n",
    "        cur_layer = layers[idx]\n",
    "        if all_positive:\n",
    "            # When we go from one volume to the next, we expect to see the\n",
    "            # layer drop from a large layer id to a smaller layer id.\n",
    "            # If we stay in the same volume, the layer id should not decrease.\n",
    "            #if (last_volume != cur_vol and (cur_layer > (last_layer - 4))) or (last_volume == cur_vol and cur_layer < last_layer):\n",
    "            if (last_volume == cur_vol and cur_layer < last_layer):\n",
    "                invalid_ix.append(hit_ix2[idx])\n",
    "            else:\n",
    "                last_volume = cur_vol\n",
    "                last_layer = cur_layer\n",
    "        elif all_negative:\n",
    "            # When we go from one volume to the next, we expect to see the\n",
    "            # layer increase from a small layer id to a larger layer id.\n",
    "            # If we stay in the same volume, the layer id should not increase.\n",
    "            #if (last_volume != cur_vol and (cur_layer < (last_layer + 4))) or (last_volume == cur_vol and cur_layer > last_layer):\n",
    "            if (last_volume == cur_vol and cur_layer > last_layer):\n",
    "                invalid_ix.append(hit_ix2[idx])\n",
    "            else:\n",
    "                last_volume = volumes[idx]\n",
    "                last_layer = layers[idx]\n",
    "        else:\n",
    "            last_volume = cur_vol\n",
    "            last_layer = cur_layer\n",
    "\n",
    "    return invalid_ix\n",
    "    \n",
    "def find_dimension_outlier(track, labels, df, dimension):\n",
    "    outlier_ix = []\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "\n",
    "    # Need at least 3 values to determine if any look like outliers\n",
    "    if len(hit_ix) < 3:\n",
    "        return outlier_ix\n",
    "\n",
    "    df2 = df.loc[hit_ix]        \n",
    "    df2 = df2.sort_values('z')\n",
    "    hit_ix2 = df2.index.values\n",
    "\n",
    "    # Note, diff[0] is diff between 0 and 1\n",
    "    diffs = np.diff(df2[dimension].values)\n",
    "\n",
    "    growing_trend = 0\n",
    "    shrinking_trend = 0\n",
    "    for idx, diff in enumerate(diffs):\n",
    "        if idx > 0 and diff > diffs[idx-1]:\n",
    "            growing_trend = growing_trend + 1\n",
    "        if idx > 0 and diff < diffs[idx-1]:\n",
    "            shrinking_trend = shrinking_trend + 1\n",
    "\n",
    "    check_largest_and_smallest = True\n",
    "    if growing_trend > math.ceil(0.6*len(diffs)) or shrinking_trend > math.ceil(0.6*len(diffs)):\n",
    "        check_largest_and_smallest = False\n",
    "\n",
    "    if check_largest_and_smallest:\n",
    "        # Find largest and smallest diffs, if largest is 20x larger than 2nd largest,\n",
    "        # or smallest is 20x smaller than 2nd smallest, consider them outliers.\n",
    "        top_two_ix = diffs.argsort()[-2:][::-1]\n",
    "        large1 = diffs[top_two_ix[0]]\n",
    "        large2 = diffs[top_two_ix[1]]\n",
    "        bot_two_ix = diffs.argsort()[:2]\n",
    "        small1 = diffs[bot_two_ix[0]]\n",
    "        small2 = diffs[bot_two_ix[1]]\n",
    "\n",
    "        largest_is_outlier = False\n",
    "        smallest_is_outlier = False\n",
    "        if large1 > 0 and large2 > 0 and large1 > 10.0 and large2 > 2.0 and (large2*7) < large1:\n",
    "            largest_is_outlier = True\n",
    "        if large1 < 0 and large2 < 0 and large1 < -10.0 and large2 < -2.0 and (large1*7) > large2:\n",
    "            largest_is_outlier = True\n",
    "        if small1 > 0 and small2 > 0 and small1 > 10.0 and small2 > 2.0 and (small2*7) < small1:\n",
    "            smallest_is_outlier = True\n",
    "        if small1 < 0 and small2 < 0 and small1 < -10.0 and small2 < -2.0 and (small1*7) > small2:\n",
    "            smallest_is_outlier = True\n",
    "\n",
    "        if largest_is_outlier or smallest_is_outlier:\n",
    "            hit_ix_list = hit_ix.tolist()\n",
    "            for idx, diff in enumerate(diffs):\n",
    "                if (largest_is_outlier and diff == large1) or (smallest_is_outlier and diff == small1):\n",
    "                    #print('Removing extreme outlier diff: ' + str(diff) + ', ix: ' + str(hit_ix2[idx + 1]) + ', from diffs: ' + str(diffs))\n",
    "                    outlier_ix.append(hit_ix2[idx + 1])\n",
    "                    hit_ix_list.remove(hit_ix2[idx + 1])\n",
    "\n",
    "            # Re-generate the diffs now that we've removed the extreme outliers\n",
    "            hit_ix = np.asarray(hit_ix_list)\n",
    "            if len(hit_ix) < 3:\n",
    "                return outlier_ix\n",
    "            df2 = df.loc[hit_ix]        \n",
    "            df2 = df2.sort_values('z')\n",
    "            hit_ix2 = df2.index.values\n",
    "            diffs = np.diff(df2[dimension].values)\n",
    "                \n",
    "    # Restrict to when the majority (75%+) of diffs are all in same direction\n",
    "    neg_diffs = np.where(diffs < 0)[0]\n",
    "    pos_diffs = np.where(diffs >= 0)[0]\n",
    "\n",
    "    #print(df2[dimension].values)\n",
    "    #print(hit_ix)\n",
    "    #print('trk: ' + str(track) + ', diffs: ' + str(diffs))\n",
    "    #print(neg_diffs)\n",
    "    #print(pos_diffs)\n",
    "    #print(df2)\n",
    "\n",
    "    # Restrict to when the majority of diffs are either positive or negative.\n",
    "    # (more difficult to detect outliers if diffs oscillate -ve and +ve)\n",
    "    dim_vals = df2[dimension].values\n",
    "    if len(neg_diffs) >= math.ceil(0.75*len(diffs)):\n",
    "        # remove large positive ones.\n",
    "        growing_trend = 0\n",
    "        previous_diff = 0\n",
    "        for idx, diff in enumerate(diffs):\n",
    "            # Some tracks trend from negative to positive diffs, don't eliminate\n",
    "            # positive values if it looks like we are trending that way.\n",
    "            if idx > 0 and diff > diffs[idx-1]:\n",
    "                growing_trend = growing_trend + 1\n",
    "                if growing_trend > 2:\n",
    "                    break\n",
    "            else:\n",
    "                growing_trend = 0\n",
    "            # Use absolute value > 1.0 in case there is small delta each time,\n",
    "            # we only try to find big jumps in the wrong direction.\n",
    "            #print('nidx-' + dimension + ': ' + str(idx) + ', diff: ' + str(diff) + ', df ix: ' + str(hit_ix2[idx+1]))\n",
    "            if diff > 1.0:\n",
    "                # We sometimes see cases like:\n",
    "                # diff[n-1] = -22\n",
    "                # diff[n] = 12\n",
    "                # diff[n+1] = -14\n",
    "                # In this case, we want to remove n-1 as the outlier, since if that\n",
    "                # was gone, diff[n] would be -10, which is more reasonable.\n",
    "                # In cases where we see:\n",
    "                # diff[0] = 23\n",
    "                # diff[1] = -5\n",
    "                # We want to check the dimension values directly instead of the diffs, it\n",
    "                # could be that val[0] is the outlier.\n",
    "                if idx == 0 and dim_vals[1] > dim_vals[2] and dim_vals[0] < dim_vals[2]:\n",
    "                    # The real outlier is the first entry in the list.\n",
    "                    outlier_ix.append(hit_ix2[0])\n",
    "                elif idx == 0 or idx == (len(diffs)-1) or ((diff + diffs[idx-1]) > 0) or diffs[idx+1] > 0:\n",
    "                    #print('Removing: ' + str(hit_ix2[idx+1]))\n",
    "                    outlier_ix.append(hit_ix2[idx + 1])\n",
    "                else:\n",
    "                    # The real outlier is the previous one (i.e. diff[n-1] in the example above!\n",
    "                    outlier_ix.append(hit_ix2[idx])\n",
    "    \n",
    "    elif len(pos_diffs) >= math.ceil(0.75*len(diffs)):\n",
    "        # remove large negative ones\n",
    "        shrinking_trend = 0\n",
    "        for idx, diff in enumerate(diffs):\n",
    "            # Some tracks trend from positive to negative diffs, don't eliminate\n",
    "            # negative values if it looks like we are trending that way.\n",
    "            if idx > 0 and diff < diffs[idx-1]:\n",
    "                shrinking_trend = shrinking_trend + 1\n",
    "                if shrinking_trend > 2:\n",
    "                    break\n",
    "            else:\n",
    "                shrinking_trend = 0\n",
    "            # Use absolute value > 1.0 in case there is small delta each time,\n",
    "            # we only try to find big jumps in the wrong direction.\n",
    "            #print('pidx-' + dimension + ': ' + str(idx) + ', diff: ' + str(diff) + ', df ix: ' + str(hit_ix2[idx+1]))\n",
    "            if diff < -1.0:\n",
    "                #print('Removing: ' + str(hit_ix2[idx+1]))\n",
    "                # Similar to the negative case above, make sure we remove the real\n",
    "                # outlier, in case the previous diff was misleading.\n",
    "                if idx == 0 and dim_vals[1] < dim_vals[2] and dim_vals[0] > dim_vals[2]:\n",
    "                    # The real outlier is the first entry in the list.\n",
    "                    outlier_ix.append(hit_ix2[0])\n",
    "                elif idx == 0 or idx == (len(diffs)-1) or ((diff + diffs[idx-1]) < 0) or diffs[idx+1] < 0:\n",
    "                    #print('Removing: ' + str(hit_ix2[idx+1]))\n",
    "                    outlier_ix.append(hit_ix2[idx + 1])\n",
    "                else:\n",
    "                    # The real outlier is the previous one (i.e. diff[n-1] in the example above!\n",
    "                    outlier_ix.append(hit_ix2[idx])\n",
    "\n",
    "\n",
    "\n",
    "    # Future ideas for patterns:\n",
    "    # - average positive jump + average negative jump, for values that oscillate +ve and -ve\n",
    "    # - absolute value of jump in same direction, this is hard since some tracks seem jumpy\n",
    "    #   i.e. small diffs followed by a bigger jump, then smaller diffs. May need to tie that\n",
    "    #   in with volume/layer/module ids, i.e. only allow bigger jumps between layers.\n",
    "    return outlier_ix                \n",
    "\n",
    "def find_duplicate_z(track, labels, df):\n",
    "    def number_is_between(a1, a2, a3):\n",
    "        return (a1 >= a2 and a2 >= a3) or (a1 <= a2 and a2 <= a3)\n",
    "\n",
    "    def numbers_are_between(a1, a2, a3, b1, b2, b3):\n",
    "        return number_is_between(a1, a2, a3) and number_is_between(b1, b2, b3)\n",
    "\n",
    "    duplicatez_ix = []\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "\n",
    "    # Need at least 4 values to be able to evaluate duplicate z-values.\n",
    "    if len(hit_ix) < 4:\n",
    "        return duplicatez_ix\n",
    "\n",
    "    df2 = df.loc[hit_ix]        \n",
    "    df2 = df2.sort_values('z_abs')\n",
    "    hit_ix2 = df2.index.values # remember new indexes after sorting\n",
    "    xs = df2.x.values\n",
    "    ys = df2.y.values\n",
    "    zs = df2.z.values\n",
    "    max_idx = len(zs) - 1\n",
    "\n",
    "    z_counts = coll.Counter(df2.z.values).most_common(len(df2.z.values))\n",
    "\n",
    "    if zs[0] == zs[1]:\n",
    "        # zs at the beginning\n",
    "        x1 = xs[2]\n",
    "        x2 = xs[3]\n",
    "        y1 = ys[2]\n",
    "        y2 = ys[3]\n",
    "        if numbers_are_between(xs[0], x1, x2, ys[0], y1, y2) and not numbers_are_between(xs[1], x1, x2, ys[1], y1, y2):\n",
    "            # The first one is more consistent, delete the 2nd duplicate value\n",
    "            duplicatez_ix.append(hit_ix2[1])\n",
    "            #print('xs[1] ' + str(xs[1]) + ' <= x1 ' + str(x1) + ' <= x2 ' + str(x2))\n",
    "            #print('ys[1] ' + str(ys[1]) + ' <= y1 ' + str(y1) + ' <= y2 ' + str(y2))\n",
    "        elif numbers_are_between(xs[1], x1, x2, ys[1], y1, y2) and not numbers_are_between(xs[0], x1, x2, ys[0], y1, y2):\n",
    "            # The second one is more consistent, delete the 1st duplicate value\n",
    "            duplicatez_ix.append(hit_ix2[0])\n",
    "            #print('b')\n",
    "        elif numbers_are_between(xs[0], x1, x2, ys[0], y1, y2) and numbers_are_between(xs[1], x1, x2, ys[1], y1, y2):\n",
    "            # Both z-values seem reasonable, need a tie-breaker to find out which is the right one.\n",
    "            add_code_here = True\n",
    "        # else, neither seem valid, unsure how to proceed, better off not rejecting any.\n",
    "\n",
    "    if zs[-1] == zs[-2]:\n",
    "        # zs at the end\n",
    "        x1 = xs[-4]\n",
    "        x2 = xs[-3]\n",
    "        y1 = ys[-4]\n",
    "        y2 = ys[-3]\n",
    "        if numbers_are_between(x1, x2, xs[-2], y1, y2, ys[-2]) and not numbers_are_between(x1, x2, xs[-1], y1, y2, ys[-1]):\n",
    "            # The first one is more consistent, delete the last duplicate value\n",
    "            duplicatez_ix.append(hit_ix2[-1])\n",
    "        elif numbers_are_between(x1, x2, xs[-1], y1, y2, ys[-1]) and not numbers_are_between(x1, x2, xs[-2], y1, y2, ys[-2]):\n",
    "            # The last one is more consistent, delete the 1st duplicate value\n",
    "            duplicatez_ix.append(hit_ix2[-2])\n",
    "        elif numbers_are_between(x1, x2, xs[-1], y1, y2, ys[-1]) and numbers_are_between(x1, x2, xs[-2], y1, y2, ys[-2]):\n",
    "            # Both z-values seem reasonable, need a tie-breaker to find out which is the right one.\n",
    "            add_code_here = True\n",
    "        # else, neither seem valid, unsure how to proceed, better off not rejecting any.\n",
    "        \n",
    "    # Idea: Find duplicate adjacent z-values. Remember x and y before and after the\n",
    "    # duplicates. Choose z that lies between the two. If z at beginning or end,\n",
    "    # need the two post (or pre-) x/y values to see the expected sign of the diff.\n",
    "\n",
    "    # Note max_idx is largest valid index, we already handled the case where the\n",
    "    # duplicate zs are at the beginning or end of the list.\n",
    "    for idx in range(0, max_idx):\n",
    "        if idx > 0 and (idx+2) <= max_idx and zs[idx] == zs[idx+1]:\n",
    "            x1 = xs[idx-1]\n",
    "            x2 = xs[idx+2]\n",
    "            y1 = ys[idx-1]\n",
    "            y2 = ys[idx+2]\n",
    "            # now, x1 <= z1 <= x2, and y1 <= z1 <= y2\n",
    "            if numbers_are_between(x1, xs[idx], x2, y1, ys[idx], y2) and not numbers_are_between(x1, xs[idx+1], x2, y1, ys[idx+1], y2):\n",
    "                # The first one is more consistent, delete the 2nd duplicate value\n",
    "                duplicatez_ix.append(hit_ix2[idx+1])\n",
    "            elif numbers_are_between(x1, xs[idx+1], x2, y1, ys[idx+1], y2) and not numbers_are_between(x1, xs[idx], x2, y1, ys[idx], y2):\n",
    "                # The second one is more consistent, delete the 1st duplicate value\n",
    "                duplicatez_ix.append(hit_ix2[idx])\n",
    "            elif numbers_are_between(x1, xs[idx], x2, y1, ys[idx], y2) and numbers_are_between(x1, xs[idx+1], x2, y1, ys[idx+1], y2):\n",
    "                # Both z-values seem reasonable, need a tie-breaker to find out which is the right one.\n",
    "                add_code_here = True\n",
    "            # else, neither seem valid, unsure how to proceed, better off not rejecting any.\n",
    "\n",
    "    #if z_counts[0][1] > 1:\n",
    "    #    print('Duplicatez found on track ' + str(track) + ', removed: ' + str(duplicatez_ix))\n",
    "\n",
    "    return duplicatez_ix\n",
    "\n",
    "# TODO pi, -pi discontinuity \n",
    "def remove_track_outliers_slope(track, labels, hits, debug=False):\n",
    "    final_outliers = []\n",
    "\n",
    "    hhh_ix = np.where(labels == track)\n",
    "    hhh_h = hits.loc[hhh_ix].sort_values('z')\n",
    "    \n",
    "    slopes_backward = []\n",
    "    slopes_forward = []\n",
    "\n",
    "    num_hits = len(hhh_h)\n",
    "    # Only reliable with tracks >= 5 hits\n",
    "    if num_hits < 5:\n",
    "        return final_outliers\n",
    "\n",
    "    if debug: print('backward:')\n",
    "    for i in np.arange(num_hits-1,0,-1):\n",
    "        a0 =  hhh_h.a0.values[i]\n",
    "        a1 =  hhh_h.a0.values[i-1]\n",
    "        r0 =  hhh_h.r.values[i]\n",
    "        r1 =  hhh_h.r.values[i-1]\n",
    "        if r0 == r1:\n",
    "            r0 = r0 + 1e-8\n",
    "        slope = (a0-a1)/(r0-r1) \n",
    "        slopes_backward.append(slope)\n",
    "        if debug: print(hhh_h.hit_id.values[i], slope, a0)\n",
    "        if i == 1:\n",
    "            a0 = hhh_h.a0.values[0]\n",
    "            a1 = hhh_h.a0.values[num_hits-1]\n",
    "            r0 =  hhh_h.r.values[0]\n",
    "            r1 =  hhh_h.r.values[num_hits-1]\n",
    "            if r0 == r1:\n",
    "                r0 = r0 + 1e-8\n",
    "            slope = (a0-a1)/(r0-r1)\n",
    "            slopes_backward.append(slope)\n",
    "            if debug: print(hhh_h.hit_id.values[0], slope, a1)\n",
    "\n",
    "    if debug: print('forward:')\n",
    "    for i in np.arange(0,num_hits-1,1):\n",
    "        a0 =  hhh_h.a0.values[i]\n",
    "        a1 =  hhh_h.a0.values[i+1]\n",
    "        r0 =  hhh_h.r.values[i]\n",
    "        r1 =  hhh_h.r.values[i+1]\n",
    "        if r0 == r1:\n",
    "            r1 = r1 + 1e-8\n",
    "        slope = (a1-a0)/(r1-r0) \n",
    "        slopes_forward.append(slope)\n",
    "        if debug: print(hhh_h.hit_id.values[i], slope, a0)\n",
    "\n",
    "        if i == num_hits-2:\n",
    "            a0 = hhh_h.a0.values[0]\n",
    "            a1 = hhh_h.a0.values[num_hits-1]\n",
    "            r0 =  hhh_h.r.values[0]\n",
    "            r1 =  hhh_h.r.values[num_hits-1]\n",
    "            if r0 == r1:\n",
    "                r1 = r1 + 1e-8\n",
    "            slope = (a1-a0)/(r1-r0) \n",
    "            slopes_forward.append(slope)\n",
    "            if debug: print(hhh_h.hit_id.values[num_hits-1], slope, a0)\n",
    "\n",
    "    slopes_backward = np.asarray(slopes_backward)\n",
    "    slopes_backward = np.reshape(slopes_backward, (-1, 1))\n",
    "    slopes_forward = np.asarray(slopes_forward)\n",
    "    slopes_forward = np.reshape(slopes_forward, (-1, 1))\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    X_back = ss.fit_transform(slopes_backward)\n",
    "    X_for = ss.fit_transform(slopes_forward)\n",
    "\n",
    "    cl = DBSCAN(eps=0.0033, min_samples=1)\n",
    "    outlier_labels_backward = cl.fit_predict(X_back)\n",
    "    outlier_labels_forward = cl.fit_predict(X_for)\n",
    "\n",
    "    if debug: print(outlier_labels_backward)\n",
    "    if debug: print(outlier_labels_forward)\n",
    "\n",
    "    track_counts = coll.Counter(outlier_labels_backward).most_common(1)\n",
    "    most_common_id = track_counts[0][0]\n",
    "    most_common_count = track_counts[0][1]\n",
    "\n",
    "    outlier_indices_backward = []\n",
    "    if most_common_count > 1 and len(np.unique(outlier_labels_forward)) < num_hits/2:\n",
    "        for i in np.arange(num_hits-1,-1,-1):\n",
    "            if outlier_labels_backward[i] != most_common_id:\n",
    "                if debug: print(hhh_h.index.values[num_hits-1-i])\n",
    "                outlier_indices_backward.append(hhh_h.index.values[num_hits-1-i])\n",
    "\n",
    "    track_counts = coll.Counter(outlier_labels_forward).most_common(1)\n",
    "    most_common_id = track_counts[0][0]\n",
    "    most_common_count = track_counts[0][1]\n",
    "\n",
    "\n",
    "    outlier_indices_forward = []\n",
    "    if most_common_count > 1 and len(np.unique(outlier_labels_forward)) < num_hits/2:\n",
    "        for i in np.arange(0,num_hits-1,1):\n",
    "            if outlier_labels_forward[i] != most_common_id:\n",
    "                if debug: print(hhh_h.index.values[i])\n",
    "                outlier_indices_forward.append(hhh_h.index.values[i])\n",
    "\n",
    "\n",
    "    outlier_candidates = list(set(outlier_indices_backward).intersection(outlier_indices_forward))\n",
    "\n",
    "\n",
    "    if debug: print('before removal:' + str(outlier_candidates))\n",
    "\n",
    "    for i in range(len(outlier_candidates)):\n",
    "        candidate = hhh_h.loc[outlier_candidates[i]]\n",
    "        found = False\n",
    "        for index, row in hhh_h.iterrows():\n",
    "            if np.absolute(candidate.z-row.z) == 0.5 and candidate.volume_id == row.volume_id \\\n",
    "            and candidate.layer_id == row.layer_id and candidate.module_id != row.module_id:\n",
    "                # true hits\n",
    "                if debug: print('true hit' + str(outlier_candidates[i]))\n",
    "                found = True\n",
    "        if found is False:\n",
    "            final_outliers.append(outlier_candidates[i])\n",
    "\n",
    "    if debug: print('new loutliers:' + str(final_outliers))\n",
    "\n",
    "    # If we determine that half (or more) of the hits need to be removed, we may have messed\n",
    "    # up, so do not return any outliers.\n",
    "    max_removal_threshold = math.floor(num_hits/2)\n",
    "    if len(final_outliers) >= max_removal_threshold:\n",
    "        final_outliers = []\n",
    "\n",
    "    return final_outliers\n",
    "\n",
    "    \n",
    "def remove_track_outliers(track, labels, hits, aggressive):\n",
    "    labels = np.copy(labels)\n",
    "    found_bad_volume = 0\n",
    "    found_bad_dimension = 0\n",
    "    found_bad_slope = 0\n",
    "    found_bad_z = 0\n",
    "\n",
    "    # Check if the sorted hits (on z-axis) go through the volumes\n",
    "    # and layers in the expected order\n",
    "    bad_volume_ix = find_invalid_volumes(track, labels, hits)\n",
    "    if len(bad_volume_ix) > 0:\n",
    "        #print('track ' + str(track) + ' bad volume: ' + str(bad_volume_ix))\n",
    "        found_bad_volume = found_bad_volume + len(bad_volume_ix)\n",
    "        for bvix in bad_volume_ix:\n",
    "            labels[bvix] = 0\n",
    "\n",
    "    if True:\n",
    "        # Check if the sorted hits (on z-axis) go through the volumes\n",
    "        # and layers in the expected order\n",
    "        duplicatez_ix = find_duplicate_z(track, labels, hits)\n",
    "        if len(duplicatez_ix) > 0:\n",
    "            #print('track ' + str(track) + ' duplicate z: ' + str(duplicatez_ix))\n",
    "            found_bad_z = found_bad_z + len(duplicatez_ix)\n",
    "            for bzix in duplicatez_ix:\n",
    "                labels[bzix] = 0\n",
    "\n",
    "    if True:\n",
    "        # Check the helix slope, discard hits that do not match\n",
    "        outlier_slope_ix = remove_track_outliers_slope(track, labels, hits)\n",
    "        if len(outlier_slope_ix) > 0:\n",
    "            #print('track ' + str(track) + ' slope outliers: ' + str(outlier_slope_ix))\n",
    "            found_bad_slope = found_bad_slope + len(outlier_slope_ix)\n",
    "            for oix in outlier_slope_ix:\n",
    "                labels[oix] = 0\n",
    "\n",
    "    if aggressive:\n",
    "        # Next analysis, from remaining hits, sort by 'z' (roughly time-based),\n",
    "        # check for anomolies in other dimensions.\n",
    "        outlier_ix = find_dimension_outlier(track, labels, hits, 'y')\n",
    "        if len(outlier_ix) > 0:\n",
    "            #print('track ' + str(track) + ' outlier dimension y: ' + str(outlier_ix))\n",
    "            found_bad_dimension = found_bad_dimension + len(outlier_ix)\n",
    "            for oix in outlier_ix:\n",
    "                labels[oix] = 0\n",
    "\n",
    "        # Next analysis, from remaining hits, sort by 'z' (roughly time-based),\n",
    "        # check for anomolies in z dimensions (i.e. outliers at beginning/end)\n",
    "        outlier_ix = find_dimension_outlier(track, labels, hits, 'z')\n",
    "        if len(outlier_ix) > 0:\n",
    "            #print('track ' + str(track) + ' outlier dimension z: ' + str(outlier_ix))\n",
    "            found_bad_dimension = found_bad_dimension + len(outlier_ix)\n",
    "            for oix in outlier_ix:\n",
    "                labels[oix] = 0\n",
    "            \n",
    "    return (labels, found_bad_volume, found_bad_dimension, found_bad_z, found_bad_slope)\n",
    "\n",
    "def remove_outliers(labels, hits, aggressive=False, print_counts=True):\n",
    "    tracks = np.unique(labels)\n",
    "    hits['z_abs'] = hits.z.abs()\n",
    "    hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "    hits['a0'] = np.arctan2(hits.y,hits.x)\n",
    "    count_rem_volume = 0\n",
    "    count_rem_dimension = 0\n",
    "    count_duplicatez = 0\n",
    "    count_rem_slope = 0\n",
    "    count_singletons = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 3:\n",
    "            (labels, c1, c2, c3, c4) = remove_track_outliers(track, labels, hits, aggressive)\n",
    "            count_rem_volume = count_rem_volume + c1\n",
    "            count_rem_dimension = count_rem_dimension + c2\n",
    "            count_duplicatez = count_duplicatez + c3\n",
    "            count_rem_slope = count_rem_slope + c4\n",
    "\n",
    "    # Remove singletons, we do not get any score for those. This is done\n",
    "    # last, in case removing the outliers (above) removed enough hits\n",
    "    # from a track to create a new singleton.\n",
    "    tracks = np.unique(labels)\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) == 1:\n",
    "            count_singletons = count_singletons + 1\n",
    "            labels[track_hits[0]] = 0\n",
    "\n",
    "    if print_counts:\n",
    "        print('Total removed due to bad volumes: ' + str(count_rem_volume))\n",
    "        print('Total removed due to bad dimensions: ' + str(count_rem_dimension))\n",
    "        print('Total removed due to duplicate zs: ' + str(count_duplicatez))\n",
    "        print('Total removed due to bad slopes: ' + str(count_rem_slope))\n",
    "        print('Total removed singleton hits: ' + str(count_singletons))\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_h3 = np.copy(labels_helix)\n",
    "\n",
    "#one_submission = create_one_event_submission(1000, hits, labels_h3)\n",
    "#score = score_event(truth, one_submission)\n",
    "#print(\"Initial score before outlier removal for event %d: %.8f\" % (1000, score))\n",
    "\n",
    "labels_h3 = remove_outliers(labels_h3, hits)\n",
    "one_submission = create_one_event_submission(1000, hits, labels_h3)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Initial score after outlier removal for event %d: %.8f\" % (1000, score))\n",
    "\n",
    "# before score: 0.57485796\n",
    "# after old outlier removal: 0.57471551\n",
    "# after slope removal: 0.56906582 (1006 removed due to bad slopes)\n",
    "# after slope removal + threshold: 0.56966146 (962 removed due to bad slopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with the track number to find a track that contains multiple particles\n",
    "hhh_ix = np.where(labels_helix == 5944)\n",
    "print(hhh_ix)\n",
    "hhh_t = truth.loc[hhh_ix]\n",
    "hhh_t = hhh_t.sort_values('tz')\n",
    "print(hhh_t)\n",
    "\n",
    "# Now add r/a0 as columns to the hits, and see if there's a way to detect\n",
    "# which hits belong to the track, and which should be considered outliers.\n",
    "hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "hits['a0'] = np.arctan2(hits.y,hits.x)\n",
    "hhh_h = hits.loc[hhh_ix]\n",
    "\n",
    "\n",
    "#hhh_h = hhh_h[(hhh_h.hit_id != 15459) & (hhh_h.hit_id != 16190) & (hhh_h.hit_id != 16155) ]\n",
    "\n",
    "hhh_h = hhh_h.sort_values('z')\n",
    "print(hhh_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dupz -> only remove when value ends in .0000 or .5000?\n",
    "# 59828: a: 0,025628, r: 13,745506, slope=a/r=0,001864463920062, 59825: a: 0,025181, r: 13,590172, slope=0,001852883098168\n",
    "# 67285: r: -35,053009, a: -0,072102, slope=0,002056941816322, 67284: r: -46,909088, a: -0,088433, slope=0,001885199729315\n",
    "# 59036: r: 25,316536, a: 0,041827, slope=0,001652161259344, 59007: r: 11,899605, a: 0,019226, 0,001615683881944\n",
    "# Ones that we had trouble with for slope outlier detection:\n",
    "#x 11/6 hits - track 1373 outliers: [24207, 39572, 39574, 48338, 78104, 78108] should remove [24207, 39572, 39574]\n",
    "#y 17/7 hits - track 1644 outliers: [21896, 29838, 36816, 89394, 89395, 111614, 95550] should remove: [89393, 89395]\n",
    "# 19/7 hits - track 1698 outliers: [80388, 73708, 42159, 73717, 86837, 86838, 41657] should remove: [86838, 86387, 80388, 73708, 42159]\n",
    "# 14/5 hits - track 1902 outliers: [23136, 37256, 116299, 30509, 116560] should remove: [116561]\n",
    "#x 15/9 hits - track 1963 outliers: [30432, 37195, 43502, 23025, 75475, 111219, 111222, 37180, 30367] should remove: [111222]\n",
    "#y 13/5 hits - track 2390 outliers: [76962, 84260, 84264, 45130, 45486] should remove: [45128, 45130, 45486, 76962, 84260, 84264, 91275]\n",
    "# --> actually, really good! this was a very complicated case.\n",
    "#x 20/11 hits - track 2445 outliers: [70818, 65315, 69259, 67852, 67858, 69236, 105080, 70776, 66589, 69246, 69247] should remove: [67852, 69246, 69236, 69259, 70776, 71464]\n",
    "# 16/6 hits - track 2522 outliers: [44685, 44686, 83245, 76114, 21534, 76126] should remove:  [21534, 21672, 44209, 44686, 76114, 83245, 90582]\n",
    "# 15/5 hits - track 3417 outliers: [28458, 81457, 81464, 80985, 35775] should remove: [81457, 80985]\n",
    "#x 15/8 hits - track 3427 outliers: [101154, 39335, 98506, 120427, 24496, 119921, 31986, 84925] should remove: []\n",
    "#y 15/6 hits - track 4225 outliers: [119013, 118535, 31339, 45003, 76853, 38453] should remove: [118535]\n",
    "#y 13/5 hits - track 4318 outliers: [84482, 45732, 77481, 77200, 77203] should remove: [77200, 77481, 84482, 84707, 119929]\n",
    "# 18/5 hits - track 5047 outliers: [44323, 44330, 44331, 30926, 37591] should remove: [22142, 30926, 37591, 44319, 44331, 44323, 44330]\n",
    "# 13/5 hits - track 5101 outliers: [117442, 22595, 22833, 111923, 95867] should remove   [117444]\n",
    "#xx 13/6 hits - track 5420 outliers: [86917, 92518, 86918, 92523, 108468, 108469] should remove [86918]\n",
    "#y 23/10 hits - track 5422 outliers: [94144, 81504, 88577, 94531, 74492, 88165, 43052, 110419, 21500, 110718] should remove [21650, 21512, 43052, 43060, 74492, 81504, 88165, 110718, 88577, 94531]\n",
    "# 17/5 hits - track 5826 outliers: [43176, 89005, 94905, 88988, 88991] should remove [43170, 43176, 89005, 88991, 88988, 94905, 94878, 111049]\n",
    "# 14/5 hits - track 6244 outliers: [34257, 34706, 34708, 19508, 34264] should remove [79229, 72675, 34708, 34264, 34706, 34697, 19718, 19508]\n",
    "#y 12/5 hits - track 6409 outliers: [35969, 73987, 28709, 73990, 42508] should remove [109342, 109340, 93542, 73987]\n",
    "# 15/6 hits - track 6740 outliers: [38990, 32046, 39374, 23633, 23666, 38997] should remove [23633, 23666, 32046, 38997, 39374]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_outlier_removal(labels, hits, truth):\n",
    "    tracks = np.unique(labels)\n",
    "    hits['z_abs'] = hits.z.abs()\n",
    "    hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "    hits['a0'] = np.arctan2(hits.y,hits.x)\n",
    "    count_removed = 0\n",
    "    count_not_removed = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 3:\n",
    "            outlier_slope_ix = remove_track_outliers_slope(track, labels, hits)\n",
    "            if len(outlier_slope_ix) > 0:\n",
    "                tdf = truth.loc[track_hits]\n",
    "                truth_count = coll.Counter(tdf.particle_id.values).most_common(1)\n",
    "                truth_particle_id = truth_count[0][0]\n",
    "                for out_ix in outlier_slope_ix:\n",
    "                    if tdf.loc[out_ix].particle_id != truth_particle_id:\n",
    "                        labels[out_ix] = 0\n",
    "                        count_removed = count_removed + 1\n",
    "                    else:\n",
    "                        count_not_removed = count_not_removed + 1\n",
    "\n",
    "    print('safe count_removed: ' + str(count_removed))\n",
    "    print('safe count_not_removed: ' + str(count_not_removed))\n",
    "    return labels\n",
    "\n",
    "def perfect_outlier_removal(labels, hits, truth):\n",
    "    tracks = np.unique(labels)\n",
    "    count_removed = 0\n",
    "    count_not_removed = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 3:\n",
    "            tdf = truth.loc[track_hits]\n",
    "            truth_count = coll.Counter(tdf.particle_id.values).most_common(1)\n",
    "            truth_particle_id = truth_count[0][0]\n",
    "            for hit_ix in track_hits:\n",
    "                if tdf.loc[hit_ix].particle_id != truth_particle_id:\n",
    "                    labels[hit_ix] = 0\n",
    "                    count_removed = count_removed + 1\n",
    "                else:\n",
    "                    count_not_removed = count_not_removed + 1\n",
    "\n",
    "    print('perfect count_removed: ' + str(count_removed))\n",
    "    print('perfect count_not_removed: ' + str(count_not_removed))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_h2 = np.copy(labels_helix)\n",
    "labels_h2 = remove_outliers(labels_h2, hits)\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "labels_h3 = safe_outlier_removal(labels_h3, hits, truth)\n",
    "labels_h4 = np.copy(labels_helix)\n",
    "labels_h4 = perfect_outlier_removal(labels_h4, hits, truth)\n",
    "\n",
    "one_submission = create_one_event_submission(1000, hits, labels_helix)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Initial score for event %d: %.8f\" % (1000, score))\n",
    "\n",
    "one_submission = create_one_event_submission(1000, hits, labels_h2)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Score after normal outlier removal for event %d: %.8f\" % (1000, score))\n",
    "\n",
    "one_submission = create_one_event_submission(1000, hits, labels_h3)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Score after safe outlier removal score for event %d: %.8f\" % (1000, score))\n",
    "\n",
    "one_submission = create_one_event_submission(1000, hits, labels_h4)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Score after perfect outlier removal for event %d: %.8f\" % (1000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_hh2 = np.copy(labels_hh)\n",
    "labels_hh2 = remove_outliers(labels_hh2, hits)\n",
    "labels_hh3 = np.copy(labels_hh)\n",
    "labels_hh3 = safe_outlier_removal(labels_hh3, hits, truth)\n",
    "labels_hh4 = np.copy(labels_hh)\n",
    "labels_hh4 = perfect_outlier_removal(labels_hh4, hits, truth)\n",
    "\n",
    "one_submission = create_one_event_submission(1003, hits, labels_hh)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Initial score for event %d: %.8f\" % (1003, score))\n",
    "\n",
    "one_submission = create_one_event_submission(1003, hits, labels_hh2)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Score after normal outlier removal for event %d: %.8f\" % (1003, score))\n",
    "\n",
    "one_submission = create_one_event_submission(1003, hits, labels_hh3)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Score after safe outlier removal score for event %d: %.8f\" % (1003, score))\n",
    "\n",
    "one_submission = create_one_event_submission(1003, hits, labels_hh4)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Score after perfect outlier removal for event %d: %.8f\" % (1003, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple replacement of unclassified hits: 0\n",
      "Similar tracks (no-op): 5598\n",
      "Multiple non-trivial tracks: 5493\n",
      "--> of which partial track ID 0 hits were updated: 0\n",
      "--> of which partial track ID non-0 hits were updated: 15074\n",
      "Tracks to be lengthened: 5693\n",
      "--> of which track ID 0 hits were updated: 0\n",
      "--> of which labels2 unique track lengths were: [ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "Simple replacement of unclassified hits: 0\n",
      "Similar tracks (no-op): 9320\n",
      "Multiple non-trivial tracks: 2954\n",
      "--> of which partial track ID 0 hits were updated: 0\n",
      "--> of which partial track ID non-0 hits were updated: 4835\n",
      "Tracks to be lengthened: 5437\n",
      "--> of which track ID 0 hits were updated: 0\n",
      "--> of which labels2 unique track lengths were: [ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 19]\n",
      "Merge score no removal for event 1003: 0.55489967\n",
      "Total removed due to bad volumes: 0\n",
      "Total removed due to bad dimensions: 0\n",
      "Total removed due to duplicate zs: 223\n",
      "Total removed due to bad slopes: 422\n",
      "Total removed singleton hits: 19779\n",
      "Total removed due to bad volumes: 0\n",
      "Total removed due to bad dimensions: 0\n",
      "Total removed due to duplicate zs: 362\n",
      "Total removed due to bad slopes: 821\n",
      "Total removed singleton hits: 18939\n",
      "Total removed due to bad volumes: 0\n",
      "Total removed due to bad dimensions: 0\n",
      "Total removed due to duplicate zs: 291\n",
      "Total removed due to bad slopes: 613\n",
      "Total removed singleton hits: 19118\n",
      "Simple replacement of unclassified hits: 876\n",
      "Similar tracks (no-op): 5648\n",
      "Multiple non-trivial tracks: 5433\n",
      "--> of which partial track ID 0 hits were updated: 2576\n",
      "--> of which partial track ID non-0 hits were updated: 12379\n",
      "Tracks to be lengthened: 4827\n",
      "--> of which track ID 0 hits were updated: 4853\n",
      "--> of which labels2 unique track lengths were: [ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "Simple replacement of unclassified hits: 370\n",
      "Similar tracks (no-op): 10326\n",
      "Multiple non-trivial tracks: 3264\n",
      "--> of which partial track ID 0 hits were updated: 551\n",
      "--> of which partial track ID non-0 hits were updated: 4898\n",
      "Tracks to be lengthened: 3751\n",
      "--> of which track ID 0 hits were updated: 2894\n",
      "--> of which labels2 unique track lengths were: [ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]\n",
      "Merge score outlier removal for event 1003: 0.55927400\n"
     ]
    }
   ],
   "source": [
    "labels_h1 = np.copy(labels_helix1)\n",
    "labels_h2 = np.copy(labels_helix2)\n",
    "labels_h3 = np.copy(labels_helix3)\n",
    "labels_merged = merge.heuristic_merge_tracks(labels_h1, labels_h2)\n",
    "labels_merged = merge.heuristic_merge_tracks(labels_merged, labels_h3)\n",
    "one_submission = create_one_event_submission(1003, hits, labels_merged)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merge score no removal for event %d: %.8f\" % (1003, score))\n",
    "\n",
    "labels_hh1 = np.copy(labels_helix1)\n",
    "labels_hh2 = np.copy(labels_helix2)\n",
    "labels_hh3 = np.copy(labels_helix3)\n",
    "labels_hh1 = remove_outliers(labels_hh1, hits)\n",
    "labels_hh2 = remove_outliers(labels_hh2, hits)\n",
    "labels_hh3 = remove_outliers(labels_hh3, hits)\n",
    "labels_merged2 = merge.heuristic_merge_tracks(labels_hh1, labels_hh2)\n",
    "labels_merged = merge.heuristic_merge_tracks(labels_merged2, labels_hh3)\n",
    "one_submission = create_one_event_submission(1003, hits, labels_merged2)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merge score outlier removal for event %d: %.8f\" % (1003, score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_merge1 = merge.heuristic_merge_tracks(labels_helix, labels_hh)\n",
    "one_submission = create_one_event_submission(1003, hits, labels_merge1)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merge score no removal for event %d: %.8f\" % (1003, score))\n",
    "\n",
    "labels_merge2 = merge.heuristic_merge_tracks(labels_h2, labels_hh2)\n",
    "one_submission = create_one_event_submission(1003, hits, labels_merge2)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merge score normal outlier removal for event %d: %.8f\" % (1003, score))\n",
    "\n",
    "labels_merge3 = merge.heuristic_merge_tracks(labels_h3, labels_hh3)\n",
    "one_submission = create_one_event_submission(1003, hits, labels_merge3)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merge score safe removal for event %d: %.8f\" % (1003, score))\n",
    "\n",
    "labels_merge4 = merge.heuristic_merge_tracks(labels_h4, labels_hh4)\n",
    "one_submission = create_one_event_submission(1003, hits, labels_merge4)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merge score perfect removal for event %d: %.8f\" % (1003, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_q1 = np.copy(labels_helix)\n",
    "labels_q1 = remove_outliers(labels_q1, hits)\n",
    "\n",
    "labels_q2 = np.copy(labels_hh)\n",
    "labels_q2 = remove_outliers(labels_q2, hits)\n",
    "\n",
    "labels_mergeq1 = merge.heuristic_merge_tracks(labels_q1, labels_q2)\n",
    "one_submission = create_one_event_submission(1003, hits, labels_mergeq1)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merge score normal no-slope removal for event %d: %.8f\" % (1003, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO pi, -pi discontinuity \n",
    "def remove_track_outliers_slope(track, labels, hits, debug=False):\n",
    "    hhh_ix = np.where(labels == track)\n",
    "    hhh_h = hits.loc[hhh_ix].sort_values('z')\n",
    "    \n",
    "    slopes_backward = []\n",
    "    slopes_forward = []\n",
    "\n",
    "\n",
    "    num_hits = len(hhh_h)\n",
    "    if debug: print('backward:')\n",
    "    for i in np.arange(num_hits-1,0,-1):\n",
    "        a0 =  hhh_h.a0.values[i]\n",
    "        a1 =  hhh_h.a0.values[i-1]\n",
    "        r0 =  hhh_h.r.values[i]\n",
    "        r1 =  hhh_h.r.values[i-1]\n",
    "        if r0 == r1:\n",
    "            r0 = r0 + 1e-8\n",
    "        slope = (a0-a1)/(r0-r1) \n",
    "        slopes_backward.append(slope)\n",
    "        if debug: print(hhh_h.hit_id.values[i], slope, a0)\n",
    "        if i == 1:\n",
    "            a0 = hhh_h.a0.values[0]\n",
    "            a1 = hhh_h.a0.values[num_hits-1]\n",
    "            r0 =  hhh_h.r.values[0]\n",
    "            r1 =  hhh_h.r.values[num_hits-1]\n",
    "            if r0 == r1:\n",
    "                r0 = r0 + 1e-8\n",
    "            slope = (a0-a1)/(r0-r1)\n",
    "            slopes_backward.append(slope)\n",
    "            if debug: print(hhh_h.hit_id.values[0], slope, a1)\n",
    "\n",
    "    if debug: print('forward:')\n",
    "    for i in np.arange(0,num_hits-1,1):\n",
    "        a0 =  hhh_h.a0.values[i]\n",
    "        a1 =  hhh_h.a0.values[i+1]\n",
    "        r0 =  hhh_h.r.values[i]\n",
    "        r1 =  hhh_h.r.values[i+1]\n",
    "        if r0 == r1:\n",
    "            r1 = r1 + 1e-8\n",
    "        slope = (a1-a0)/(r1-r0) \n",
    "        slopes_forward.append(slope)\n",
    "        if debug: print(hhh_h.hit_id.values[i], slope, a0)\n",
    "\n",
    "        if i == num_hits-2:\n",
    "            a0 = hhh_h.a0.values[0]\n",
    "            a1 = hhh_h.a0.values[num_hits-1]\n",
    "            r0 =  hhh_h.r.values[0]\n",
    "            r1 =  hhh_h.r.values[num_hits-1]\n",
    "            if r0 == r1:\n",
    "                r1 = r1 + 1e-8\n",
    "            slope = (a1-a0)/(r1-r0) \n",
    "            slopes_forward.append(slope)\n",
    "            if debug: print(hhh_h.hit_id.values[num_hits-1], slope, a0)\n",
    "\n",
    "    slopes_backward = np.asarray(slopes_backward)\n",
    "    slopes_backward = np.reshape(slopes_backward, (-1, 1))\n",
    "    slopes_forward = np.asarray(slopes_forward)\n",
    "    slopes_forward = np.reshape(slopes_forward, (-1, 1))\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    X_back = ss.fit_transform(slopes_backward)\n",
    "    X_for = ss.fit_transform(slopes_forward)\n",
    "\n",
    "    cl = DBSCAN(eps=0.0033, min_samples=1)\n",
    "    outlier_labels_backward = cl.fit_predict(X_back)\n",
    "    outlier_labels_forward = cl.fit_predict(X_for)\n",
    "\n",
    "    if debug: print(outlier_labels_backward)\n",
    "    if debug: print(outlier_labels_forward)\n",
    "\n",
    "    track_counts = coll.Counter(outlier_labels_backward).most_common(1)\n",
    "    most_common_id = track_counts[0][0]\n",
    "    most_common_count = track_counts[0][1]\n",
    "\n",
    "    outlier_indices_backward = []\n",
    "    if most_common_count > 1 and len(np.unique(outlier_labels_forward)) < num_hits/2:\n",
    "        for i in np.arange(num_hits-1,-1,-1):\n",
    "            if outlier_labels_backward[i] != most_common_id:\n",
    "                if debug: print(hhh_h.index.values[num_hits-1-i])\n",
    "                outlier_indices_backward.append(hhh_h.index.values[num_hits-1-i])\n",
    "\n",
    "    track_counts = coll.Counter(outlier_labels_forward).most_common(1)\n",
    "    most_common_id = track_counts[0][0]\n",
    "    most_common_count = track_counts[0][1]\n",
    "\n",
    "\n",
    "    outlier_indices_forward = []\n",
    "    if most_common_count > 1 and len(np.unique(outlier_labels_forward)) < num_hits/2:\n",
    "        for i in np.arange(0,num_hits-1,1):\n",
    "            if outlier_labels_forward[i] != most_common_id:\n",
    "                if debug: print(hhh_h.index.values[i])\n",
    "                outlier_indices_forward.append(hhh_h.index.values[i])\n",
    "\n",
    "\n",
    "    outlier_candidates = list(set(outlier_indices_backward).intersection(outlier_indices_forward))\n",
    "    final_outliers = []\n",
    "\n",
    "    if debug: print('before removal:' + str(outlier_candidates))\n",
    "\n",
    "    for i in range(len(outlier_candidates)):\n",
    "        candidate = hhh_h.loc[outlier_candidates[i]]\n",
    "        found = False\n",
    "        for index, row in hhh_h.iterrows():\n",
    "            if np.absolute(candidate.z-row.z) == 0.5 and candidate.volume_id == row.volume_id \\\n",
    "            and candidate.layer_id == row.layer_id and candidate.module_id != row.module_id:\n",
    "                # true hits\n",
    "                if debug: print('true hit' + str(outlier_candidates[i]))\n",
    "                found = True\n",
    "        if found is False:\n",
    "            final_outliers.append(outlier_candidates[i])\n",
    "\n",
    "    if debug: print('new loutliers:' + str(final_outliers))\n",
    "\n",
    "    return final_outliers\n",
    "\n",
    "\n",
    "def remove_outliers_slope(labels, hits):\n",
    "    tracks = np.unique(labels)\n",
    "    count_rem_volume = 0\n",
    "    count_rem_dimension = 0\n",
    "    count_duplicatez = 0\n",
    "    count_singletons = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 4:\n",
    "            outliers = remove_track_outliers_slope(track, labels, hits)\n",
    "            if len(outliers) > 0:\n",
    "                do_something = True\n",
    "                # filter out the outliers\n",
    "            \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_h3 = np.copy(labels_helix)\n",
    "outliers = remove_track_outliers_slope(14, labels_h3, hits)\n",
    "print('Slope outliers: ' + str(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Removing helix outliers')\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "labels_h3 = remove_outliers_slope(labels_h3, hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with the track number to find a track that contains multiple particles\n",
    "hhh_ix = np.where(labels_helix == 14)\n",
    "print(hhh_ix)\n",
    "hhh_t = truth.loc[hhh_ix]\n",
    "hhh_t = hhh_t.sort_values('tz')\n",
    "print(hhh_t)\n",
    "\n",
    "# Now add r/a0 as columns to the hits, and see if there's a way to detect\n",
    "# which hits belong to the track, and which should be considered outliers.\n",
    "hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "hits['a0'] = np.arctan2(hits.y,hits.x)\n",
    "hhh_h = hits.loc[hhh_ix]\n",
    "\n",
    "\n",
    "#hhh_h = hhh_h[(hhh_h.hit_id != 15459) & (hhh_h.hit_id != 16190) & (hhh_h.hit_id != 16155) ]\n",
    "\n",
    "hhh_h = hhh_h.sort_values('z')\n",
    "print(hhh_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Removing cone outliers')\n",
    "labels_c3 = np.copy(labels_cone)\n",
    "labels_c3 = remove_outliers_slope(labels_c3, hits)\n",
    "\n",
    "print('Removing helix outliers')\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "labels_h3 = remove_outliers_slope(labels_h3, hits)\n",
    "\n",
    "print('Merging tracks')\n",
    "tmp_labels = merge.heuristic_merge_tracks(labels_h3, labels_c3)\n",
    "one_submission = create_one_event_submission(1000, hits, tmp_labels)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merged score for event %d: %.8f\" % (1000, score))\n",
    "\n",
    "#Merged score for event 1000: 0.58701361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_c2 = np.copy(labels_cone)\n",
    "labels_c2 = remove_outliers(labels_c2, hits)\n",
    "\n",
    "labels_simple_merge = merge.merge_tracks(labels_helix, labels_c2)\n",
    "one_submission = create_one_event_submission(1000, hits, labels_simple_merge)\n",
    "#one_submission = create_one_event_submission(1000, hits, labels_c2)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Outlier removal score for cone event %d: %.8f\" % (1000, score))\n",
    "# Orig Score no removals: 0.26152679\n",
    "# With outlier removal:\n",
    "# volume: 1231\n",
    "# bad dim: 6733\n",
    "# score: 0.18735180\n",
    "# With volume removal: 3520, score: 0.23268937\n",
    "# With volume removal, treat -ve+8--> +ve: 2841, score: 0.23394451\n",
    "# With ignore of 8, otherwise full checks: 1056, score: 0.25499520\n",
    "# With ignore of 8, light checks: 1, score: 0.26152679\n",
    "# Bad volume removal: 1, Bad dims: 222, score: 0.25587569, merged: 0.48291748\n",
    "# Bad volume removal: 1, Bad dims: 220, duplicatez: 1675, score: 0.25657711, merged: 0.48742414\n",
    "# HELIX: Bad vol: 15, score: 0.51204521\n",
    "# HELIX: Bad vol: 15, duplicatez: 399, score: 0.51065321\n",
    "# HELIX: Bad vol: 15, duplicatez: 399, bad dim: 286, score: 0.49635631\n",
    "# HELIX: Bad vol: 15, duplicatez: 399, bad dim: 1518, score: 0.50364853\n",
    "# CONE: Bad vol: 0, duplicatez: 1675, bad dim: 1239, score: 0.25871803, merged: 0.48569633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_labels = merge.heuristic_merge_tracks(labels_helix, labels_c2)\n",
    "one_submission = create_one_event_submission(1000, hits, tmp_labels)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"cone score for event %d: %.8f\" % (1000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_h2 = np.copy(labels_helix)\n",
    "labels_h2 = remove_outliers(labels_h2, hits)#, aggressive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_to_remove = 157\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "xxx_ix = np.where(labels_h3 == track_to_remove)[0]\n",
    "print('xxx_ix: ' + str(xxx_ix))\n",
    "(labels_h3x, _, _, _) = remove_track_outliers(track_to_remove, labels_h3, hits, False)#, True)\n",
    "xxx_ixx = np.where(labels_h3x == track_to_remove)[0]\n",
    "print('xxx_ixx: ' + str(xxx_ixx))\n",
    "#(labels_h3, cx, cy, cz) = remove_track_outliers(track_to_remove, labels_c3, hits)\n",
    "#labels_c3x = np.copy(labels_c3)\n",
    "#one_submission = create_one_event_submission(1000, hits, labels_c3)\n",
    "#score = score_event(truth, one_submission)\n",
    "#print(\"Outlier removal score for cone event %d: %.8f\" % (1000, score))\n",
    "# rem 0,0, score 0.26152679\n",
    "#xxx_ix = np.where(labels_c3 == track_to_remove)[0]\n",
    "#print('xxx_ix: ' + str(xxx_ix))\n",
    "xxx_df = truth.loc[xxx_ix]\n",
    "xxx_df = xxx_df.sort_values('tz')\n",
    "xxx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_to_remove = 157\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "xxx_ix = np.where(labels_h3 == track_to_remove)[0]\n",
    "print('xxx_ix: ' + str(xxx_ix))\n",
    "(labels_h3x, _, _, _) = remove_track_outliers(track_to_remove, labels_h3, hits, False)#, True)\n",
    "xxx_ixx = np.where(labels_h3x == track_to_remove)[0]\n",
    "print('xxx_ixx: ' + str(xxx_ixx))\n",
    "#(labels_h3, cx, cy, cz) = remove_track_outliers(track_to_remove, labels_c3, hits)\n",
    "#labels_c3x = np.copy(labels_c3)\n",
    "#one_submission = create_one_event_submission(1000, hits, labels_c3)\n",
    "#score = score_event(truth, one_submission)\n",
    "#print(\"Outlier removal score for cone event %d: %.8f\" % (1000, score))\n",
    "# rem 0,0, score 0.26152679\n",
    "#xxx_ix = np.where(labels_c3 == track_to_remove)[0]\n",
    "#print('xxx_ix: ' + str(xxx_ix))\n",
    "xxx_df = hits.loc[xxx_ix]\n",
    "xxx_df = xxx_df.sort_values('z')\n",
    "xxx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr2_df = truth.loc[truth['particle_id'] == 306250134780379136]\n",
    "tr2_df = tr2_df.sort_values('tz')\n",
    "tr2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr2_df = truth.loc[truth['particle_id'] == 49542619558051840]\n",
    "tr2_df = tr2_df.sort_values('tz')\n",
    "tr2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helidx = np.where(labels_helix==7095)[0]\n",
    "print(helidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_submission = create_one_event_submission(1000, hits, labels_cone)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Outlier removal score for cone event %d: %.8f\" % (1000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_c2 = np.copy(labels_cone)\n",
    "#trks2 = np.unique(labels_c2)\n",
    "#count_rem_volume = 0\n",
    "#count_rem_dimension = 0\n",
    "#for trk2 in trks2:\n",
    "#    if trk2 == 0:\n",
    "#        continue\n",
    "#    trk2_hits = np.where(labels_c2 == trk2)[0]\n",
    "#    if len(trk2_hits) > 3:\n",
    "#        (labels_c2, c1, c2) = remove_track_outliers(trk2, labels_c2, hits)\n",
    "#        count_rem_volume = count_rem_volume + c1\n",
    "#        count_rem_dimension = count_rem_dimension + c2\n",
    "\n",
    "#print('Total removed due to bad volumes: ' + str(count_rem_volume))\n",
    "#print('Total removed due to bad dimensions: ' + str(count_rem_dimension))\n",
    "\n",
    "track_to_remove = 63542\n",
    "labels_c3 = np.copy(labels_cone)\n",
    "xxx_ix = np.where(labels_c3 == track_to_remove)[0]\n",
    "print('xxx_ix: ' + str(xxx_ix))\n",
    "(labels_c3, cx, cy, cz) = remove_track_outliers(track_to_remove, labels_c3, hits)\n",
    "labels_c3x = np.copy(labels_c3)\n",
    "one_submission = create_one_event_submission(1000, hits, labels_c3)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Outlier removal score for cone event %d: %.8f\" % (1000, score))\n",
    "# rem 0,0, score 0.26152679\n",
    "xxx_ix = np.where(labels_c3 == track_to_remove)[0]\n",
    "print('xxx_ix: ' + str(xxx_ix))\n",
    "xxx_df = hits.loc[xxx_ix]\n",
    "xxx_df = xxx_df.sort_values('z')\n",
    "# Bad indexes: 59855, 61697\n",
    "print(xxx_df)\n",
    "xxx_ix = np.where(labels_cone == 63542)\n",
    "xxx_df = hits.loc[xxx_ix]\n",
    "xxx_df = xxx_df.sort_values('z')\n",
    "# indexes in question: ???\n",
    "xxx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_helix = merge.renumber_labels(labels_helix)\n",
    "max_track = np.amax(labels_helix)\n",
    "labels_cone[labels_cone != 0] = labels_cone[labels_cone != 0] + max_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsx = np.copy(labels_cone)\n",
    "trackx = 63949\n",
    "outx = find_dimension_outlier(trackx, labelsx, hits, 'y')\n",
    "print('outx: ' + str(outx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx_ix = np.where(labels_cone == 63542)\n",
    "xxx_df = hits.loc[xxx_ix]\n",
    "xxx_df = xxx_df.sort_values('z')\n",
    "# indexes in question: ???\n",
    "xxx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = truth.loc[xxx_ix]\n",
    "tr_df = tr_df.sort_values('tz')\n",
    "# BAD: 59855, 61697\n",
    "# sorted z, see steps for 'y', ensure they are consistent (good for 59855, ok for 61697)\n",
    "# --> if most in same direction except one/two outliers, and outlier magnitude is large....\n",
    "all_y1 = tr_df.ty.values\n",
    "all_y = np.diff(all_y1)\n",
    "print('mean: ' + str(all_y.mean()))\n",
    "print('max: ' + str(all_y.max()))\n",
    "print('min: ' + str(all_y.min()))\n",
    "#[49193, 59886, 61710]\n",
    "tr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr2_df = truth.loc[truth['particle_id'] == 166642325903114240]\n",
    "tr2_df = tr2_df.sort_values('tz')\n",
    "tr2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track 150 outlier dimension y: [100706]\n",
    "trk large -: 160, large1: -1.592, large2: -61.533\n",
    "track 160 outlier dimension y: [41261]\n",
    "track 169 outlier dimension y: [17867]\n",
    "track 181 outlier dimension y: [103556]\n",
    "track 188 outlier dimension y: [661]\n",
    "track 194 outlier dimension y: [40343]\n",
    "trk large -: 208, large1: -1.2258, large2: -30.6159\n",
    "track 208 outlier dimension y: [23693]\n",
    "trk large -: 223, large1: -1.3697, large2: -36.1324\n",
    "track 223 outlier dimension y: [29081]\n",
    "trk large +: 263, large1: 38.0123, large2: 1.7837\n",
    "track 263 outlier dimension y: [19765]\n",
    "trk large +: 281, large1: 102.727, large2: 3.78752\n",
    "track 281 outlier dimension y: [100657]\n",
    "track 286 outlier dimension y: [62478]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhh_ix = np.where(labels_helix == 5549)\n",
    "hhh_df = truth.loc[hhh_ix]\n",
    "hhh_df = hhh_df.sort_values('tz')\n",
    "# indexes in question: ???\n",
    "hhh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = truth.loc[hhh_ix]\n",
    "tr_df = tr_df.sort_values('tz')\n",
    "# BAD: 59855, 61697\n",
    "# sorted z, see steps for 'y', ensure they are consistent (good for 59855, ok for 61697)\n",
    "# --> if most in same direction except one/two outliers, and outlier magnitude is large....\n",
    "all_y1 = tr_df.ty.values\n",
    "all_y = np.diff(all_y1)\n",
    "print('mean: ' + str(all_y.mean()))\n",
    "print('max: ' + str(all_y.max()))\n",
    "print('min: ' + str(all_y.min()))\n",
    "#[48366, 48517, 51460, 54360, 54471, 57091, 57133]\n",
    "#[6596, 6539, 4232, 4180]\n",
    "tr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_x = np.array([\n",
    "    [\n",
    "        [-12.166300, -156.817993, 962.000],\n",
    "        [20.349701, -242.162003, 1498.500],\n",
    "        [48.221401, -284.333008, 1795.500],\n",
    "        [48.529099, -284.743988, 1798.500],\n",
    "        [122.458000, -404.139008, 2554.500]\n",
    "    ],\n",
    "    [\n",
    "        [-7.17245, -44.973099, -1098.0],\n",
    "        [-6.67366, -39.278999, -962.5],\n",
    "        [-6.65474, -39.091202, -958.0],\n",
    "        [-6.01885, -33.464001, -822.5],\n",
    "        [-5.99555, -33.277901, -818.0]\n",
    "    ],\n",
    "    [\n",
    "        [-6.829120, 30.917900, 40.626202],\n",
    "        [-28.525200, 169.742004, 212.766998],\n",
    "        [-34.125198, 258.694000, 325.647003],\n",
    "        [-31.138201, 361.954987, 453.306000],\n",
    "        [-12.355800, 503.367004, 629.344971]\n",
    "    ]\n",
    "])\n",
    "\n",
    "track_y = np.array([\n",
    "    [1, 1, 1, 1, 0],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [0, 0, 1, 1, 1]\n",
    "])\n",
    "print(track_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = np.unique(labels_h3)\n",
    "track_x = np.zeros((len(tracks), 10, 3))\n",
    "track_y = np.zeros((len(tracks), 10, 1))\n",
    "max_xval = np.amax(hits.x.values)\n",
    "print(max_xval)\n",
    "max_yval = np.amax(hits.y.values)\n",
    "print(max_yval)\n",
    "max_zval = np.amax(hits.z.values)\n",
    "print(max_zval)\n",
    "for idx, track in enumerate(tracks):\n",
    "    if track == 0:\n",
    "        continue\n",
    "    hit_ix = np.where(labels_h3 == track)[0]\n",
    "    df2 = hits.loc[hit_ix]\n",
    "    df2 = df2.sort_values('z_abs')\n",
    "    hit_ix2 = df2.index.values # remember new indexes after sorting\n",
    "    #print(df2)\n",
    "    xs = df2.x.values\n",
    "    ys = df2.y.values\n",
    "    zs = df2.z.values\n",
    "\n",
    "    # From this track, figure out the most common particle ID from the truth.\n",
    "    # Any hits from our track that belong to that particle will be set to '1'\n",
    "    # (correctly predicted hit), otherwise '0' (outlier).\n",
    "    tf2 = truth.loc[hit_ix]\n",
    "    counters = coll.Counter\n",
    "    track_counts = coll.Counter(tf2.particle_id.values).most_common(len(hit_ix2))\n",
    "    track_particle_id = track_counts[0][0]\n",
    "\n",
    "    for i in range(len(hit_ix2)):\n",
    "        if i < 10:\n",
    "            track_x[idx][i][0] = xs[i] / max_xval\n",
    "            track_x[idx][i][1] = ys[i] / max_yval\n",
    "            track_x[idx][i][2] = zs[i] / max_zval\n",
    "            if (truth.loc[hit_ix2[i]].particle_id == track_particle_id):\n",
    "                track_y[idx][i][0] = 3\n",
    "            else:\n",
    "                track_y[idx][i][0] = 30\n",
    "            #track_y[idx][i][0] = (truth.loc[hit_ix2[i]].particle_id == track_particle_id)\n",
    "            \n",
    "    #if idx < 10:\n",
    "    #    ignore_it = True\n",
    "    #elif idx < 20:\n",
    "    #    print(tf2)\n",
    "    #    print(df2)\n",
    "    #    print(track_x[idx])\n",
    "    #    print(track_y[idx])\n",
    "    #else:\n",
    "    #    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as L\n",
    "import keras.models as M\n",
    "\n",
    "# The inputs to the model.\n",
    "# We will create two data points, just for the example.\n",
    "data_x = np.array([\n",
    "    # Datapoint 1\n",
    "#    [\n",
    "#        # Input features at timestep 1\n",
    "#        [1, 2, 3],\n",
    "#        # Input features at timestep 2\n",
    "#        [4, 5, 6]\n",
    "#    ],\n",
    "    # Datapoint 2\n",
    "    [\n",
    "        # Features at timestep 1\n",
    "        [7, 8, 9],\n",
    "        # Features at timestep 2\n",
    "        [10, 11, 12]\n",
    "    ]\n",
    "])\n",
    "\n",
    "# The desired model outputs.\n",
    "# We will create two data points, just for the example.\n",
    "data_y = np.array([\n",
    "    # Datapoint 1\n",
    "    # Target features at timestep 2\n",
    "#    [105, 106, 107, 108, 109],\n",
    "    # Datapoint 2\n",
    "    # Target features at timestep 2\n",
    "    [205, 206, 207, 208, 209]\n",
    "])\n",
    "\n",
    "# Each input data point has 2 timesteps, each with 3 features.\n",
    "# So the input shape (excluding batch_size) is (2, 3), which\n",
    "# matches the shape of each data point in data_x above.\n",
    "model_input = L.Input(shape=(10, 3))\n",
    "\n",
    "# This RNN will return timesteps with 4 features each.\n",
    "# Because return_sequences=True, it will output 2 timesteps, each\n",
    "# with 4 features. So the output shape (excluding batch size) is\n",
    "# (2, 4), which matches the shape of each data point in data_y above.\n",
    "#model_output = L.LSTM(4, return_sequences=False)(model_input)\n",
    "#model_output = L.LSTM(100, return_sequences=True)(model_input)\n",
    "#model_output = L.Dense(100, activation='linear')(model_output)\n",
    "#model_output = L.Dense(4, activation='linear')(model_output)\n",
    "#model_output = L.LSTM(4, return_sequences=False)(model_output)\n",
    "model_output = L.Bidirectional(L.LSTM(10, return_sequences=True, stateful=False))(model_input)\n",
    "#model_output = L.LSTM(30, return_sequences=True, stateful=False)(model_output)\n",
    "model_output = L.Bidirectional(L.LSTM(10, return_sequences=True, stateful=False))(model_output)\n",
    "model_output = L.TimeDistributed(L.Dense(1, activation='linear'))(model_output)\n",
    "#model_output = L.Dense(100, activation='linear')(model_output)\n",
    "#model_output = L.Dense(30, activation='linear')(model_output)\n",
    "# Create the model.\n",
    "model = M.Model(inputs=model_input, outputs=model_output)\n",
    "\n",
    "# You need to pick appropriate loss/optimizers for your problem.\n",
    "# I'm just using these to make the example compile.\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train\n",
    "#model.fit(data_x, data_y)\n",
    "#model.fit(track_x, track_y)\n",
    "\n",
    "# batch_size=3\n",
    "# num_steps=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for e in range(100):\n",
    "#    #for i in range(track_x.shape[0]):\n",
    "#    #    tx = np.expand_dims(track_x[i], axis=0)\n",
    "#    #    ty = np.expand_dims(track_y[i], axis=0)\n",
    "model.fit(track_x, track_y, batch_size=1, epochs=5, verbose=1)\n",
    "#for i in range(track_x.shape[0]):\n",
    "#        tx = np.expand_dims(track_x[i], axis=0)\n",
    "\n",
    "for i in range(track_x.shape[0]):\n",
    "    tx = np.expand_dims(track_x[i], axis=0)\n",
    "    yhat = model.predict(tx)\n",
    "    ty = track_y[i]\n",
    "    print(ty)\n",
    "    print(yhat)\n",
    "    if i > 10:\n",
    "        break\n",
    "# old - (3 LSTM Layers): Loss: 0.0810\n",
    "# old - (2 LSTM, 2 Dense (100, 30)): Loss ~ 0.08\n",
    "# New TimeDistributed Loss: 0.2072\n",
    "# New normalized TimeDistributed loss: 0.3196\n",
    "# New normalized Bidi-LSTM TimeDistributed loss: 0.3100, 0.1920, 0.1362, 0.1273, 0.1193\n",
    "#  -> same, but with 10-hit track input: 0.5413, 0.4646, 0.4242, 0.3943, 0.3550\n",
    "#  -> 3 for right hit, 10 for outlier, 0 for ignore: 5.1141, 5.0738, 5.0602, 5.0038, 4.6757,\n",
    "#                                                    4.1964, 3.9969, 3.8963, 3.8313, 3.7737\n",
    "#  -> 3 for right hit, 30 for outlier, 0 for ignore: 62.5914"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(track_x.shape[0]):\n",
    "    if i < 1000:\n",
    "        continue\n",
    "    tx = np.expand_dims(track_x[i], axis=0)\n",
    "    yhat = model.predict(tx)\n",
    "    ty = track_y[i]\n",
    "    print('Prediction: ' + str(i))\n",
    "    print(ty)\n",
    "    print(yhat)\n",
    "    if i > 1020:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hits.head())\n",
    "print(len(hits))\n",
    "\n",
    "print(particles.head())\n",
    "print(len(particles))\n",
    "\n",
    "print(cells.head())\n",
    "print(len(cells))\n",
    "\n",
    "print(truth.head())\n",
    "print(len(truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the XY plane\n",
    "g = sns.jointplot(hits.x, hits.y, size=12)\n",
    "\n",
    "#Clear the axes containing the scatter plot\n",
    "g.ax_joint.cla()\n",
    "# Set the current axis to the parent of ax\n",
    "plt.sca(g.ax_joint)\n",
    "\n",
    "volumes = hits.volume_id.unique()\n",
    "for volume in volumes:\n",
    "    v = hits[hits.volume_id == volume]\n",
    "    # scattering the hit coordinates with the particle size = 1\n",
    "    plt.scatter(v.x, v.y, s=1, label='volume {}'.format(volume))\n",
    "\n",
    "plt.xlabel('X (mm)')\n",
    "plt.ylabel('Y (mm)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the YZ plane\n",
    "g = sns.jointplot(hits.z, hits.y, s=1, size=12)\n",
    "g.ax_joint.cla()\n",
    "plt.sca(g.ax_joint)\n",
    "\n",
    "volumes = hits.volume_id.unique()\n",
    "for volume in volumes:\n",
    "    v = hits[hits.volume_id == volume]\n",
    "    plt.scatter(v.z, v.y, s=1, label='volume {}'.format(volume))\n",
    "\n",
    "plt.xlabel('Z (mm)')\n",
    "plt.ylabel('Y (mm)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From XYZ 3D perspective\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for volume in volumes:\n",
    "    v = hits[hits.volume_id == volume]\n",
    "    ax.scatter(v.z, v.x, v.y, s=1, label='volume {}'.format(volume), alpha=0.5)\n",
    "ax.set_title('SHit Locations')\n",
    "ax.set_xlabel('Z (millimeters)')\n",
    "ax.set_ylabel('X (millimeters)')\n",
    "ax.set_zlabel('Y (millimeters)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(particles.nhits.values, axlabel='Hits/Particle', bins=50)\n",
    "plt.title('Distribution of number of hits per particle for event 1000.')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(particles.groupby('q')['vx'].count(),\n",
    "        labels=['negative', 'positive'],\n",
    "        autopct='%.0f%%',\n",
    "        shadow=True,\n",
    "        radius=1)\n",
    "plt.title('Distribution of particle charges.')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the original particles of tracks, most particle collisions are generated from the origin\n",
    "\n",
    "g = sns.jointplot(particles.vz, particles.vy,  s=3, size=12)\n",
    "g.ax_joint.cla()\n",
    "plt.sca(g.ax_joint)\n",
    "\n",
    "n_hits = particles.nhits.unique()\n",
    "for n_hit in n_hits:\n",
    "    p = particles[particles.nhits == n_hit]\n",
    "    plt.scatter(p.vz, p.vy, s=1, label='Hits {}'.format(n_hit))\n",
    "\n",
    "plt.xlabel('Z (mm)')\n",
    "plt.ylabel('Y (mm)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "\n",
    "for charge in [-1, 1]:\n",
    "    q = particles[particles.q == charge]\n",
    "    ax.scatter(q.vz, q.vx, q.vy, s=1, label='Charge {}'.format(charge), alpha=0.5)\n",
    "ax.set_title('Sample of 1000 Particle initial location')\n",
    "ax.set_xlabel('Z (millimeters)')\n",
    "ax.set_ylabel('X (millimeters)')\n",
    "ax.set_zlabel('Y (millimeters)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIT_COUNT = 12\n",
    "particle1 = particles.loc[particles.nhits == HIT_COUNT].iloc[0]\n",
    "particle2 = particles.loc[particles.nhits == HIT_COUNT].iloc[1]\n",
    "particle3 = particles.loc[particles.nhits == HIT_COUNT].iloc[2]\n",
    "\n",
    "\n",
    "p_traj_surface1 = truth[truth.particle_id == particle1.particle_id][['tx', 'ty', 'tz']]\n",
    "p_traj_surface2 = truth[truth.particle_id == particle2.particle_id][['tx', 'ty', 'tz']]\n",
    "p_traj_surface3 = truth[truth.particle_id == particle3.particle_id][['tx', 'ty', 'tz']]\n",
    "\n",
    "\n",
    "\n",
    "p_traj1 = (p_traj_surface1\n",
    "          .append({'tx': particle1.vx, 'ty': particle1.vy, 'tz': particle1.vz}, ignore_index=True)\n",
    "          .sort_values(by='tz'))\n",
    "\n",
    "p_traj2 = (p_traj_surface2\n",
    "          .append({'tx': particle2.vx, 'ty': particle2.vy, 'tz': particle2.vz}, ignore_index=True)\n",
    "          .sort_values(by='tz'))\n",
    "\n",
    "p_traj3 = (p_traj_surface3\n",
    "          .append({'tx': particle3.vx, 'ty': particle3.vy, 'tz': particle3.vz}, ignore_index=True)\n",
    "          .sort_values(by='tz'))\n",
    "\n",
    "\n",
    "# Visualize XY projection to the Z-axis\n",
    "\n",
    "plt.plot(p_traj1.tz, p_traj1.ty, '-o', label='hits')\n",
    "plt.plot(p_traj2.tz, p_traj2.ty, '-o', label='hits')\n",
    "plt.plot(p_traj3.tz, p_traj3.ty, '-o', label='hits')\n",
    "plt.xlabel('Z (mm)')\n",
    "plt.ylabel('Y (mm)')\n",
    "plt.title('ZY projection to the X-axis')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot(\n",
    "    xs=p_traj1.tx,\n",
    "    ys=p_traj1.ty,\n",
    "    zs=p_traj1.tz, marker='o')\n",
    "ax.plot(\n",
    "    xs=p_traj2.tx,\n",
    "    ys=p_traj2.ty,\n",
    "    zs=p_traj2.tz, marker='o')\n",
    "ax.plot(\n",
    "    xs=p_traj3.tx,\n",
    "    ys=p_traj3.ty,\n",
    "    zs=p_traj3.tz, marker='o')\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xlabel('X (mm)')\n",
    "ax.set_ylabel('Y (mm)')\n",
    "ax.set_zlabel('Z  (mm) -- Detection layers')\n",
    "plt.title('Trajectories of two particles as they cross the detection surface ($Z$ axis).')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
