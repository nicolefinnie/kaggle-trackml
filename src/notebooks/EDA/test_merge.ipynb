{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "from trackml.dataset import load_event\n",
    "from trackml.randomize import shuffle_hits\n",
    "from trackml.score import score_event\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import collections as coll\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "import merge as merge\n",
    "import extension as ext\n",
    "import zroutlier as zro\n",
    "import free_hits as free\n",
    "import track_score as score2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../../../input/train_1'\n",
    "event_id = 1029"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event000001029 memory usage 16.61 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "event_prefix = 'event00000' + str(event_id)\n",
    "hits, cells, particles, truth = load_event(os.path.join(TRAIN_PATH, event_prefix))\n",
    "\n",
    "mem_bytes = (hits.memory_usage(index=True).sum() \n",
    "             + cells.memory_usage(index=True).sum() \n",
    "             + particles.memory_usage(index=True).sum() \n",
    "             + truth.memory_usage(index=True).sum())\n",
    "print('{} memory usage {:.2f} MB'.format(event_prefix, mem_bytes / 2**20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_helix1 = pd.read_csv('../../best_csvs12/event_' + str(event_id) + '_labels_train_helix1.csv').label.values\n",
    "#labels_helix2 = pd.read_csv('../../best_csvs12/event_' + str(event_id) + '_labels_train_helix2.csv').label.values\n",
    "#labels_helix1a = pd.read_csv('../../event_' + str(event_id) + '_labels_train_helix1_phase1_dbscan1.csv').label.values\n",
    "#labels_helix2 = pd.read_csv('../../event_' + str(event_id) + '_labels_train_helix1_phase1_dbscan1_processed.csv').label.values\n",
    "labels_helix1 = pd.read_csv('../../event_' + str(event_id) + '_labels_train_helix1.csv').label.values\n",
    "labels_helix2 = pd.read_csv('../../event_' + str(event_id) + '_labels_train_helix2.csv').label.values\n",
    "#labels_helix3 = pd.read_csv('../../best_csvs12/event_' + str(event_id) + '_labels_train_helix3.csv').label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_event_submission(event_id, hits, labels):\n",
    "    sub_data = np.column_stack(([event_id]*len(hits), hits.hit_id.values, labels))\n",
    "    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n",
    "    return submission\n",
    "\n",
    "def score_one_submission(event_id, hits, labels, truth):\n",
    "    submission = create_one_event_submission(event_id, hits, labels)\n",
    "    score = score_event(truth, submission)\n",
    "    print(\"Score for event %d: %.32f\" % (event_id, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_helix1_filter = merge.remove_outliers(labels_helix1, hits, print_counts=True)\n",
    "#labels_helix2_filter = merge.remove_outliers(labels_helix2, hits, print_counts=True)\n",
    "labels_helix1_filter = remove_outliers_zr(labels_helix1, hits)\n",
    "labels_helix1_filter = remove_outliers_zr(labels_helix1_filter, hits)\n",
    "labels_helix1_filter = remove_outliers_zr(labels_helix1_filter, hits)\n",
    "labels_helix2_filter = remove_outliers_zr(labels_helix2, hits)\n",
    "labels_helix2_filter = remove_outliers_zr(labels_helix2_filter, hits)\n",
    "labels_helix2_filter = remove_outliers_zr(labels_helix2_filter, hits)\n",
    "#labels_helix3_filter = merge.remove_outliers(labels_helix3, hits, print_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_merged = merge.heuristic_merge_tracks(labels_helix1_filter, labels_helix2_filter, overwrite_limit=3)\n",
    "#labels_merged = merge.heuristic_merge_tracks(labels_merged, labels_helix3_filter)\n",
    "one_submission = create_one_event_submission(event_id, hits, labels_merged)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merged score for event %d: %.8f\" % (event_id, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_helix1_filter2 = remove_outliers_zr(labels_helix1, hits)\n",
    "labels_helix1_filter3 = remove_outliers_zr(labels_helix1_filter2, hits)\n",
    "#labels_helix1_filter4 = safe_outlier_removal(labels_helix1_filter3, hits, truth, debug=False)\n",
    "labels_helix1_filter4 = remove_outliers_zr(labels_helix1_filter3, hits)\n",
    "labels_helix1_filter5 = merge.remove_outliers(labels_helix1_filter4, hits, print_counts=True)\n",
    "labels_helix2_filter2 = remove_outliers_zr(labels_helix2, hits)\n",
    "labels_helix2_filter3 = remove_outliers_zr(labels_helix2_filter2, hits)\n",
    "labels_helix2_filter4 = remove_outliers_zr(labels_helix2_filter3, hits)\n",
    "#labels_helix2_filter4 = safe_outlier_removal(labels_helix2_filter3, hits, truth, debug=False)\n",
    "labels_helix2_filter5 = merge.remove_outliers(labels_helix2_filter4, hits, print_counts=True)\n",
    "labels_merged = merge.heuristic_merge_tracks(labels_helix1_filter5, labels_helix2_filter5, overwrite_limit=3)\n",
    "one_submission = create_one_event_submission(event_id, hits, labels_merged)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merged score for event %d: %.8f\" % (event_id, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_helix1_filter2 = safe_outlier_removal(labels_helix1, hits, truth, find_all=True, debug=False)\n",
    "#labels_helix1_filter2 = safe_outlier_removal(labels_helix1_filter2, hits, truth, debug=False)\n",
    "#labels_helix2_filter2 = safe_outlier_removal(labels_helix2, hits, truth, debug=False)\n",
    "#labels_helix2_filter2 = safe_outlier_removal(labels_helix2_filter2, hits, truth, debug=False)\n",
    "#labels_helix2_filter2 = safe_outlier_removal(labels_helix2_filter2, hits, truth, debug=False)\n",
    "#labels_helix1_filter3 = safe_outlier_removal(labels_helix1_filter2, hits, truth)\n",
    "# 321 v 167, 394 v 199 for find_all\n",
    "# 2143 v 407, 2634 v 512 for find_all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_circle_curvature(x1, y1, x2, y2, x3, y3):\n",
    "    x01 = d01.x();\n",
    "    y01 = d01.y();\n",
    "    x12 = d12.x();\n",
    "    y12 = d12.y();\n",
    "    x02 = x01 + x12;\n",
    "    y02 = y01 + y12;\n",
    "    # length of the triangle sides\n",
    "    a = std::hypot(x12, y12);\n",
    "    b = std::hypot(x02, y02);\n",
    "    c = std::hypot(x01, y01);\n",
    "    # 2 * (signed) area of the triangle\n",
    "    k = (x02 * y01 - x01 * y02);\n",
    "    # radius = product of side lengths / 4 times triangle area\n",
    "    return (2 * k) / (a * b * c);\n",
    "\n",
    "\n",
    "def estimate_circle_d0(p0, phi0, kappa):\n",
    "    x0_sin = p0.x * std::sin(phi0)\n",
    "    y0_cos = p0.y * std::cos(phi0)\n",
    "    return (y0_cos - x0_sin) + 0.5 * kappa * (p0.x**2 + p0.y**2)\n",
    "        + 0.5 * kappa * (x0_sin**2 + y0_cos**2 - 2 * x0_sin * y0_cos);\n",
    "\n",
    "# Estimate the z position at vanishing transverse radius.\n",
    "def estimate_z_helix_z0(p0, theta):\n",
    "    return p0.z - std::hypot(p0.x, p0.y * std::tan(M_PI_2 - theta);\n",
    "\n",
    "namespace Acts {\n",
    "namespace Seeding {\n",
    "\n",
    "  struct HelixSeedConfig\n",
    "  {\n",
    "    double rangePhi1     = 0.2;  // search range in phi at layer 1\n",
    "    double rangePhi2     = 0.2;  // search range in phi at layer 2\n",
    "    double maxDeltaTheta = 0.1;  // cut on difference in theta between doublets\n",
    "  };\n",
    "\n",
    "  /// Find 3-point seeds with a combinatorial algorithm.\n",
    "  template <typename Identifier>\n",
    "  void\n",
    "  findHelixSeeds(const HelixSeedConfig&               cfg,\n",
    "                 const BarrelSpacePoints<Identifier>& barrel0,\n",
    "                 const BarrelSpacePoints<Identifier>& barrel1,\n",
    "                 const BarrelSpacePoints<Identifier>& barrel2,\n",
    "                 TrackSeeds3<Identifier>&             seeds);\n",
    "\n",
    "}  // namespace Seeding\n",
    "}  // namespace Acts\n",
    "\n",
    "template <typename Identifier>\n",
    "inline void\n",
    "Acts::Seeding::findHelixSeeds(const HelixSeedConfig&               cfg,\n",
    "                              const BarrelSpacePoints<Identifier>& barrel0,\n",
    "                              const BarrelSpacePoints<Identifier>& barrel1,\n",
    "                              const BarrelSpacePoints<Identifier>& barrel2,\n",
    "                              TrackSeeds3<Identifier>&             seeds)\n",
    "{\n",
    "  for (const auto& p0 : barrel0.points) {\n",
    "    for (const auto& p1 : barrel1.rangeDeltaPhi(p0.phi(), cfg.rangePhi1)) {\n",
    "      Vector3D d01     = p1.position() - p0.position();\n",
    "      double   theta01 = d01.theta();\n",
    "      // Acts::Vector3D at2\n",
    "      //     = detail::calcLineCircleIntersection(p0, d01, barrel2.radius);\n",
    "      for (const auto& p2 : barrel2.rangeDeltaPhi(p1.phi(), cfg.rangePhi2)) {\n",
    "        Vector3D d12     = p2.position() - p1.position();\n",
    "        double   theta12 = d12.theta();\n",
    "\n",
    "        if (cfg.maxDeltaTheta < std::abs(theta12 - theta01)) continue;\n",
    "\n",
    "        double kappa = detail::calcCircleCurvature(d01, d12);\n",
    "        // initial direction correction due to curvature, use\n",
    "        //   chord = 2 * radius * sin(propagation angle / 2)\n",
    "        // and assume sin(x) = x\n",
    "        double phi01 = d01.phi() - d01.head<2>().norm() * kappa / 2;\n",
    "        // track parameters defined at the first space point\n",
    "        seeds.emplace_back(phi01, theta01, kappa, p0, p1, p2);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total removed due to bad volumes: 0\n",
      "Total removed due to bad zr values: 1079\n",
      "Total removed due to bad dimensions: 0\n",
      "Total removed due to duplicate zs: 2172\n",
      "Total removed due to bad slopes: 949\n",
      "Total removed small tracks (<2) hits: 1084\n",
      "Total removed due to bad volumes: 0\n",
      "Total removed due to bad zr values: 853\n",
      "Total removed due to bad dimensions: 0\n",
      "Total removed due to duplicate zs: 2256\n",
      "Total removed due to bad slopes: 934\n",
      "Total removed small tracks (<2) hits: 100\n",
      "Score for event 1029: 0.69792186236579401015234225269523\n"
     ]
    }
   ],
   "source": [
    "labels_helix1_filter = merge.remove_outliers(labels_helix1, hits, print_counts=True)\n",
    "labels_helix2_filter = merge.remove_outliers(labels_helix2, hits, print_counts=True)\n",
    "labels_merged = merge.heuristic_merge_tracks(labels_helix1_filter, labels_helix2_filter, hits, overwrite_limit=6)\n",
    "score_one_submission(event_id, hits, labels_merged, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_helix1_filter = merge.remove_outliers(labels_helix1, hits, print_counts=True)\n",
    "#labels_helix2_filter = merge.remove_outliers(labels_helix2, hits, print_counts=True)\n",
    "#labels_helix1_filter2 = assign_free_hits(labels_helix1_filter, hits)\n",
    "#labels_helix2_filter2 = assign_free_hits(labels_helix2_filter, hits)\n",
    "labels_merged = merge.heuristic_merge_tracks(labels_helix1_filter, labels_helix2_filter, overwrite_limit=3)\n",
    "one_submission = create_one_event_submission(event_id, hits, labels_merged)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merged score for event %d: %.8f\" % (event_id, score))\n",
    "labels_merged2 = free.assign_free_hits(labels_merged, hits)\n",
    "one_submission = create_one_event_submission(event_id, hits, labels_merged2)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merged free-hit score for event %d: %.8f\" % (event_id, score))\n",
    "#labels_merged2 = np.copy(labels_merged)\n",
    "#labels_merged2 = assign_free_hits(labels_merged2, hits)\n",
    "#one_submission = create_one_event_submission(event_id, hits, labels_merged2)\n",
    "#score = score_event(truth, one_submission)\n",
    "#print(\"Merged score for event %d: %.8f\" % (event_id, score))\n",
    "# zr:     0.68239816, 0.68255797, 0.68265559\n",
    "# crs:    0.68239816, 0.68264529, 0.68269281\n",
    "# crs+zr: 0.68239816, 0.68454839, 0.68476117  (rem 2,3,4)\n",
    "# crs+zr: 0.68239816, 0.68396086, 0.68419647  (rem 1,2,3)\n",
    "# crs+zr: 0.68239816, 0.68471627, 0.68479848  (rem 2,3, no short track favoring)\n",
    "# crs+zr: 0.68239816, 0.68432189, 0.68465735, 0.68476141  (rem 1,2,3, no short track favoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits['z_abs'] = hits.z.abs()\n",
    "hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "hits['zr'] = hits['z'] / hits['r']\n",
    "hits['azr'] = np.arctan2(hits['z'], hits['r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track : 58\n",
      "Track : 121\n",
      "Track : 325\n",
      "Track : 846\n",
      "Track : 2131\n",
      "Track : 2257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/trackml/lib/python3.6/site-packages/ipykernel_launcher.py:82: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/trackml/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track : 2812\n",
      "Track : 3365\n",
      "Track : 3398\n",
      "Track : 5228\n",
      "Track : 5658\n",
      "Track : 5689\n",
      "Track : 5763\n",
      "Track : 5831\n",
      "Track : 6185\n",
      "Track : 6452\n",
      "Track : 6873\n",
      "Track : 6895\n",
      "Track : 6905\n",
      "Track : 7405\n",
      "Track : 8046\n",
      "Track : 8342\n",
      "Track : 10002\n",
      "Track : 10635\n",
      "Track : 17796\n",
      "Found: 3225\n"
     ]
    }
   ],
   "source": [
    "#labels_helix1_filter\n",
    "labels = np.copy(labels_merged)\n",
    "tracks = np.unique(labels)\n",
    "straight_tracks = []\n",
    "straight_ids = []\n",
    "for track in tracks:\n",
    "    if track == 0: continue\n",
    "    (is_straight, track_dims) = is_straight_track(track, labels, hits)\n",
    "    if is_straight:\n",
    "        straight_tracks.append(track_dims)\n",
    "        straight_ids.append(track)\n",
    "\n",
    "print('Found: ' + str(len(straight_tracks)))\n",
    "#if len(straight_tracks) > 10:\n",
    "#    draw_prediction_xyz(straight_tracks, straight_tracks)\n",
    "#print('straight tracks: ' + str(straight_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('straight tracks: ' + str(straight_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num equal to ground truth: 0\n",
      "Num differences: 3225\n"
     ]
    }
   ],
   "source": [
    "straight_ids_equal = []\n",
    "straight_ids_unequal = []\n",
    "for straight_track in straight_ids:\n",
    "    straight_ids_unequal.append(straight_track)\n",
    "    #if matches_straight_truth_track(straight_track, labels, hits, truth):\n",
    "    #    straight_ids_equal.append(straight_track)\n",
    "    #else:\n",
    "    #    straight_ids_unequal.append(straight_track)\n",
    "\n",
    "print('Num equal to ground truth: ' + str(len(straight_ids_equal)))\n",
    "print('Num differences: ' + str(len(straight_ids_unequal)))\n",
    "#print('Differences: ' + str(straight_ids_unequal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_id = 2988\n",
    "lengthen_straight_track(track_id, labels, hits, truth)\n",
    "#labels_straightx = straight_track_extension(track_id, labels, hits)\n",
    "# 21.988119\n",
    "# 21.968767\n",
    "# ??? --> 21.949 --> wrong one :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal!\n",
      "[  893  2413  4232  4284  6277  6341  8770  8846 11415 11491 14070 14185]\n",
      "[  893  2413  4232  4284  6277  6341  8770  8846 11415 11491 14070 14185]\n",
      "               x           y       z         zr  volume_id  layer_id\n",
      "893    94.062401  106.974998 -1498.0 -10.516129          7         2\n",
      "2413   78.993301   95.871597 -1298.0 -10.448967          7         4\n",
      "4284   64.841499   83.995796 -1102.0 -10.385273          7         6\n",
      "4232   64.557503   83.730904 -1098.0 -10.385081          7         6\n",
      "6341   55.073200   74.895203  -962.0 -10.348065          7         8\n",
      "6277   54.819199   74.616997  -958.0 -10.346730          7         8\n",
      "8846   45.669701   65.387398  -822.0 -10.306261          7        10\n",
      "8770   45.435398   65.092796  -818.0 -10.304647          7        10\n",
      "11491  38.001301   56.892899  -702.0 -10.260592          7        12\n",
      "11415  37.765900   56.574902  -698.0 -10.261397          7        12\n",
      "14185  31.883499   49.589001  -602.0 -10.211274          7        14\n",
      "14070  31.630100   49.313801  -598.0 -10.207225          7        14\n",
      "LIAM: Track: 5879, matches: 1, zr_min: -10.6212907887, zr_exp: -10.5832920074, zr_max: -10.4109681988, x_min: 94.0624008179, y_min: 106.974998474\n",
      "LIAM: Track: 5879, matches: 1, zr_min: -10.6212907887, zr_exp: -10.5832920074, zr_max: -10.4109681988, searching for extension from 7.2 down to 12.4, targets: [12073]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Volume 12:\n",
    "#- Track: 6867, zr_exp: -5.7741856575, assigning hits: [58909] from tracks: [0]\n",
    "#- Track: 6867, zr_exp: -5.93534708023, assigning hits: [61206 61193] from tracks: [4684 4684]\n",
    "# --> BAD! Weak track, only 2/5 hits were correct, so extension took wrong ones. Need better checks.\n",
    "#Track: 8948, zr_exp: -4.7241795063, assigning hits: [59865] from tracks: [2585]\n",
    "# --> BAD! Weak track, only 2/4 hits were correct, so extension took wrong ones. Need better checks.\n",
    "#Track: 12573, stole ixes: [58663] from all dup zrs: [58663, 58665], from tracks: [6425 6425]\n",
    "#Track: 12573, zr_exp: -6.42055606842, assigning dup-z hits: [58663]\n",
    "\n",
    "# Volume 8:\n",
    "#Track: 6153, zr_exp: 2.65081858635, assigning hits: [27925] from tracks: [0]\n",
    "#Track: 6652, zr_exp: 2.48342061043, 12 possible matches:\n",
    "# --> should be able to narrow down list to best 2 candidates\n",
    "#Track: 6974, zr_exp: -2.18972229958, assigning hits: [24403 24415] from tracks: [2383 9591]\n",
    "#Track: 7712, zr_exp: 1.27639406919, 6 possible matches:\n",
    "#Track: 8100, zr_exp: 0.832951366901, assigning hits: [19340 19354] from tracks: [840 840]\n",
    "#Track: 20161, zr_exp: 1.28490996361, assigning hits: [40152 40160 40162] from tracks: [2389 1935 5400]\n",
    "\n",
    "#+ Track: 4692,  is a positive possible cleansing target\n",
    "#+ Track: 6153,  is a negative possible cleansing target\n",
    "#- Track: 6200,  is a positive possible cleansing target\n",
    "#- Track: 6817,  is a negative possible cleansing target\n",
    "# --> but, other hits don't belong to same track, so ok to remove this one.\n",
    "#- Track: 7251,  is a negative possible cleansing target\n",
    "#+ Track: 8100,  is a positive possible cleansing target\n",
    "#- Track: 8909,  is a negative possible cleansing target\n",
    "# --> but two hits in last layer, one good, one bad. Good one is in right direction.\n",
    "#Track: 11224,  is a positive possible cleansing target\n",
    "\n",
    "#+ Track: 6153,  is a negative possible cleansing target\n",
    "#- Track: 6200,  is a positive possible cleansing target\n",
    "#- Track: 6817,  is a negative possible cleansing target\n",
    "#- Track: 8909,  is a negative possible cleansing target\n",
    "#Track: 9209,  is a negative possible cleansing target\n",
    "#Track: 11224,  is a positive possible cleansing target\n",
    "\n",
    "#LIAM: Track: 52, zr_exp: -10.3781471252, searching for extension from 7.2 down to 12.4, targets: [   0 3327 4435 4833]\n",
    "#[   0 3327 4435 4833]\n",
    "#[   0 4234 4280 9089]\n",
    "#56883  282.296997 -54.082100 -2945.5 -10.247684         12         2\n",
    "#57944  239.776001 -53.367401 -2545.5 -10.362589         12         4\n",
    "#628    135.182007 -50.608200 -1502.0 -10.405654          7         2\n",
    "#609    134.794998 -50.581402 -1498.0 -10.404742          7         2\n",
    "#2084   114.913002 -47.705399 -1298.0 -10.432249          7         4\n",
    "#+ LIAM: Track: 52, zr_min: -10.4739866257, zr_exp: -10.3781471252, zr_max: -10.4051980972,\n",
    "#+--> assign 0 hit\n",
    "#-LIAM: Track: 5306, matches: 4zr_min: -10.4110998344, zr_exp: -10.3198881149, zr_max: -10.2049394417, x_min: -71.7042961121, y_min: 132.317993164\n",
    "#-LIAM: Track: 5306, matches: 4zr_min: -10.4110998344, zr_exp: -10.3198881149, zr_max: -10.2049394417, searching for extension from 7.2 down to 12.4, targets: [ 3441  4234 11683]\n",
    "# --> need to use +/- 1% (change of direction)\n",
    "# --> need to identify bad track below, i.e. 3 volumes, only one layer in 7, only one layer in 8\n",
    "# --> ???\n",
    "#57262 -94.080704  274.156006 -2945.500000 -10.162173         12         2\n",
    "#58339 -84.451897  232.964996 -2545.500000 -10.272400         12         4\n",
    "#2664  -48.299000  117.907997 -1302.000000 -10.218417          7         4\n",
    "#2678  -48.010399  117.335999 -1298.000000 -10.238346          7         4\n",
    "#15235 -13.748400   29.023199  -331.877991 -10.334093          8         2\n",
    "#- LIAM: Track: 5569, matches: 4, zr_min: -10.2550249672, zr_exp: -10.1617898941, zr_max: -10.0519551659, x_min: 76.0539016724, y_min: 126.42199707\n",
    "#- LIAM: Track: 5569, matches: 4, zr_min: -10.2550249672, zr_exp: -10.1617898941, zr_max: -10.0519551659, searching for extension from 7.2 down to 12.4,\n",
    "#- targets: [   0 2194 8270]\n",
    "# --> can steal 8270, good candidate (only 12.2 and 12.4 hits)\n",
    "#- LIAM: Track: 5751, zr_exp: -10.121670723, searching for extension from 7.2 down to 12.4, targets: [   0  915 2194 5759]\n",
    "# --> need to use +4% (no change in direction)\n",
    "# --> difficult, closer match with +/- 1% found\n",
    "#--LIAM: Track: 5879, zr_exp: -10.5832920074, searching for extension from 7.2 down to 12.4, targets: [ 3876  5029  5332  7220 12073]\n",
    "# --> can discard found track, since it contains multiple layers in volume 7.\n",
    "\n",
    "# Decisions on how to eliminate merge candidates:\n",
    "# if contains vol == 12 and len(layers) > 3\n",
    "# if contains vol == 7 and len(layers) > 2\n",
    "# otherwise (12.layers <= 3) and (7.layers <= 2), good candidate to steal 12.layers from?\n",
    "\n",
    "track_id = 5879\n",
    "#graph_my_track(track_id, labels, hits)\n",
    "lengthen_straight_track(track_id, labels, hits, truth)\n",
    "labels_straightx = straight_track_extension(track_id, labels, hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected track: [ 6488 11620 58203]\n",
      "Truth track:    [  936  2465  4275  4301  6378  8826 11475 14097 14156 58203]\n",
      "                x           y       z         zr  volume_id  layer_id\n",
      "58203  102.656998  219.024002 -2548.5 -10.535853         12         4\n",
      "6488    26.585100   87.285599  -962.5 -10.548591          7         8\n",
      "11620   21.292999   63.508801  -702.5 -10.487693          7        12\n",
      "                x           y       z         zr  volume_id  layer_id\n",
      "58203  102.656998  219.024002 -2548.5 -10.535853         12         4\n",
      "936     68.966698  125.412003 -1498.0 -10.466425          7         2\n",
      "2465    61.579800  107.731003 -1298.0 -10.460247          7         4\n",
      "4275    53.819401   90.554901 -1102.0 -10.461267          7         6\n",
      "4301    53.657501   90.219902 -1098.0 -10.460105          7         6\n",
      "6378    47.938702   78.563797  -962.0 -10.452578          7         8\n",
      "8826    41.802700   66.732498  -822.0 -10.438828          7        10\n",
      "11475   36.348000   56.682899  -702.0 -10.425340          7        12\n",
      "14156   31.621000   48.395000  -602.0 -10.413473          7        14\n",
      "14097   31.430300   48.078999  -598.0 -10.410706          7        14\n"
     ]
    }
   ],
   "source": [
    "track_id = 12073\n",
    "lengthen_straight_track(track_id, labels, hits, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lengthen_straight_track(18, labels, hits, truth)\n",
    "possible_matches = hits.loc[(hits['zr'] > 25.25) & (hits['zr'] < 25.35) & (hits['volume_id'] == 9) & (hits['layer_id'] == 6)]\n",
    "px1 = possible_matches[['x','y', 'z', 'zr', 'volume_id', 'layer_id']]\n",
    "print(px1)\n",
    "#48855 -27.366800  17.225401   817.5  25.280945          9         6\n",
    "#49002 -27.362101  17.259899   818.0  25.285137          9         6  25.288\n",
    "#48928 -27.522600  17.305700   822.5  25.298941          9         6\n",
    "#51147 -32.477798  19.420700   957.5  25.302980          9         8          0.020\n",
    "#51276 -32.487801  19.437099   958.0  25.304823          9         8  25.308\n",
    "#51224 -32.635700  19.500700   962.5  25.316973          9         8\n",
    "#53173 -37.587700  21.615101  1097.5  25.311626          9        10          0.005\n",
    "#53258 -37.613602  21.614401  1098.0  25.310255          9        10  25.313\n",
    "#53208 -37.779999  21.656300  1102.5  25.317593          9        10          0.019\n",
    "#54913 -44.953602  24.600599  1298.0  25.329466          9        12  25.332\n",
    "#54870 -45.099701  24.675800  1302.5  25.336063          9        12          0.023\n",
    "#56306 -52.273998  27.524599  1498.0  25.356438          9        14  25.355\n",
    "#56281 -52.442101  27.591801  1502.5  25.355331          9        14\n",
    "\n",
    "# Possible ones:\n",
    "#48144  23.423700  22.537201  822.5  25.303558          9         6\n",
    "#48215  23.288200  22.455200  818.0  25.285286          9         6\n",
    "#48257  23.405001  22.547501  822.0  25.293100          9         6\n",
    "#48855 -27.366800  17.225401  817.5  25.280945          9         6\n",
    "#48928 -27.522600  17.305700  822.5  25.298941          9         6\n",
    "#49002 -27.362101  17.259899  818.0  25.285137          9         6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_id = 3105\n",
    "ix1 = np.where(labels == track_id)[0]\n",
    "print(ix1)\n",
    "lengthen_straight_track(track_id, labels, hits, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_id = 123\n",
    "ix1 = np.where(labels == track_id)[0]\n",
    "print(ix1)\n",
    "lengthen_straight_track(track_id, labels, hits, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_id = 325\n",
    "ix1 = np.where(labels == track_id)[0]\n",
    "print(ix1)\n",
    "lengthen_straight_track(track_id, labels, hits, truth)\n",
    "#ix1 = np.where(labels == track_id)[0]\n",
    "#print(ix1)\n",
    "#straight_labels = straight_track_extension(track_id, labels, hits)\n",
    "#ix1 = np.where(straight_labels == track_id)[0]\n",
    "#print(ix1)\n",
    "#Duplicatez found on track 2988, removed: [44825, 44759, 42056, 41974]\n",
    "#track: 123 steal_ixes: [], dup-zs: [51149, 51190, 51084, 51099]\n",
    "#--> old tracks: [ 325 3105  325 3105]\n",
    "#               x          y      z         zr  volume_id  layer_id  module_id\n",
    "##51149 -45.120602  47.249100  957.5  14.655781          9         8         96\n",
    "#51190 -47.384998  44.974800  957.5  14.656254          9         8         96\n",
    "##51084 -45.347500  47.436501  962.0  14.659087          9         8         93\n",
    "#51099 -47.596699  45.219700  962.0  14.652879          9         8         93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_id = 7461\n",
    "ix1 = np.where(labels == track_id)[0]\n",
    "print(ix1)\n",
    "lengthen_straight_track(track_id, labels, hits, truth)\n",
    "#Merge tracks [7456, 7436] allowed, matching volume [9] and non-overlapping adjacent layers: [ 8 10], [6]\n",
    "#Track: 7456, assigning hits: [47110 47164 47244]\n",
    "#Track: 7456, assigning hits: [53774] from tracks: [0]\n",
    "#Merge tracks [7461, 7440] allowed, matching volume [9] and non-overlapping adjacent layers: [ 8 10 12 14], [6]\n",
    "#Track: 7461, assigning hits: [47716 47792 47823]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_id = 2988\n",
    "outliers = find_duplicate_z_using_zr(track_id, labels, hits)\n",
    "print('Duplicate-z outliers: ' + str(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_id = 16\n",
    "\n",
    "straight_labels = straight_track_extension(track_id, labels, hits)\n",
    "\n",
    "score_one_submission(event_id, hits, labels, truth)\n",
    "score_one_submission(event_id, hits, straight_labels, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(straight_ids_unequal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_straight = np.copy(labels)\n",
    "straight_id = 123\n",
    "labels_straight = straight_track_extension(straight_id, labels_straight, hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge tracks [16, 7449] allowed, matching volume [9] and non-overlapping adjacent layers: [ 8 10 12 14], [6]\n",
      "Track: 16, zr_exp: 25.3033593496, assigning hits: [48855 48928 49002]\n",
      "Track: 21 taking un-assigned hits, ixes: [54895, 54900], old tracks: [   0 1050]\n",
      "Track: 21, zr_exp: 21.8822822571, assigning dup-z hits: [54895]\n",
      "Track: 41, zr_exp: -11.5820484161, assigning hits: [8175 8121] from tracks: [0 0]\n",
      "LIAM: Track: 52, zr_min: -10.4739866257, zr_exp: -10.3781471252, zr_max: -10.4051980972, searching for extension from 7.2 down to 12.4, targets: [   0 3327 4435 4833]\n",
      "Merge tracks [123, 325] allowed, matching volume [9] and non-overlapping adjacent layers: [2 4 6], [ 8 10 12 14]\n",
      "Track: 123, zr_exp: 14.640765667, assigning dup-z hits: [51084 51149 53128 53158 54782 54843 56185 56240]\n",
      "Track: 171, zr_exp: 10.3980481625, assigning hits: [45280 45319] from tracks: [0 0]\n",
      "Track: 357 taking un-assigned hits, ixes: [4605, 4606, 4556, 4581], old tracks: [3808    0 3808    0]\n",
      "Track: 357, zr_exp: -16.7166028023, assigning dup-z hits: [4606, 4581]\n",
      "Track: 375 taking un-assigned hits, ixes: [54193, 54209], old tracks: [2185    0]\n",
      "Track: 375, zr_exp: 14.3809010983, assigning dup-z hits: [54209]\n",
      "Track: 490, zr_exp: 15.1984575589, assigning hits: [44195 44253] from tracks: [0 0]\n",
      "Track: 544, zr_exp: -15.1924648285, assigning hits: [12667] from tracks: [0]\n",
      "Track: 551, zr_exp: -14.0582380295, assigning hits: [1138] from tracks: [58]\n",
      "Track: 727 taking un-assigned hits, ixes: [10952, 10974, 10875, 10890, 10994, 11022], old tracks: [   0 3173 3173    0    0 3173]\n",
      "Track: 727, zr_exp: -17.5386199951, assigning dup-z hits: [10952, 10890, 10994]\n",
      "Track: 727 taking un-assigned hits, ixes: [13608, 13627, 13578, 13579, 13686, 13699], old tracks: [3173    0 3173    0    0 3173]\n",
      "Track: 727, zr_exp: -17.5334234238, assigning dup-z hits: [13627, 13579, 13686]\n",
      "Track: 1007, zr_exp: -12.05179739, assigning hits: [5310] from tracks: [6315]\n",
      "Track: 1225, zr_exp: 12.0649061203, assigning hits: [49706] from tracks: [0]\n",
      "Track: 1352, zr_exp: 13.9185819626, assigning hits: [56021] from tracks: [0]\n",
      "Merge tracks [1407, 7227] allowed, matching volume [7] and non-overlapping adjacent layers: [ 2  4  6  8 10 12], [14]\n",
      "Track: 1407, zr_exp: -12.2295570374, assigning hits: [12694 12749]\n",
      "Track: 2457, zr_exp: 17.5742621422, assigning hits: [51444] from tracks: [0]\n",
      "Track: 2721, zr_exp: -11.8737044334, assigning hits: [12645 12549] from tracks: [0 0]\n",
      "Track: 2810, zr_exp: 13.9980413914, assigning hits: [47849 47790] from tracks: [374 374]\n",
      "Track: 2991, zr_exp: 15.352057457, assigning hits: [55838] from tracks: [3795]\n",
      "Track: 3064, zr_exp: 25.3974603017, assigning hits: [46618 46707 46532] from tracks: [0 0 0]\n",
      "Track: 3213, zr_exp: 12.2137336731, assigning hits: [55633] from tracks: [9335]\n",
      "Track: 3561, zr_exp: -10.7637209892, assigning hits: [2016 1981] from tracks: [0 0]\n",
      "Track: 3675, zr_exp: 15.5069036484, assigning hits: [56101] from tracks: [0]\n",
      "Track: 3795, zr_exp: 15.3522424698, assigning hits: [55838] from tracks: [2991]\n",
      "Track: 3808, zr_exp: -16.6469669342, assigning hits: [1155] from tracks: [0]\n",
      "Track: 4179 taking un-assigned hits, ixes: [4668, 4678], old tracks: [   0 1967]\n",
      "Track: 4179, zr_exp: -12.7618861198, assigning dup-z hits: [4668]\n",
      "Track: 4179 taking un-assigned hits, ixes: [6916, 6922, 6848, 6860], old tracks: [1967    0 1967    0]\n",
      "Track: 4179, zr_exp: -12.7445468903, assigning dup-z hits: [6922, 6860]\n",
      "Track: 4373, zr_exp: 11.0610351562, assigning hits: [43348 43247] from tracks: [0 0]\n",
      "Track: 4785, zr_exp: -15.1723372936, assigning hits: [7386 7453] from tracks: [0 0]\n",
      "Track: 5126, zr_exp: 16.1157336235, assigning hits: [45748 45855] from tracks: [0 0]\n",
      "Track: 5134, zr_exp: -13.9130077362, assigning hits: [8043 8099] from tracks: [0 0]\n",
      "Track: 5196 taking un-assigned hits, ixes: [5993, 6008, 5923, 5924, 5941, 5946], old tracks: [1245    0    0 1245    0 1245]\n",
      "Track: 5196, zr_exp: -19.3260288239, assigning dup-z hits: [6008, 5923, 5941]\n",
      "Track: 5227, zr_exp: 21.260157903, assigning hits: [53814] from tracks: [0]\n",
      "Track: 5227, zr_exp: 21.323149999, assigning hits: [55368] from tracks: [3107]\n",
      "LIAM: Track: 5306, zr_min: -10.3080196381, zr_exp: -10.3198881149, zr_max: -10.2930498123, searching for extension from 7.2 down to 12.4, targets: [2580 3386]\n",
      "LIAM: Track: 5569, zr_min: -10.1927776337, zr_exp: -10.1617898941, zr_max: -10.1534900665, searching for extension from 7.2 down to 12.4, targets: [2090 3439 4831]\n",
      "LIAM: Track: 5582, zr_min: -9.4737238884, zr_exp: -9.43213939667, zr_max: -9.39055490494, searching for extension from 7.2 down to 12.4, targets: [ 1854  3323  5662  6634 11870]\n",
      "Track: 5601, zr_exp: -16.1955623627, assigning hits: [1204] from tracks: [5763]\n",
      "TODO! Track: 5645 cannot take hits, more work needed, ixes: [966, 968], old tracks: [ 597 2684]\n",
      "Track: 5645, zr_exp: -12.8688583374, assigning dup-z hits: [929]\n",
      "Track: 5725 taking un-assigned hits, ixes: [56368, 56371, 56332, 56338], old tracks: [3262    0    0 3262]\n",
      "Track: 5725, zr_exp: 17.3321580887, assigning dup-z hits: [56371, 56332]\n",
      "LIAM: Track: 5751, zr_min: -10.1713914871, zr_exp: -10.121670723, zr_max: -10.0719499588, searching for extension from 7.2 down to 12.4, targets: [   0  915 2194 5759]\n",
      "Track: 5849, zr_exp: -14.1198029518, assigning hits: [12218 12179] from tracks: [7225 7225]\n",
      "Track: 5849, zr_exp: -14.1306371689, assigning hits: [14919 14855] from tracks: [13478 13478]\n",
      "LIAM: Track: 5879, zr_min: -10.6504545212, zr_exp: -10.5832920074, zr_max: -10.5161294937, searching for extension from 7.2 down to 12.4, targets: [ 3876  5029  5332  7220 12073]\n",
      "Track: 5921 taking un-assigned hits, ixes: [49369, 49394, 49293, 49320], old tracks: [   0 3800 3800    0]\n",
      "Track: 5921, zr_exp: 25.9335551262, assigning dup-z hits: [49446, 49369, 49320]\n",
      "Track: 5956, zr_exp: -25.7599747976, assigning hits: [9085 9022] from tracks: [5127 5127]\n",
      "Track: 6040, zr_exp: -16.5139541626, assigning hits: [6100 6159] from tracks: [0 0]\n",
      "Track: 6040, zr_exp: -16.5144262314, assigning hits: [4043 4105] from tracks: [0 0]\n",
      "Track: 6100 many unique tracks, only taking un-assigned hits, ixes: [44862, 44895, 44923, 44945], old tracks: [2569 1757 2569    0]\n",
      "Track: 6100, zr_exp: 7.64390587807, assigning dup-z hits: [44945]\n",
      "TODO! Track: 6100 cannot take hits, more work needed, ixes: [42173, 42176], old tracks: [3673  785]\n",
      "Track: 6100, zr_exp: 7.50060653687, assigning dup-z hits: [42202]\n",
      "Track: 6214, zr_exp: 10.8177661896, assigning hits: [49145 46556] from tracks: [402 402]\n",
      "Track: 6285, zr_exp: 21.6467030843, assigning hits: [50793 50766] from tracks: [4674    0]\n",
      "Track: 6285 many unique tracks, only taking un-assigned hits, ixes: [52837, 52851, 52807, 52812], old tracks: [   0 4674 7488 4674]\n",
      "Track: 6285, zr_exp: 21.5297768911, assigning dup-z hits: [52821, 52837]\n",
      "Track: 6285 taking un-assigned hits, ixes: [54562, 54579, 54522, 54548], old tracks: [   0 4674    0 4674]\n",
      "Track: 6285, zr_exp: 21.4798994064, assigning dup-z hits: [54562, 54522]\n",
      "Track: 6285 taking un-assigned hits, ixes: [56007, 56008, 55970, 55973], old tracks: [   0 4674 4674 4674]\n",
      "Track: 6285, zr_exp: 21.3827505112, assigning dup-z hits: [56007]\n",
      "Track: 6333 taking un-assigned hits, ixes: [53021, 53041, 52977, 52985], old tracks: [4331    0 4331    0]\n",
      "Track: 6333, zr_exp: 13.7895693779, assigning dup-z hits: [53041, 52985]\n",
      "Track: 6342 taking un-assigned hits, ixes: [55855, 55859], old tracks: [952   0]\n",
      "Track: 6342, zr_exp: 13.5732316971, assigning dup-z hits: [55831, 55859]\n",
      "Track: 6576 taking un-assigned hits, ixes: [7379, 7401], old tracks: [   0 1959]\n",
      "Track: 6576, zr_exp: -13.4632180532, assigning dup-z hits: [7379]\n",
      "Track: 6576 taking un-assigned hits, ixes: [5095, 5114], old tracks: [   0 1959]\n",
      "Track: 6576, zr_exp: -13.3664617538, assigning dup-z hits: [5095]\n",
      "Track: 6576, zr_exp: -13.3223991394, assigning hits: [3152] from tracks: [0]\n",
      "Track: 6576, zr_exp: -13.2972021103, assigning hits: [1519] from tracks: [0]\n",
      "Track: 6576, zr_exp: -13.2283744812, assigning hits: [200 165] from tracks: [0 0]\n",
      "Track: 6624, zr_exp: -12.3436260223, assigning hits: [7194] from tracks: [3169]\n",
      "Track: 6624, zr_exp: -12.39442873, assigning hits: [9829 9776] from tracks: [0 0]\n",
      "Merge tracks [6625, 10910] allowed, matching volume [7] and non-overlapping adjacent layers: [ 8 10 12 14], [2 4 6]\n",
      "Track: 6625, zr_exp: -18.0581417084, assigning hits: [ 262 1658 3317 3376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track: 7121, zr_exp: -10.2453260422, assigning hits: [728 758] from tracks: [8235 8235]\n",
      "Track: 7121, zr_exp: -10.5580883026, assigning hits: [11223 11136] from tracks: [12891   144]\n",
      "Track: 7121, zr_exp: -10.5283002853, assigning hits: [13897 13971] from tracks: [12620 12620]\n",
      "Track: 7438 taking un-assigned hits, ixes: [44730, 44759, 44801, 44825], old tracks: [2988    0 2988    0]\n",
      "Track: 7438, zr_exp: 15.8452892303, assigning dup-z hits: [44759, 44825]\n",
      "Track: 7438, zr_exp: 16.2782993317, assigning hits: [53940] from tracks: [0]\n",
      "Track: 7438 taking un-assigned hits, ixes: [41967, 41974, 42025, 42056], old tracks: [2988    0 2988    0]\n",
      "Track: 7438, zr_exp: 15.8013286591, assigning dup-z hits: [41974, 42056]\n",
      "Track: 7438, zr_exp: 16.4634122849, assigning hits: [55477] from tracks: [0]\n",
      "Merge tracks [7456, 7436] allowed, matching volume [9] and non-overlapping adjacent layers: [ 8 10], [6]\n",
      "Track: 7456, zr_exp: 15.6146831512, assigning hits: [47110 47164 47244]\n",
      "Track: 7456, zr_exp: 15.7733082771, assigning hits: [53774] from tracks: [0]\n",
      "Merge tracks [7456, 7414] allowed, matching volume [9] and non-overlapping adjacent layers: [ 6  8 10 12], [4]\n",
      "Track: 7456, zr_exp: 15.506728967, assigning hits: [44390 44438]\n",
      "Track: 7456, zr_exp: 15.8025693893, assigning hits: [55335] from tracks: [0]\n",
      "Merge tracks [7456, 7390] allowed, matching volume [9] and non-overlapping adjacent layers: [ 4  6  8 10 12 14], [2]\n",
      "Track: 7456, zr_exp: 15.3519881566, assigning hits: [41653 41688]\n",
      "Merge tracks [7461, 7440] allowed, matching volume [9] and non-overlapping adjacent layers: [ 8 10 12 14], [6]\n",
      "Track: 7461, zr_exp: 26.736284256, assigning hits: [47716 47792 47823]\n",
      "Track: 7472, zr_exp: 21.6121056875, 6 possible matches\n",
      "Track: 7472, zr_exp: 21.6121056875, [48839 48985 48995 49061 48906 48918]\n",
      "Track: 7472, zr_exp: 21.6121056875, [5159 5159   21   21 5159   21]\n",
      "Track: 12302, zr_exp: -11.2988538742, assigning hits: [7925] from tracks: [0]\n",
      "Track: 12302, zr_exp: -11.2662286758, assigning hits: [10524] from tracks: [6479]\n",
      "Track: 14379, zr_exp: 22.5620002747, assigning hits: [52410 52386] from tracks: [3063 3063]\n",
      "Track: 14379, stole ixes: [50247, 50291, 50182] from all dup zrs: [50291, 50302, 50182, 50206, 50247, 50261], from tracks: [3063 3063 3063 3063 3063 3063]\n",
      "Track: 14379, zr_exp: 22.6757125854, assigning dup-z hits: [50247, 50291, 50182]\n",
      "Track: 14379, stole ixes: [47766, 47848, 47720] from all dup zrs: [47848, 47855, 47693, 47720, 47766, 47799], from tracks: [3063 3063 3063 3063 3063 3063]\n",
      "Track: 14379, zr_exp: 22.7605791092, assigning dup-z hits: [47766, 47848, 47720]\n",
      "Track: 14379, zr_exp: 22.8505134583, assigning hits: [45129 45027 45100] from tracks: [3063 3063 3063]\n",
      "TODO! Track: 15707 cannot take hits, more work needed, ixes: [9471, 9472, 9512, 9516], old tracks: [  603 20068   603 20068]\n",
      "Track: 15707, zr_exp: -16.6671447754, assigning dup-z hits: [9417]\n",
      "Track: 18101, zr_exp: -8.26735210419, assigning hits: [14329] from tracks: [4439]\n",
      "Score for event 1029: 0.69792186236579401015234225269523\n",
      "Score for event 1029: 0.69932251250215871074544793373207\n"
     ]
    }
   ],
   "source": [
    "labels_straight = np.copy(labels)\n",
    "for straight_id in straight_ids_unequal:\n",
    "    labels_straight = straight_track_extension(straight_id, labels_straight, hits)\n",
    "score_one_submission(event_id, hits, labels, truth)\n",
    "score_one_submission(event_id, hits, labels_straight, truth)\n",
    "#Score for event 1029: 0.69792186236579401015234225269523\n",
    "#Score for event 1029: 0.69855311091658223165978824908962\n",
    "# multi-round score:   0.69863787156127243882508537353715\n",
    "# final score:         0.69869633909142048189266915869666\n",
    "# final score 7+9:     0.69918788392731723302375712592038\n",
    "# 7+9 with 1 outlier:  0.69929738679703468129389420937514\n",
    "# 7+9, middle opt:     0.69932251250215871074544793373207"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_range(ix, xs, ys, zrs, use_largest_zrdiff=True):\n",
    "    def get_min_max(old_val, diff):\n",
    "        new_val = old_val - diff\n",
    "        if new_val > old_val:\n",
    "            min_val = old_val\n",
    "            max_val = new_val - diff\n",
    "        else:\n",
    "            min_val = new_val - diff\n",
    "            max_val = old_val\n",
    "        return (min_val, new_val, max_val)\n",
    "\n",
    "    # FIXME: Simple for now, expect linear changes\n",
    "    # Should be enhanced to look at trends as well.\n",
    "    xdiffs = np.diff(xs)\n",
    "    ydiffs = np.diff(ys)\n",
    "    zrdiffs = np.diff(zrs)\n",
    "    # The difference in zr values is typically much smaller than the\n",
    "    # difference in x and y values. Use the largest found zr diff,\n",
    "    # instead of the adjacent zr diff (x and y use adjacent values).\n",
    "    best_zrdiff = 0\n",
    "    for iix, zr in enumerate(zrs):\n",
    "        if iix < (len(zrs)-1) and zr != 0 and zrs[iix+1] != 0:\n",
    "            if (zrdiffs[iix] < 0 and zrdiffs[iix] < best_zrdiff) or (zrdiffs[iix] > 0 and zrdiffs[iix] > best_zrdiff):\n",
    "                best_zrdiff = zrdiffs[iix]\n",
    "\n",
    "    if ix < (len(xs) - 1) and ix > 0 and xs[ix-1] != 0 and xs[ix+1] != 0:\n",
    "        # Look at next 2 hits in track to find the expected value.\n",
    "        x_min = min(xs[ix-1], xs[ix+1])\n",
    "        x_max = max(xs[ix-1], xs[ix+1])\n",
    "        x_exp = (x_min + x_max) / 2\n",
    "        y_min = min(ys[ix-1], ys[ix+1])\n",
    "        y_max = max(ys[ix-1], ys[ix+1])\n",
    "        y_exp = (y_min + y_max) / 2\n",
    "        zr_min = min(zrs[ix-1], zrs[ix+1])\n",
    "        zr_max = max(zrs[ix-1], zrs[ix+1])\n",
    "        zr_exp = (zr_min + zr_max) / 2\n",
    "    elif ix < (len(xs) - 2) and xs[ix+1] != 0 and xs[ix+2] != 0:\n",
    "        # Look at next 2 hits in track to find the expected value.\n",
    "        (x_min, x_exp, x_max) = get_min_max(xs[ix+1], xdiffs[ix+1])\n",
    "        (y_min, y_exp, y_max) = get_min_max(ys[ix+1], ydiffs[ix+1])\n",
    "        if use_largest_zrdiff:\n",
    "            # Use best_zrdiff to determine the min/max range, but actual zrdiff\n",
    "            # to select most likely value\n",
    "            (zr_min, zr_exp, zr_max) = get_min_max(zrs[ix+1], best_zrdiff)\n",
    "            (_, zr_exp, _) = get_min_max(zrs[ix+1], zrdiffs[ix+1])\n",
    "        else:\n",
    "            (zr_min, zr_exp, zr_max) = get_min_max(zrs[ix+1], zrdiffs[ix+1])\n",
    "    else:\n",
    "        # Look at previous 2 hits in track to find the expected value.\n",
    "        (x_min, x_exp, x_max) = get_min_max(xs[ix-1], -xdiffs[ix-2])\n",
    "        (y_min, y_exp, y_max) = get_min_max(ys[ix-1], -ydiffs[ix-2])\n",
    "        if use_largest_zrdiff:\n",
    "            (zr_min, zr_exp, zr_max) = get_min_max(zrs[ix-1], -best_zrdiff)\n",
    "            (_, zr_exp, _) = get_min_max(zrs[ix-1], -zrdiffs[ix-2])\n",
    "        else:\n",
    "            (zr_min, zr_exp, zr_max) = get_min_max(zrs[ix-1], -zrdiffs[ix-2])\n",
    "    return (x_min, x_exp, x_max, y_min, y_exp, y_max, zr_min, zr_exp, zr_max)\n",
    "\n",
    "def get_volume_switch_expected_zr_range(zr, factor=0.01):\n",
    "    new_val_1 = zr * (1.00-factor)\n",
    "    new_val_2 = zr * (1.00+factor)\n",
    "    if new_val_1 > zr:\n",
    "        min_val = new_val_2\n",
    "        max_val = new_val_1\n",
    "    else:\n",
    "        min_val = new_val_1\n",
    "        max_val = new_val_2\n",
    "    return (min_val, max_val)\n",
    "\n",
    "def can_merge_tracks(track1, track2, labels, hits):\n",
    "    # FIXME: Can be much smarter, for now, only consider merging both\n",
    "    # tracks if they are both in the same volume, and do not have\n",
    "    # any overlapping layers. Caller should verify that the tracks\n",
    "    # are likely related, i.e. possibly by comparing zr values in\n",
    "    # neighbouring layers to see if they are similar.\n",
    "    merge_valid = False\n",
    "    t1_ix = np.where(labels == track1)[0]\n",
    "    t2_ix = np.where(labels == track2)[0]\n",
    "    df1 = hits.loc[t1_ix]\n",
    "    df2 = hits.loc[t2_ix]\n",
    "    volume1 = np.unique(df1.volume_id.values)\n",
    "    volume2 = np.unique(df2.volume_id.values)\n",
    "    if len(volume1) == 1 and len(volume2) == 1 and volume1[0] == volume2[0]:\n",
    "        layers1 = np.unique(df1.layer_id.values)\n",
    "        layers2 = np.unique(df2.layer_id.values)\n",
    "        if (layers1[-1] + 2 == layers2[0]) or (layers2[-1] + 2 == layers1[0]):\n",
    "            print('Merge tracks [' + str(track1) + ', ' + str(track2) + '] allowed, matching volume ' + str(volume1) + ' and non-overlapping adjacent layers: ' + str(layers1) + ', ' + str(layers2))\n",
    "            merge_valid = True\n",
    "    return (merge_valid, t1_ix, t2_ix)\n",
    "\n",
    "def find_nearest_zrs(dup_ixes, ixes, zs, zrs, ideal_zr):\n",
    "    test_zrs = []\n",
    "    test_zs = []\n",
    "    test_ixes = []\n",
    "    for aix, ix in enumerate(ixes):\n",
    "        if ix in dup_ixes:\n",
    "            test_zrs.append(zrs[aix])\n",
    "            test_zs.append(zs[aix])\n",
    "            test_ixes.append(ix)\n",
    "    array = np.asarray(test_zrs)\n",
    "    nearest_zrs_ix = []\n",
    "    while len(test_zrs) > 0:\n",
    "        array = np.asarray(test_zrs)\n",
    "        idx = (np.abs(array - ideal_zr)).argmin()\n",
    "        rem_z_value = test_zs[idx]\n",
    "        nearest_zrs_ix.append(test_ixes[idx])\n",
    "        test_zrs.pop(idx)\n",
    "        test_zs.pop(idx)\n",
    "        test_ixes.pop(idx)\n",
    "        indexes = [i for i,z in enumerate(test_zs) if z == rem_z_value]\n",
    "        for ii in indexes:\n",
    "            test_zrs.pop(ii)\n",
    "            test_zs.pop(ii)\n",
    "            test_ixes.pop(ii)\n",
    "    return nearest_zrs_ix\n",
    "\n",
    "def select_best_zr_matches(track, labels, ix, xs, ys, zrs, px1, hits):\n",
    "    # FIXME: Can be much smarter, for now, just split them into\n",
    "    # low and high pairs\n",
    "    px1_ixes = px1.index.values\n",
    "    z_values = px1.z.values\n",
    "    zr_values = px1.zr.values\n",
    "    steal_ixes = []\n",
    "    # First, assign any hits that do not contain duplicates\n",
    "    duplicate_zs = []\n",
    "    next_ix_is_dup = False\n",
    "    for iix, z in enumerate(z_values):\n",
    "        if next_ix_is_dup:\n",
    "            duplicate_zs.append(px1_ixes[iix])\n",
    "            next_ix_is_dup = False\n",
    "        elif iix == (len(z_values) - 1) or z != z_values[iix+1]:\n",
    "            steal_ixes.append(px1_ixes[iix])\n",
    "        else:\n",
    "            duplicate_zs.append(px1_ixes[iix])\n",
    "            next_ix_is_dup = True\n",
    "    if len(duplicate_zs) > 0:\n",
    "        old_tracks = labels[duplicate_zs]\n",
    "        #print('track: ' + str(track) + ' steal_ixes: ' + str(steal_ixes) + ', dup-zs: ' + str(duplicate_zs) + ', old tracks: ' + str(old_tracks))\n",
    "        unique_tracks = np.unique(old_tracks)\n",
    "        if len(unique_tracks) == 1:\n",
    "            steal_z_from_other_track = True\n",
    "            (x_min, x_exp, x_max, y_min, y_exp, y_max, zr_min, zr_exp, zr_max) = get_expected_range(ix, xs, ys, zrs)\n",
    "            steal_ixes = find_nearest_zrs(duplicate_zs, px1_ixes, z_values, zr_values, zr_exp)\n",
    "            print('Track: ' + str(track) + ', stole ixes: ' + str(steal_ixes) + ' from all dup zrs: ' + str(duplicate_zs) + ', from tracks: ' + str(old_tracks))\n",
    "        elif len(unique_tracks) == 2:\n",
    "            # There are 2 separate tracks that the z values are distributed to.\n",
    "            # First, check if we can merge.\n",
    "            can_merge1 = False\n",
    "            can_merge2 = False\n",
    "            if unique_tracks[0] != 0:\n",
    "                (can_merge1, t1_ix, t2_ix) = can_merge_tracks(track, unique_tracks[0], labels, hits)\n",
    "                if can_merge1:\n",
    "                    #print('Track: ' + str(track) + ' can be merged with track1: ' + str(unique_tracks[0]))\n",
    "                    steal_ixes = t2_ix\n",
    "            if not can_merge1:\n",
    "                (can_merge2, t1_ix, t2_ix) = can_merge_tracks(track, unique_tracks[1], labels, hits)\n",
    "                if can_merge2:\n",
    "                    #print('Track: ' + str(track) + ' can be merged with track2: ' + str(unique_tracks[1]))\n",
    "                    steal_ixes = t2_ix\n",
    "            if not can_merge1 and not can_merge2:\n",
    "                if unique_tracks[0] == 0:\n",
    "                    # just take the un-assigned hits, less risk of taking the wrong ones.\n",
    "                    print('Track: ' + str(track) + ' taking un-assigned hits, ixes: ' + str(duplicate_zs) + ', old tracks: ' + str(old_tracks))\n",
    "                    for dupz in duplicate_zs:\n",
    "                        if labels[dupz] == 0:\n",
    "                            steal_ixes.append(dupz)\n",
    "                else:\n",
    "                    print('TODO! Track: ' + str(track) + ' cannot take hits, more work needed, ixes: ' + str(duplicate_zs) + ', old tracks: ' + str(old_tracks))\n",
    "        elif unique_tracks[0] == 0:\n",
    "            # just take the un-assigned hits, less risk of taking the wrong ones.\n",
    "            print('Track: ' + str(track) + ' many unique tracks, only taking un-assigned hits, ixes: ' + str(duplicate_zs) + ', old tracks: ' + str(old_tracks))\n",
    "            test_steal_ixes = steal_ixes\n",
    "            steal_ixes = []\n",
    "            for tsix in test_steal_ixes:\n",
    "                if labels[tsix] == 0:\n",
    "                    steal_ixes.append(tsix)\n",
    "            for dupz in duplicate_zs:\n",
    "                if labels[dupz] == 0:\n",
    "                    steal_ixes.append(dupz)\n",
    "        else:\n",
    "            print('TODO! Track: ' + str(track) + ', too many tracks to steal duplicate zs from, ignoring dups: ' + str(duplicate_zs) + ', old tracks: ' + str(old_tracks))\n",
    "    #print(px1)\n",
    "    return steal_ixes\n",
    "\n",
    "def generate_zr_layer_data(x, y, zr, layer, lmap):\n",
    "    xs = [0,0,0,0,0,0,0]\n",
    "    ys = [0,0,0,0,0,0,0]\n",
    "    zrs = [0,0,0,0,0,0,0]\n",
    "    counts = [0,0,0,0,0,0,0]\n",
    "    for ix, l in enumerate(layer):\n",
    "        aix = lmap[l]\n",
    "        counts[aix] = counts[aix] + 1\n",
    "        xs[aix] = xs[aix] + x[ix]\n",
    "        ys[aix] = ys[aix] + y[ix]\n",
    "        zrs[aix] = zrs[aix] + zr[ix]\n",
    "    for ix, count in enumerate(counts):\n",
    "        if count != 0:\n",
    "            xs[ix] = xs[ix] / count\n",
    "            ys[ix] = ys[ix] / count\n",
    "            zrs[ix] = zrs[ix] / count\n",
    "    return (xs, ys, zrs, counts)\n",
    "\n",
    "def one_round_straight_track_extension(track, labels, hits):\n",
    "    more_rounds_possible = False\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "    if len(hit_ix) == 0:\n",
    "        return (more_rounds_possible, labels)\n",
    "    df = hits.loc[hit_ix]\n",
    "    msg = 'Track: ' + str(track) + ', '\n",
    "    if not np.all(df.volume_id.values == 7) and not np.all(df.volume_id.values == 9):\n",
    "        # FIXME: Future improvement, handle other volumes, and handle\n",
    "        # tracks that span volumes.\n",
    "        #print(msg + 'Can only lengthen straight tracks in volume 9, found: ' + str(df.volume_id.values))\n",
    "        return (more_rounds_possible, labels)\n",
    "    df = df.sort_values('z')\n",
    "    x,y,zr = df[['x', 'y', 'zr']].values.astype(np.float32).T\n",
    "    volume,layer = df[['volume_id', 'layer_id' ]].values.T\n",
    "    #  indexes:  [2->0,4->1,6->2,8->3,10->4,12->5,14->6]\n",
    "    lmap = [0,0,0,0,1,0,2,0,3,0,4,0,5,0,6]\n",
    "    all_layers = [2,4,6,8,10,12,14]\n",
    "    uniq_layers = np.unique(layer)\n",
    "    uniq_volumes = np.unique(volume)\n",
    "    if len(uniq_volumes) > 1:\n",
    "        # FIXME: Future improvement, handle tracks across volumes\n",
    "        #print(msg + 'All hits must be in same volume, volumes found: ' + str(uniq_volumes))\n",
    "        return (more_rounds_possible, labels)\n",
    "    elif np.array_equal(all_layers, uniq_layers):\n",
    "        # FIXME: Future improvement, we can have multiple hits per layer,\n",
    "        # check if we are missing some hits. Hits within the same layer\n",
    "        # should have very small deltas\n",
    "        #print(msg + 'All layers already have at least one hit')\n",
    "        return (more_rounds_possible, labels)\n",
    "    elif len(uniq_layers) == 1:\n",
    "        #print(msg + 'Only one layer defined, unable to determine trends for extension')\n",
    "        return (more_rounds_possible, labels)\n",
    "\n",
    "    (xs, ys, zrs, counts) = generate_zr_layer_data(x, y, zr, layer, lmap)\n",
    "    #print(msg + 'xs: ' + str(xs))\n",
    "    #print(msg + 'ys: ' + str(ys))\n",
    "    #print(msg + 'zrs: ' + str(zrs))\n",
    "    for ix, l in enumerate(all_layers):\n",
    "        if xs[ix] == 0:\n",
    "            if (ix >= 2 and xs[ix-1] != 0 and xs[ix-2] != 0) or (ix < (len(xs) - 2) and xs[ix+1] != 0 and xs[ix+2] != 0):\n",
    "                (x_min, x_exp, x_max, y_min, y_exp, y_max, zr_min, zr_exp, zr_max) = get_expected_range(ix, xs, ys, zrs)\n",
    "                # DO IT!\n",
    "                #print('x: ' + str(x_min) + ', ' + str(x_max) + ', y: ' + str(y_min) + ', ' + str(y_max) + ', zr: ' + str(zr_min) + ', ' + str(zr_max))\n",
    "                possible_matches = hits.loc[(hits['y'] > y_min) & (hits['y'] < y_max) & (hits['x'] > x_min) & (hits['x'] < x_max) & (hits['zr'] > zr_min) & (hits['zr'] < zr_max) & (hits['volume_id'] == uniq_volumes[0]) & (hits['layer_id'] == l)]\n",
    "                possible_matches = possible_matches.sort_values('z')\n",
    "                px1 = possible_matches[['x','y', 'z', 'zr', 'volume_id', 'layer_id', 'module_id']]\n",
    "                msg2 = msg + 'zr_exp: ' + str(zr_exp) + ', '\n",
    "                if len(px1) >= 2 and len(np.unique(px1.z.values)) < len(px1) and len(np.unique(px1.z.values)) <= 3:\n",
    "                    steal_ixs = select_best_zr_matches(track, labels, ix, xs, ys, zrs, px1, hits)\n",
    "                    if len(steal_ixs) > 0:\n",
    "                        # Assign hits!\n",
    "                        #print(steal_ixs)\n",
    "                        #print(labels[steal_ixs])\n",
    "                        print(msg2 + 'assigning dup-z hits: ' + str(steal_ixs))\n",
    "                        labels[steal_ixs] = track\n",
    "                        more_rounds_possible = True\n",
    "                        #print(labels[steal_ixs])\n",
    "                        #print(px1)\n",
    "                elif len(px1) >= 1 and len(px1) <= 3:\n",
    "                    #(can_merge, t1_ix, t2_ix) = can_merge_tracks(track, unique_tracks[0], labels, hits)\n",
    "                    # Assign hits!\n",
    "                    steal_ixs = px1.index.values\n",
    "                    steal_from_tracks = np.unique(labels[steal_ixs])\n",
    "                    can_merge = False\n",
    "                    if len(steal_from_tracks) == 1:\n",
    "                        (can_merge, t1_ix, t2_ix) = can_merge_tracks(track, steal_from_tracks[0], labels, hits)\n",
    "                    if can_merge:\n",
    "                        print(msg2 + 'assigning hits: ' + str(t2_ix))\n",
    "                        labels[t2_ix] = track\n",
    "                    else:\n",
    "                        #print(steal_ixs)\n",
    "                        #print(labels[steal_ixs])\n",
    "                        print(msg2 + 'assigning hits: ' + str(steal_ixs) + ' from tracks: ' + str(labels[steal_ixs]))\n",
    "                        labels[steal_ixs] = track\n",
    "                        #print(labels[steal_ixs])\n",
    "                    more_rounds_possible = True\n",
    "                    #print(px1)\n",
    "                elif len(px1) > 0:\n",
    "                    do_something_here = True\n",
    "                    print(msg2 + str(len(px1)) + ' possible matches')\n",
    "                    steal_ixs = px1.index.values\n",
    "                    print(msg2 + str(steal_ixs))\n",
    "                    print(msg2 + str(labels[steal_ixs]))\n",
    "                    #print(msg2 + 'assigning hits: ' + str(steal_ixs))\n",
    "                    #labels[steal_ixs] = track\n",
    "    return (more_rounds_possible, labels)\n",
    "\n",
    "def cleanse_straight_track(track, labels, hits):\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "    if len(hit_ix) == 0:\n",
    "        return labels\n",
    "    df = hits.loc[hit_ix]\n",
    "    msg = 'Track: ' + str(track) + ', '\n",
    "    if not np.all(df.volume_id.values == 12) and not np.all(df.volume_id.values == 8):\n",
    "        # FIXME: Future improvement, handle other volumes, and handle\n",
    "        # tracks that span volumes.\n",
    "        #print(msg + 'Can only lengthen straight tracks in volume 9, found: ' + str(df.volume_id.values))\n",
    "        return (labels)\n",
    "    df = df.sort_values('z')\n",
    "    hit_ix2 = df.index.values\n",
    "    x,y,zr = df[['x', 'y', 'zr']].values.astype(np.float32).T\n",
    "    volume,layer = df[['volume_id', 'layer_id' ]].values.T\n",
    "    #  indexes:  [2->0,4->1,6->2,8->3,10->4,12->5,14->6]\n",
    "    lmap = [0,0,0,0,1,0,2,0,3,0,4,0,5,0,6]\n",
    "    all_layers = [2,4,6,8,10,12,14]\n",
    "    uniq_layers = np.unique(layer)\n",
    "    uniq_volumes = np.unique(volume)\n",
    "    xs = [0,0,0,0,0,0,0]\n",
    "    ys = [0,0,0,0,0,0,0]\n",
    "    zrs = [0,0,0,0,0,0,0]\n",
    "    counts = [0,0,0,0,0,0,0]\n",
    "    # FIXME: LIAM: Within a single layer, only allow 0.5% variation?\n",
    "    for ix, l in enumerate(layer):\n",
    "        aix = lmap[l]\n",
    "        counts[aix] = counts[aix] + 1\n",
    "        xs[aix] = xs[aix] + x[ix]\n",
    "        ys[aix] = ys[aix] + y[ix]\n",
    "        zrs[aix] = zrs[aix] + zr[ix]\n",
    "    for ix, count in enumerate(counts):\n",
    "        if count != 0:\n",
    "            xs[ix] = xs[ix] / count\n",
    "            ys[ix] = ys[ix] / count\n",
    "            zrs[ix] = zrs[ix] / count\n",
    "\n",
    "    # DOES NOT WORK! \n",
    "    #if len(uniq_layers) == 3:\n",
    "    #    # Favour keeping beginning of track, cut out high end if it looks wrong\n",
    "    #    # sample zrs: 6.16, 6.22, 6.05\n",
    "    #    if zrs[1] > zrs[0] and zrs[2] < (0.995*zrs[1]):\n",
    "    #        print(msg + ' is a positive possible cleansing target')\n",
    "    #    elif zrs[1] < zrs[0] and zrs[2] > (0.995*zrs[1]):\n",
    "    #        print(msg + ' is a negative possible cleansing target')\n",
    "\n",
    "    return labels\n",
    "\n",
    "def merge_with_other_volumes(track, labels, hits):\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "    if len(hit_ix) == 0:\n",
    "        return labels\n",
    "    df = hits.loc[hit_ix]\n",
    "    msg = 'Track: ' + str(track) + ', '\n",
    "    if not np.all(df.volume_id.values == 7) and not np.all(df.volume_id.values == 9):\n",
    "        # FIXME: Future improvement, handle other volumes, and handle\n",
    "        # tracks that span volumes.\n",
    "        #print(msg + 'Can only lengthen straight tracks in volume 9, found: ' + str(df.volume_id.values))\n",
    "        return (labels)\n",
    "    df = df.sort_values('z')\n",
    "    x,y,zr = df[['x', 'y', 'zr']].values.astype(np.float32).T\n",
    "    volume,layer = df[['volume_id', 'layer_id' ]].values.T\n",
    "    #  indexes:  [2->0,4->1,6->2,8->3,10->4,12->5,14->6]\n",
    "    lmap = [0,0,0,0,1,0,2,0,3,0,4,0,5,0,6]\n",
    "    all_layers = [2,4,6,8,10,12,14]\n",
    "    uniq_layers = np.unique(layer)\n",
    "    uniq_volumes = np.unique(volume)\n",
    "    if len(uniq_volumes) > 1:\n",
    "        # FIXME: Future improvement, handle tracks across volumes\n",
    "        #print(msg + 'All hits must be in same volume, volumes found: ' + str(uniq_volumes))\n",
    "        return (labels)\n",
    "    elif len(uniq_layers) == 1:\n",
    "        #print(msg + 'Only one layer defined, unable to determine trends for extension')\n",
    "        return (labels)\n",
    "\n",
    "    (xs, ys, zrs, counts) = generate_zr_layer_data(x, y, zr, layer, lmap)\n",
    "    if uniq_volumes[0] == 7 and zrs[0] != 0 and zrs[1] != 0:\n",
    "        #print(msg + 'searching for extension from 7.2 down to 12.4')\n",
    "        xs.insert(0,0)\n",
    "        ys.insert(0,0)\n",
    "        zrs.insert(0,0)\n",
    "        counts.insert(0,0)\n",
    "        (x_min, x_exp, x_max, y_min, y_exp, y_max, zr_min, zr_exp, zr_max) = get_expected_range(0, xs, ys, zrs, use_largest_zrdiff=False)\n",
    "        #print('x: ' + str(x_min) + ', ' + str(x_max) + ', y: ' + str(y_min) + ', ' + str(y_max) + ', zr: ' + str(zr_min) + ', ' + str(zr_max))\n",
    "        #possible_matches = hits.loc[(hits['y'] > y_min) & (hits['y'] < y_max) & (hits['x'] > x_min) & (hits['x'] < x_max) & (hits['zr'] > zr_min) & (hits['zr'] < zr_max) & (hits['volume_id'] == 12) & (hits['layer_id'] == 4)]\n",
    "        (zr_min, zr_max) = get_volume_switch_expected_zr_range(zrs[1], factor=0.01)\n",
    "        if xs[1] > xs[2] and ys[1] > ys[2]:\n",
    "            possible_matches = hits.loc[(hits['x'] > xs[1]) & (hits['y'] > ys[1]) & (hits['zr'] > zr_min) & (hits['zr'] < zr_max) & (hits['volume_id'] == 12) & (hits['layer_id'] == 4)]\n",
    "        elif xs[1] > xs[2] and ys[1] < ys[2]:\n",
    "            possible_matches = hits.loc[(hits['x'] > xs[1]) & (hits['y'] < ys[1]) & (hits['zr'] > zr_min) & (hits['zr'] < zr_max) & (hits['volume_id'] == 12) & (hits['layer_id'] == 4)]\n",
    "        elif xs[1] < xs[2] and ys[1] > ys[2]:\n",
    "            possible_matches = hits.loc[(hits['x'] < xs[1]) & (hits['y'] > ys[1]) & (hits['zr'] > zr_min) & (hits['zr'] < zr_max) & (hits['volume_id'] == 12) & (hits['layer_id'] == 4)]\n",
    "        else:\n",
    "            possible_matches = hits.loc[(hits['x'] < xs[1]) & (hits['y'] < ys[1]) & (hits['zr'] > zr_min) & (hits['zr'] < zr_max) & (hits['volume_id'] == 12) & (hits['layer_id'] == 4)]\n",
    "        #possible_matches = hits.loc[(hits['zr'] > zr_min) & (hits['zr'] < zr_max) & (hits['volume_id'] == 12) & (hits['layer_id'] == 4)]\n",
    "        possible_matches = possible_matches.sort_values('z')\n",
    "        px1 = possible_matches[['x','y', 'z', 'zr', 'volume_id', 'layer_id', 'module_id']]\n",
    "        msg2 = 'LIAM: ' + msg + 'matches: ' + str(len(px1)) + ', zr_min: ' + str(zr_min) + ', zr_exp: ' + str(zr_exp) + ', zr_max: ' + str(zr_max) + ', '\n",
    "        print(msg2 + 'x_min: ' + str(x_min) + ', y_min: ' + str(y_min))\n",
    "        #print(msg + 'searching for extension from 7.2 down to 12.4, targets: ' + str(unique_tracks))\n",
    "        if len(px1) > 0 and len(px1) <= 15:\n",
    "            old_tracks = labels[px1.index.values]\n",
    "            #print('track: ' + str(track) + ' steal_ixes: ' + str(steal_ixes) + ', dup-zs: ' + str(duplicate_zs) + ', old tracks: ' + str(old_tracks))\n",
    "            unique_tracks = np.unique(old_tracks)\n",
    "            print(msg2 + 'searching for extension from 7.2 down to 12.4, targets: ' + str(unique_tracks))\n",
    "            for utrk in unique_tracks:\n",
    "                if utrk == 0:\n",
    "                    utrk_hits_ix = []\n",
    "                    for iix in px1.index.values:\n",
    "                        if labels[iix] == 0:\n",
    "                            utrk_hits_ix.append(iix)\n",
    "                else:\n",
    "                    utrk_hits_ix = np.where(labels == utrk)[0]\n",
    "                utrk_hits = hits.loc[utrk_hits_ix]\n",
    "                if utrk == 0:\n",
    "                    px2 = utrk_hits[['x','y', 'z', 'zr', 'volume_id', 'layer_id']]\n",
    "                    print(px2)\n",
    "                elif len(np.unique(utrk_hits.volume_id.values)) == 1:\n",
    "                    uniq_layers = np.unique(utrk_hits.layer_id.values)\n",
    "                    if uniq_layers[-1] == 4:\n",
    "                        print(msg2 + 'can potentially steal track: ' + str(utrk))\n",
    "                        # steal_ixs = utrk_hits_ix\n",
    "                        # labels[steal_ixs] = track\n",
    "\n",
    "    return labels\n",
    "\n",
    "def straight_track_extension(track, labels, hits):\n",
    "    labels = np.copy(labels)\n",
    "    labels = cleanse_straight_track(track, labels, hits)\n",
    "    more_rounds = True\n",
    "    while more_rounds:\n",
    "        (more_rounds, labels) = one_round_straight_track_extension(track, labels, hits)\n",
    "    labels = merge_with_other_volumes(track, labels, hits)\n",
    "    return labels\n",
    "\n",
    "def matches_straight_truth_track(track, labels, hits, truth):\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "    \n",
    "    tdf = truth.loc[hit_ix]\n",
    "    hdf = hits.loc[hit_ix]\n",
    "    truth_count = coll.Counter(tdf.particle_id.values).most_common(2)\n",
    "    truth_particle_id = truth_count[0][0]\n",
    "    if truth_particle_id == 0:\n",
    "        if len(truth_count) > 1:\n",
    "            truth_particle_id = truth_count[1][0]\n",
    "        else:\n",
    "            return False\n",
    "    tdf2 = truth.loc[(truth.particle_id == truth_particle_id)]\n",
    "    if np.all(hdf.volume_id == 7) or np.all(hdf.volume_id.values == 9):\n",
    "        arr_s1 = np.copy(hit_ix)\n",
    "        arr_s1.sort()\n",
    "        arr_s2 = np.copy(tdf2.index.values)\n",
    "        arr_s2.sort()\n",
    "        return np.array_equal(arr_s1, arr_s2)\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def lengthen_straight_track(track, labels, hits, truth):\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "    df = hits.loc[hit_ix]\n",
    "    \n",
    "    # Sort on abs(z)? Do we need to sort at all? We may want to find hits in the middle too....\n",
    "    df = df.sort_values('z')\n",
    "    hit_ix2 = df.index.values\n",
    "    x,y,z = df[['x', 'y', 'z' ]].values.astype(np.float32).T\n",
    "    track_dims = np.column_stack((x, y, z))\n",
    "    (m,b) = solve_linear_equation(track_dims)\n",
    "    #hits['y_hat'] = (hits['x'] * m) + b\n",
    "    #hits['y_hat_diff'] = hits['y'] - hits['y_hat']\n",
    "    #print('Straight track ix: ' + str(hit_ix))\n",
    "    df = hits.loc[hit_ix]\n",
    "    df = df.sort_values('z')\n",
    "    #print('m: ' + str(m) + ', b: ' + str(b))\n",
    "    \n",
    "    tdf = truth.loc[hit_ix]\n",
    "    truth_count = coll.Counter(tdf.particle_id.values).most_common(2)\n",
    "    truth_particle_id = truth_count[0][0]\n",
    "    if truth_particle_id == 0 and len(truth_count) > 1:\n",
    "        truth_particle_id = truth_count[1][0]\n",
    "    tdf2 = truth.loc[(truth.particle_id == truth_particle_id)]\n",
    "    tdf2 = tdf2.sort_values('tz')\n",
    "    tdfx2 = tdf2[['tx', 'ty', 'tz']]\n",
    "    #print(tdfx2)\n",
    "\n",
    "    arr_s1 = np.copy(hit_ix)\n",
    "    arr_s1.sort()\n",
    "    arr_s2 = np.copy(tdf2.index.values)\n",
    "    arr_s2.sort()\n",
    "    dfx1 = df[['x','y', 'z', 'zr', 'volume_id', 'layer_id']]\n",
    "    if np.array_equal(arr_s1, arr_s2):\n",
    "        print('Equal!')\n",
    "        print(arr_s1)\n",
    "        print(arr_s2)\n",
    "        print(dfx1)\n",
    "    else:\n",
    "        print('Detected track: ' + str(arr_s1))\n",
    "        print('Truth track:    ' + str(arr_s2))\n",
    "        print(dfx1)\n",
    "\n",
    "        df3 = hits.loc[tdf2.index.values]\n",
    "        dfx3 = df3[['x','y', 'z', 'zr', 'volume_id', 'layer_id']]\n",
    "        print(dfx3)\n",
    "        \n",
    "    #df2 = hits.loc[(hits['y_hat_diff'] < 0.1) & (hits['y_hat_diff'] > -0.1)]\n",
    "    #dfx2 = df2[['x','y', 'z', 'y_hat', 'y_hat_diff']]\n",
    "    #print(dfx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve for y = mx + b\n",
    "def solve_single_linear_equation(val1, val2):\n",
    "    x1 = val1[0]\n",
    "    x2 = val2[0]\n",
    "    y1 = val1[1]\n",
    "    y2 = val2[1]\n",
    "    is_valid = (x1 != x2 and y1 != y2)\n",
    "    m = 0\n",
    "    b = 0\n",
    "    if is_valid:\n",
    "        a = np.array([[x1, 1], [x2, 1]])\n",
    "        b = np.array([y1, y2])\n",
    "        sol = np.linalg.solve(a, b)\n",
    "        m = sol[0]\n",
    "        b = sol[1]\n",
    "    return is_valid, m, b\n",
    "\n",
    "\n",
    "def solve_linear_equation(vals):\n",
    "    \"\"\"Take average of y=mx+b slope across all pair-wise elements, as well as between\n",
    "       first and last elements.\"\"\"\n",
    "    m = 0\n",
    "    b = 0\n",
    "    count = 0\n",
    "    for i in range(len(vals) - 1):\n",
    "        is_valid, m2, b2 = solve_single_linear_equation(vals[i], vals[i+1])\n",
    "        if is_valid:\n",
    "            count = count + 1\n",
    "            m = m + m2\n",
    "            b = b + b2\n",
    "    is_valid, m2, b2 = solve_single_linear_equation(vals[0], vals[-1])\n",
    "    if is_valid:\n",
    "        count = count + 1\n",
    "        m = m + m2\n",
    "        b = b + b2\n",
    "    if count > 1:\n",
    "        m = m / count\n",
    "        b = b / count\n",
    "    return m, b\n",
    "\n",
    "\n",
    "def check_is_track_straight(vals, m, b):\n",
    "    # m/b solved for vals[0], vals[1], test with others\n",
    "    xdiffs = []\n",
    "    ydiffs = []\n",
    "    is_reject = 0\n",
    "    is_straight = 1\n",
    "    if len(vals) < 4:\n",
    "        is_straight = 0\n",
    "    outlier_count = 0\n",
    "    y_sum = abs(np.sum(vals[:,1]))\n",
    "    for ix, val in enumerate(vals):\n",
    "        yhat = m * val[0] + b\n",
    "        ydiff = abs(yhat - val[1])\n",
    "        ydiffs.append(ydiff)\n",
    "        #print('yhat: ' + str(yhat) + ', y: ' + str(val[1]) + ', diff: ' + str(ydiff))\n",
    "        xhat = (val[1] - b) / m\n",
    "        xdiff = abs(xhat - val[0])\n",
    "        xdiffs.append(xdiff)\n",
    "        #print('xhat: ' + str(xhat) + ', x: ' + str(val[0]) + ', diff: ' + str(xdiff))\n",
    "        #print('Diff at ix ' + str(ix) + ' is: ' + str(diff))\n",
    "        if (ydiff > 1):#abs(val[1]*0.01)):#(ydiff > 3 and ydiff > abs(val[1]*0.05)):\n",
    "            outlier_count = outlier_count + 1\n",
    "            is_straight = 0\n",
    "            if (ydiff > abs(val[1]*0.5)):\n",
    "                is_reject = 1\n",
    "    #print(str(is_straight) + ', ' + str(y_sum) + ', ' + str(sum(ydiffs)))\n",
    "    # Tolerate a single outlier if the total diff is minimal\n",
    "    #if outlier_count == 1 and is_reject == 0 and sum(ydiffs) <= y_sum*0.02:\n",
    "    #    is_straight = 1\n",
    "    #    #print('Straightened!')\n",
    "    #    #print('Straightened: ' + str(is_straight) + ', ' + str(y_sum) + ', ' + str(sum(ydiffs)))\n",
    "    #if is_straight:\n",
    "    #    print(str(is_straight) + ', ' + str(y_sum) + ', ' + str(sum(ydiffs)))\n",
    "    return is_straight, xdiffs, ydiffs\n",
    "\n",
    "def check_if_zr_straight(zr_values):\n",
    "    diff_zrs = np.diff(zr_values)\n",
    "    abs_diff_zrs = np.absolute(diff_zrs)\n",
    "    min_zr = zr_values.min()\n",
    "    max_zr = zr_values.max()\n",
    "    mean_diff_zr = diff_zrs.mean()\n",
    "    median_zr = abs(np.median(zr_values))\n",
    "    mean_zr = zr_values.mean()\n",
    "    num_outliers = 0\n",
    "    if mean_zr < 0:\n",
    "        allowed_min = mean_zr * 1.02\n",
    "        allowed_max = mean_zr * 0.98\n",
    "    else:\n",
    "        allowed_min = mean_zr * 0.98\n",
    "        allowed_max = mean_zr * 1.02\n",
    "    if len(zr_values) < 4:\n",
    "        is_straight = 0\n",
    "    else:\n",
    "        is_straight = 1\n",
    "    # FIXME: Ignores outliers for now, tracks with outliers will\n",
    "    # not likely be considered 'straight'.\n",
    "    for zr_value in zr_values:\n",
    "        if zr_value < allowed_min or zr_value > allowed_max:\n",
    "            num_outliers = num_outliers + 1\n",
    "            is_straight = 0\n",
    "    return (is_straight, num_outliers)\n",
    "\n",
    "def is_straight_track(track, labels, hits):\n",
    "    is_straight = 0\n",
    "    # Idea: Solve y=mx+b for each pair of hits, allow for a few outliers\n",
    "    # (one outlier will cause up to two bad values).\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "\n",
    "    df = hits.loc[hit_ix]\n",
    "    # Sort on abs(z)? Do we need to sort at all? We may want to find hits in the middle too....\n",
    "    df = df.sort_values('z')\n",
    "    hit_ix2 = df.index.values\n",
    "    \n",
    "    #x,y,z = df[['x', 'y', 'z' ]].values.astype(np.float32).T\n",
    "    #track_dims = np.column_stack((x, y, z/3))\n",
    "    #(m,b) = solve_linear_equation(track_dims)\n",
    "    #(is_straight, x_diffs, y_diffs) = check_is_track_straight(track_dims, m, b)\n",
    "    x,y,z,zr,v,azr = df[['x', 'y', 'z', 'zr', 'volume_id', 'azr' ]].values.astype(np.float32).T\n",
    "    track_dims = np.column_stack((x, y, z/3))\n",
    "    (is_straight, num_outliers) = check_if_zr_straight(zr)\n",
    "    if not is_straight and len(np.unique(v)) == 1 and num_outliers == 1 and len(hit_ix2) > 5:\n",
    "        print('Track : ' + str(track))# + ', possible outlier: ' + str(zr) + ', azr: ' + str(azr))\n",
    "    #if is_straight:\n",
    "    #    my_track = []\n",
    "    #    my_track.append(track_dims)\n",
    "    #    draw_prediction_xyz(my_track, my_track)\n",
    "    return is_straight, track_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_my_track(track, labels, hits):\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "    df = hits.loc[hit_ix]\n",
    "    \n",
    "    # Sort on abs(z)? Do we need to sort at all? We may want to find hits in the middle too....\n",
    "    df = df.sort_values('z')\n",
    "    hit_ix2 = df.index.values\n",
    "    x,y,z = df[['x', 'y', 'z' ]].values.astype(np.float32).T\n",
    "    track_dims = np.column_stack((x, y, z))\n",
    "    draw_prediction_xyz([track_dims], [track_dims])\n",
    "\n",
    "def draw_prediction(truth, predict, start=0, end=1):\n",
    "   \n",
    "    fig1 = plt.figure(figsize=(12,12))\n",
    "    ax1  = fig1.add_subplot(111, projection='3d')\n",
    "    fig1.patch.set_facecolor('white')\n",
    "    ax1.set_xlabel('x', fontsize=16)\n",
    "    ax1.set_ylabel('y', fontsize=16)\n",
    "    ax1.set_zlabel('z', fontsize=16)\n",
    "        \n",
    "\n",
    "    fig2 = plt.figure(figsize=(12,12))\n",
    "    ax2  = fig2.add_subplot(111, projection='3d')\n",
    "    fig2.patch.set_facecolor('white')\n",
    "    ax2.set_xlabel('a', fontsize=16)\n",
    "    ax2.set_ylabel('r', fontsize=16)\n",
    "    ax2.set_zlabel('z', fontsize=16)\n",
    "\n",
    "    for n in range(start,end,1):\n",
    "        x, y, z, v, l, m, a, r, zdr = truth[n].T\n",
    "        #x = r*np.cos(a)\n",
    "        #y = r*np.sin(a)\n",
    "        \n",
    "        ex, ey, ez, ev, el, em, ea, er, ezdr = predict[n].T\n",
    "        #ex = er*np.cos(ea)\n",
    "        #ey = er*np.sin(ea)\n",
    "        \n",
    "        color = np.random.uniform(0,1,3)\n",
    "        ax1.plot(ex,ey,ez,'.-',color = [0.75,0.75,0.75], markersize=10)\n",
    "        ax1.plot(x,y,z,'.-',color = color, markersize=5)\n",
    "        \n",
    "        \n",
    "        ax2.plot(ea,er,ez,'.-',color = [0.75,0.75,0.75], markersize=10)\n",
    "        ax2.plot(a,r,z,'.-',color = color, markersize=5)\n",
    "        if n==50: plt.show(1)\n",
    "\n",
    "def draw_prediction_xyz(truth, predict):\n",
    "   \n",
    "    fig1 = plt.figure(figsize=(12,12))\n",
    "    ax1  = fig1.add_subplot(111, projection='3d')\n",
    "    fig1.patch.set_facecolor('white')\n",
    "    ax1.set_xlabel('x', fontsize=16)\n",
    "    ax1.set_ylabel('y', fontsize=16)\n",
    "    ax1.set_zlabel('z', fontsize=16)\n",
    "        \n",
    "\n",
    "    #fig2 = plt.figure(figsize=(12,12))\n",
    "    #ax2  = fig2.add_subplot(111, projection='3d')\n",
    "    #fig2.patch.set_facecolor('white')\n",
    "    #ax2.set_xlabel('a', fontsize=16)\n",
    "    #ax2.set_ylabel('r', fontsize=16)\n",
    "    #ax2.set_zlabel('z', fontsize=16)\n",
    "\n",
    "    predict_size = len(predict)\n",
    "    #predict_size = 10\n",
    "    for n in range(0,predict_size,1):\n",
    "        x, y, z = truth[n].T\n",
    "        ex, ey, ez = predict[n].T\n",
    "        \n",
    "        color = np.random.uniform(0,1,3)\n",
    "        ax1.plot(ex,ey,ez,'.-',color = [0.75,0.75,0.75], markersize=10)\n",
    "        ax1.plot(x,y,z,'.-',color = color, markersize=5)\n",
    "        #ax1.axis('scaled')\n",
    "        #plt.gca().set_aspect('equal', adjustable='box')\n",
    "        plt.axis('equal')\n",
    "        \n",
    "        #if n==50: plt.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_outliers_zr(labels, hits):\n",
    "    labels = np.copy(labels)\n",
    "    tracks = np.unique(labels)\n",
    "    hits['z_abs'] = hits.z.abs()\n",
    "    hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "    hits['zr'] = hits['z'] / hits['r']\n",
    "    count_rem_zr_slope = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 4:\n",
    "            outliers = find_track_outliers_zr(track, labels, hits)\n",
    "            if len(outliers) > 0:\n",
    "                count_rem_zr_slope = count_rem_zr_slope + len(outliers)\n",
    "                for oix in outliers:\n",
    "                    labels[oix] = 0\n",
    "            \n",
    "    print('zr outliers removed: ' + str(count_rem_zr_slope))\n",
    "\n",
    "    return labels\n",
    "\n",
    "def safe_outlier_removal(labels, hits, truth, find_all=False, debug=False):\n",
    "    labels = np.copy(labels)\n",
    "    tracks = np.unique(labels)\n",
    "    hits['z_abs'] = hits.z.abs()\n",
    "    hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "    hits['zr'] = hits['z'] / hits['r']\n",
    "    count_removed = 0\n",
    "    count_not_removed = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 3:\n",
    "            outlier_ix = find_track_outliers_zr(track, labels, hits, find_all=find_all, truth=truth, debug=debug)\n",
    "            if len(outlier_ix) > 0:\n",
    "                tdf = truth.loc[track_hits]\n",
    "                truth_count = coll.Counter(tdf.particle_id.values).most_common(1)\n",
    "                truth_particle_id = truth_count[0][0]\n",
    "                for out_ix in outlier_ix:\n",
    "                    if tdf.loc[out_ix].particle_id != truth_particle_id:\n",
    "                        labels[out_ix] = 0\n",
    "                        count_removed = count_removed + 1\n",
    "                    else:\n",
    "                        count_not_removed = count_not_removed + 1\n",
    "\n",
    "    print('safe count_removed: ' + str(count_removed))\n",
    "    print('safe count_not_removed: ' + str(count_not_removed))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS HISTORY:\n",
    "# all +ve or all -ve, remove excessive slope: 13 awesome, 3 crappy --> 81%\n",
    "# most +ve, remove most -ve: 773 awesome, 310 crappy --> 71%\n",
    "# most +ve, remove extreme else remove most -ve: 793 awesome, 290 crappy --> 73%\n",
    "# most +ve, rem 1st opposing jump else rem extreme else rem most -ve: 799 awesome, 289 crappy --> 73%\n",
    "# most +ve, rem 1st opposing jump else rem extreme else rem most -ve > 2*mean: 739 awesome, 184 crappy --> 80%\n",
    "# same as above, remove final extreme outlier test: 719 awesome, 170 crappy --> 80.9%\n",
    "# reverse_opt tweak, 10x mean limit: 720 v 166, 81%\n",
    "# same as above, re-enabled extreme jump test: 743 awesome, 166 crappy --> 81.7%\n",
    "# most -ve, same as above: 302 awesome, 404 crappy --> 43%\n",
    "# most -ve, flip sign diff_zrs, opposing jump only: 113 awesome, 42 crappy --> 73%\n",
    "# most -ve, flip sign diff_zrs, opposing jump only, reverse opt for -ve jump1: 128 awesome, 21 crappy --> 86%\n",
    "# most -ve, same as above, remove extreme jump: 172 awesome, 27 crappy --> 86%\n",
    "# most -ve, all checks as +ve slopes, some -ve-specific checks: 456 awesome, 124 crappy --> 78.6%\n",
    "# Combined: 1199 awesome vs 294 crappy: 80.3%   (reverse opt mean fix in opposing jump: 1199 vs 290, 80.5%)\n",
    "# Combined with abs(mean) fix in opposing jumps: 1206 awesome, 284 crappy: 80.9%\n",
    "# Combined with abs(mean), favour 1st in opposing jumps: 1206 awesome, 276 crappy: 81.4%\n",
    "# Combined as above, limit to tracks >= 5 hits: 1166 awesome, 235 crappy: 83.2%\n",
    "# Combined, fix negative jump check, limit to >4x mean, 1108 v 227: 82.9%\n",
    "# +ve,-ve jump threshold 4x, fix negative finder, 1042 v 203: 83.7%\n",
    "# +ve,-ve jump threshold 2x, fix negative finder, 1155 vs 246: 82.4%\n",
    "# +ve,-ve jump threshold 3x: 1097 v 221: 83.2%\n",
    "# threshold 4x: 1042 v 204: 83.6%\n",
    "# threshold 4x, no smart 0/1 picker: 1048 v 197: 84.1%\n",
    "# threshold 2x, no smart 0/1 picker: 1166 v 235: 83.2%\n",
    "# fix diff_zrs, now 1154 v 238, 82.9%\n",
    "# fix diff_zrs, threshold 4x, 1091 v 215: 83.5%\n",
    "\n",
    "# 4: chop valley tail x2.5, opposing jumps std mean: 152 awesome, 22 crappy: 87%\n",
    "#  favour_1st_removal opposing jumps: 142 v 20, 87.6%\n",
    "#  + remove extreme: 160 v 23, 87.4%\n",
    "#  + remove final slope too large: 165 v 25, 86.8%\n",
    "#  + simple opposites compare prev diff: 235 v 46: 83.6%\n",
    "#  + fix simple opposites bug: 215 v 35: 86%\n",
    "#  + ensure valley decreases at beginning: 225 v 35: 86.5%  (274 v 44, 86.1%, biggest opposing)\n",
    "# Total 1,2,3,4: 1452 awesome, 319 crappy: 82%\n",
    "# AFTER ALL CRAZY CHANGES:\n",
    "# 3: 225 v 110, 67.1%\n",
    "# 3: check for bad initial slope earlier: 226 v 109, 67.5%\n",
    "# 3: -> and extreme opposite jump: 234 v 109, 68.2%\n",
    "# 3: -> and set extreme threshold to 10x (def. 3x?): 199 v 63, 76.0%\n",
    "# 3: -> remove negative extreme jump: 190 v 38, 83.3%\n",
    "# 3: -> positive extreme jump using abs. mean 10x: 142 v 23, 86%\n",
    "# 3: -> positive extreme jump using abs. mean 5x: 167 v 27, 86%\n",
    "# 3: -> positive extreme jump using abs. mean 10x: 138 v 22, 86.3%  (10/20 ext. 144 v 25, 85.2% )\n",
    "# 3:  169 v 34: 83.3%\n",
    "# 3: common 4/8: 159 v 30: 84.1%\n",
    "#----\n",
    "# 4: chop valley tail: 12 v 1, 92%\n",
    "# 4: remove first wrong slope: 37 v 5, 88%\n",
    "# 4: remove first wrong slope > 3x mean: 34 v 2, 94.4%\n",
    "# 4: add biggest opposing jump: 91 v 19, 82.7%\n",
    "# 4: add extreme opposing jumps: 100 v 24, 80.6%\n",
    "# 4: do not do reverse extreme opposing jump opt: 101 v 23, 81.4%\n",
    "# 4: favor first extreme opposing jump opt: 102 v 22, 82.3%\n",
    "# 4: extreme jump: 130 v 36, 78.3%\n",
    "# 4: 5x extreme jumps: 102 v 22, 82.3% -- 105 v 26 common code, 80.2% (107 v 28 common, 79.3%)\n",
    "#    10/20 ext 113 v 31, 78.5%\n",
    "# 4: 126 v 35, 78.3%\n",
    "# 4: common 4/8: 116 v 29, 80%\n",
    "#---\n",
    "# 0: first slope wrong: 11 v 0, 100%\n",
    "# 0: last slope wrong: 19 v 1, 95%\n",
    "# 0: biggest opposing jump: 137 v 23, 85.6%\n",
    "# 0: opposing extreme jumps: 170 v 34, 83.3%\n",
    "# 0: add pos/neg extreme jump removal: 171 v 34, 83.4%\n",
    "# 0: 8x pos/neg extreme jump removal: 177 v 35, 83.5%\n",
    "# 0: Reverse opt true, 165 v 32, 83.8%\n",
    "# 0: 168 v 33, 83.6%\n",
    "# 0: 3/8 split: 208 v 45: 82.2%\n",
    "# 0: common 4/8: 186 v 37: 83.4%\n",
    "#---\n",
    "# 1: 768 v 138, 84.8% (769 v 139 common code, 84.7%)\n",
    "#  --> common code! 779 v 128, 85.9%  (763 v 126 for common, 85.8%)\n",
    "#  --> 20/30 for all: 761 v 121, 86.3%\n",
    "#  --> common 4/8: 724 v 105, 87.3%\n",
    "#---\n",
    "# 2: 323 v 77, 80.8% (322 v 81 common code, 79.9%)\n",
    "#  --> common code! 328 v 71, 82.2%  (327 v 71 for common, 82.2%)\n",
    "#  --> 20/30 for all: 327 v 71, 82.2%\n",
    "#  --> common 4/8: 301 v 70, 81.1%\n",
    "#--------------------\n",
    "# OVERALL (5% cut-off): 1486 awesome, 271 crappy: 84.6%\n",
    "# 0.0% threshold to ignore: 2237 awesome, 752 crappy: 74.8% \n",
    "# 2.5% threshold to ignore: 1904 awesome, 396 crappy: 82.8%\n",
    "# 4.0% threshold to ignore: 1665 awesome, 319 crappy: 83.9%\n",
    "# 10.0% threshold to ignore: 664 awesome, 117 crappy: 85.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits['z_abs'] = hits.z.abs()\n",
    "hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "hits['zr'] = hits['z'] / hits['r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTENSION_ATTEMPT = 5\n",
    "EXTENSION_STANDARD_LIMITS = [0.02]#, 0.04, 0.06, 0.08, 0.10]\n",
    "\n",
    "\n",
    "l1 = np.copy(labels_helix1a)\n",
    "\n",
    "one_submission = create_one_event_submission(event_id, hits, l1)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Pre-extension score for event %d: %.8f\" % (event_id, score))\n",
    "\n",
    "for ix, limit in enumerate(EXTENSION_STANDARD_LIMITS):\n",
    "    l1 = extend_labels(ix, l1, hits, truth=truth, do_swap=ix%2==1, limit=(limit))\n",
    "    one_submission = create_one_event_submission(event_id, hits, l1)\n",
    "    score = score_event(truth, one_submission)\n",
    "    print(\"Extension score for event %d: %.8f\" % (event_id, score))\n",
    "\n",
    "one_submission = create_one_event_submission(event_id, hits, l1)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Post-extension score for event %d: %.8f\" % (event_id, score))\n",
    "# 0.54586603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTENSION_ATTEMPT = 5\n",
    "EXTENSION_STANDARD_LIMITS = [0.02]#, 0.04, 0.06, 0.08, 0.10]\n",
    "\n",
    "\n",
    "l2 = np.copy(labels_helix1a)\n",
    "\n",
    "one_submission = create_one_event_submission(event_id, hits, l2)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Pre-extension score for event %d: %.8f\" % (event_id, score))\n",
    "\n",
    "for ix, limit in enumerate(EXTENSION_STANDARD_LIMITS):\n",
    "    l2 = extend_labels(ix, l2, hits, truth=truth, do_swap=ix%2==1, limit=(limit))\n",
    "    one_submission = create_one_event_submission(event_id, hits, l2)\n",
    "    score = score_event(truth, one_submission)\n",
    "    print(\"Pre-extension score for event %d: %.8f\" % (event_id, score))\n",
    "\n",
    "one_submission = create_one_event_submission(event_id, hits, l2)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Post-extension score for event %d: %.8f\" % (event_id, score))\n",
    "# 0.55049849\n",
    "# 0.55770785 for min track length of 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "l3 = np.copy(labels_helix1)\n",
    "\n",
    "one_submission = create_one_event_submission(event_id, hits, l3)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Pre-extension score for event %d: %.8f\" % (event_id, score))\n",
    "\n",
    "#extensions = [0.02]\n",
    "extensions = EXTENSION_STANDARD_LIMITS\n",
    "for ix, limit in enumerate(extensions):\n",
    "    l3 = extend_labels(ix, l3, hits, truth=truth, do_swap=ix%2==1, limit=(limit))\n",
    "    one_submission = create_one_event_submission(event_id, hits, l3)\n",
    "    score = score_event(truth, one_submission)\n",
    "    print(\"Extension score for event %d: %.8f\" % (event_id, score))\n",
    "\n",
    "one_submission = create_one_event_submission(event_id, hits, l3)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Post-extension score for event %d: %.8f\" % (event_id, score))\n",
    "# Initial attempt: 0.54601485 (slightly better - orig: 0.54584)\n",
    "# Initial scoring: 0.54450423\n",
    "# Scoring v2, good_len + priority for longer lengths: 0.54905218\n",
    "# Scoring v3, add -0.25 modifier if new track is considered outlier: 0.54951588\n",
    "#  -0.5 modifier: 0.54946390\n",
    "#  -0.2 modifier: 0.54946713\n",
    "#  -0.3 modifier: 0.54952876\n",
    "#  10% hit: 0.54934390\n",
    "#  20% hit: 0.54951323\n",
    "#  30% hit: 0.54936905\n",
    "#  25% hit: 0.54952633\n",
    "# Limit 0.08 and 0.10 have much higher noise\n",
    "# Always use 0.75 modifier for outliers: 0.4787, 0.5504, 0.5909, 0.6054, 0.6099, 0.6107\n",
    "#   use 0.6 above limit 0.6: 0.4787, 0.5504, 0.5909, 0.6054, 0.6098, 0.6108 \n",
    "#   use 0.9 above limit 0.6: 0.4787, 0.5504, 0.5909, 0.6054, 0.6093, 0.6098\n",
    "#   use 0.5 above limit 0.6: 0.4787, 0.5504, 0.5909, 0.6054, 0.6098, 0.6108\n",
    "# with small track fix:      0.4787, 0.5504, 0.5911, 0.6059, 0.6101, 0.6109\n",
    "# in-order from large track: 0.4787, 0.5504, 0.5910, 0.6057, 0.6099, 0.6104\n",
    "# in-order from large slice: 0.4787, 0.5505, 0.5912, 0.6062, 0.6105, 0.6111\n",
    "# cleaned up code:           0.4787, 0.5505, 0.5913, 0.6063, 0.6106, 0.6120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hack_one_extend(labels, hits, truth):\n",
    "    angle=82\n",
    "    limit=0.04\n",
    "    df = hits.copy(deep=True)\n",
    "    df['track_id'] = labels.tolist()\n",
    "    df = df.assign(d = np.sqrt( df.x**2 + df.y**2 + df.z**2 ))\n",
    "    df = df.assign(r = np.sqrt( df.x**2 + df.y**2))\n",
    "    df = df.assign(arctan2 = np.arctan2(df.z, df.r))\n",
    "    df = _one_cone_slice(df, truth, angle, 1, limit)\n",
    "    return df.track_id.values\n",
    "\n",
    "def extend(iter, df, truth=None, do_swap=False, limit=0.04):\n",
    "    if do_swap:\n",
    "        df = df.assign(x = -df.x)\n",
    "        df = df.assign(y = -df.y)\n",
    "\n",
    "    df = df.assign(d = np.sqrt( df.x**2 + df.y**2 + df.z**2 ))\n",
    "    df = df.assign(r = np.sqrt( df.x**2 + df.y**2))\n",
    "    df = df.assign(arctan2 = np.arctan2(df.z, df.r))\n",
    "\n",
    "    count_good = 0\n",
    "    count_bad = 0\n",
    "    count_out_correct = 0\n",
    "    count_out_wrong = 0\n",
    "\n",
    "    for angle in range(-90,90,1):\n",
    "\n",
    "        print ('\\r%d %f '%(iter,angle), end='',flush=True)\n",
    "        df1 = df.loc[(df.arctan2>(angle-1.0)/180*np.pi) & (df.arctan2<(angle+1.0)/180*np.pi)]\n",
    "\n",
    "        num_hits = len(df1)\n",
    "        # Dynamically adjust the delta based on how many hits are found\n",
    "        if num_hits > 2000:\n",
    "            (df, cg1, cb1, coc, cow) = _one_cone_slice_orig(df, truth, angle-0.6, 0.4, limit)\n",
    "            count_good = count_good + cg1\n",
    "            count_bad = count_bad + cb1\n",
    "            count_out_correct = count_out_correct + coc\n",
    "            count_out_wrong = count_out_wrong + cow\n",
    "            (df, cg1, cb1, coc, cow) = _one_cone_slice_orig(df, truth, angle-0.2, 0.4, limit)\n",
    "            count_good = count_good + cg1\n",
    "            count_bad = count_bad + cb1\n",
    "            count_out_correct = count_out_correct + coc\n",
    "            count_out_wrong = count_out_wrong + cow\n",
    "            (df, cg1, cb1, coc, cow) = _one_cone_slice_orig(df, truth, angle+0.2, 0.4, limit)\n",
    "            count_good = count_good + cg1\n",
    "            count_bad = count_bad + cb1\n",
    "            count_out_correct = count_out_correct + coc\n",
    "            count_out_wrong = count_out_wrong + cow\n",
    "            (df, cg1, cb1, coc, cow) = _one_cone_slice_orig(df, truth, angle+0.6, 0.4, limit)\n",
    "            count_good = count_good + cg1\n",
    "            count_bad = count_bad + cb1\n",
    "            count_out_correct = count_out_correct + coc\n",
    "            count_out_wrong = count_out_wrong + cow\n",
    "        else:\n",
    "            (df, cg1, cb1, coc, cow) = _one_cone_slice_orig(df, truth, angle, 1, limit)\n",
    "            count_good = count_good + cg1\n",
    "            count_bad = count_bad + cb1\n",
    "            count_out_correct = count_out_correct + coc\n",
    "            count_out_wrong = count_out_wrong + cow\n",
    "\n",
    "    print('Count properly extended: ' + str(count_good))\n",
    "    print('Count badly extended: ' + str(count_bad))\n",
    "    print('Count outlier predicted properly: ' + str(count_out_correct))\n",
    "    print('Count outlier incorrect: ' + str(count_out_wrong))\n",
    "    return df\n",
    "\n",
    "def extend_labels(iter, labels, hits, truth=None, do_swap=False, limit=0.04):\n",
    "    df = hits.copy(deep=True)\n",
    "    df['track_id'] = labels.tolist()\n",
    "    return extend(iter, df, truth, do_swap, limit).track_id.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_outlier_len(track_id, labels, hits, outlier_ix):\n",
    "    labels = np.copy(labels)\n",
    "    orig_len = len(np.where(labels == track_id)[0])\n",
    "    contains_outlier = False\n",
    "    outliers = find_track_outliers_zr(track_id, labels, hits, find_all=True)\n",
    "    if not contains_outlier:\n",
    "        contains_outlier = (outlier_ix in outliers)\n",
    "    for outx in outliers:\n",
    "        labels[outx] = 0\n",
    "    if len(outliers) > 0:\n",
    "        outliers = find_track_outliers_zr(track_id, labels, hits, find_all=True)\n",
    "        if not contains_outlier:\n",
    "            contains_outlier = (outlier_ix in outliers)\n",
    "        for outx in outliers:\n",
    "            labels[outx] = 0\n",
    "        if len(outliers) > 0:\n",
    "            outliers = find_track_outliers_zr(track_id, labels, hits, find_all=True)\n",
    "            if not contains_outlier:\n",
    "                contains_outlier = (outlier_ix in outliers)\n",
    "            for outx in outliers:\n",
    "                labels[outx] = 0\n",
    "            if len(outliers) > 0:\n",
    "                outliers = find_track_outliers_zr(track_id, labels, hits, find_all=True)\n",
    "                if not contains_outlier:\n",
    "                    contains_outlier = (outlier_ix in outliers)\n",
    "                for outx in outliers:\n",
    "                    labels[outx] = 0\n",
    "    new_len = len(np.where(labels == track_id)[0])\n",
    "    return (orig_len, new_len, contains_outlier)\n",
    "\n",
    "def calculate_track_score(track_id, labels, hits, outlier_modifier=0.75, outlier_ix=-1):\n",
    "    (cur_len, no_outlier_len, has_outlier) = non_outlier_len(track_id, labels, hits, outlier_ix)\n",
    "    modifier=1.0\n",
    "    if has_outlier:\n",
    "        modifier=outlier_modifier\n",
    "    score1 = no_outlier_len / cur_len\n",
    "    if cur_len < 4:\n",
    "        score2 = 0\n",
    "    else:\n",
    "        score2 = min(cur_len/20.0, 1.0)\n",
    "    return ((score1 + score2) * modifier)/2.0\n",
    "\n",
    "def _one_cone_slice_new(df, truth, angle, delta_angle, limit=0.04, num_neighbours=18):\n",
    "\n",
    "    df1 = df.loc[(df.arctan2>(angle - delta_angle)/180*np.pi) & (df.arctan2<(angle + delta_angle)/180*np.pi)]\n",
    "\n",
    "    min_num_neighbours = len(df1)\n",
    "    if min_num_neighbours < 3: \n",
    "        return df, 0, 0, 0, 0\n",
    "\n",
    "    hit_ids = df1.hit_id.values\n",
    "    x,y,z = df1.as_matrix(columns=['x', 'y', 'z']).T\n",
    "    r  = (x**2 + y**2)**0.5\n",
    "    r  = r/1000\n",
    "    a  = np.arctan2(y,x)\n",
    "    c = np.cos(a)\n",
    "    s = np.sin(a)\n",
    "    tree = KDTree(np.column_stack([c,s,r]), metric='euclidean')\n",
    "\n",
    "    track_ids = list(df1.track_id.unique())\n",
    "    num_track_ids = len(track_ids)\n",
    "    min_length=2\n",
    "    \n",
    "    labels = df.track_id.values\n",
    "    count_good = 0\n",
    "    count_bad = 0\n",
    "    count_outlier_correct = 0\n",
    "    count_outlier_wrong = 0\n",
    "\n",
    "    label_track_counts = coll.Counter(df1.track_id.values).most_common(num_track_ids)\n",
    "    \n",
    "    for track_count in label_track_counts:\n",
    "        p = track_count[0]\n",
    "        if p == 0: continue\n",
    "\n",
    "        idx = np.where(df1.track_id==p)[0]\n",
    "        cur_track_len = len(idx)\n",
    "\n",
    "        if cur_track_len<min_length: continue\n",
    "        label_idx = np.where(labels==p)[0]\n",
    "\n",
    "        # Un-comment following code to find the truth particle ID for the track.\n",
    "        #truth_ix = []\n",
    "        #for ii in idx:\n",
    "        #    truth_ix.append(hit_ids[ii] - 1)\n",
    "        #tdf = truth.loc[truth_ix]\n",
    "        #truth_count = coll.Counter(tdf.particle_id.values).most_common(1)\n",
    "        #truth_particle_id = truth_count[0][0]\n",
    "        #print('track: ' + str(p) + ', len: ' + str(len(idx)) + ', idx: ' + str(idx))\n",
    "        #print('truth particle: ' + str(truth_particle_id) + ', count:' + str(truth_count[0][1]))\n",
    "            \n",
    "        if angle>0:\n",
    "            idx = idx[np.argsort( z[idx])]\n",
    "        else:\n",
    "            idx = idx[np.argsort(-z[idx])]\n",
    "\n",
    "        ## start and end points  ##\n",
    "        idx0,idx1 = idx[0],idx[-1]\n",
    "        a0 = a[idx0]\n",
    "        a1 = a[idx1]\n",
    "        r0 = r[idx0]\n",
    "        r1 = r[idx1]\n",
    "        c0 = c[idx0]\n",
    "        c1 = c[idx1]\n",
    "        s0 = s[idx0]\n",
    "        s1 = s[idx1]\n",
    "\n",
    "        da0 = a[idx[1]] - a[idx[0]]  #direction\n",
    "        dr0 = r[idx[1]] - r[idx[0]]\n",
    "        direction0 = np.arctan2(dr0,da0)\n",
    "\n",
    "        da1 = a[idx[-1]] - a[idx[-2]]\n",
    "        dr1 = r[idx[-1]] - r[idx[-2]]\n",
    "        direction1 = np.arctan2(dr1,da1)\n",
    "\n",
    "        ## extend start point\n",
    "        ns = tree.query([[c0, s0, r0]], k=min(num_neighbours, min_num_neighbours), return_distance=False)\n",
    "        ns = np.concatenate(ns)\n",
    "\n",
    "        direction = np.arctan2(r0 - r[ns], a0 - a[ns])\n",
    "        diff = 1 - np.cos(direction - direction0)\n",
    "        ns = ns[(r0 - r[ns] > 0.01) & (diff < (1 - np.cos(limit)))]\n",
    "        for n in ns:\n",
    "            df_ix = hit_ids[n] - 1\n",
    "            # Un-comment this to see if we are extending the track properly\n",
    "            #is_good = (truth.loc[df_ix, 'particle_id'] == truth_particle_id)\n",
    "\n",
    "            outlier_modifier = 0.75\n",
    "            orig_label = labels[df_ix]\n",
    "            labels[df_ix] = p\n",
    "            new_score = score2.calculate_track_score(p, labels, hits, outlier_modifier=outlier_modifier, outlier_ix=df_ix)\n",
    "            labels[df_ix] = orig_label\n",
    "            if orig_label != 0:\n",
    "                orig_score = score2.calculate_track_score(orig_label, labels, hits, outlier_modifier=outlier_modifier, outlier_ix=df_ix)\n",
    "            else:\n",
    "                orig_score = 0\n",
    "\n",
    "            if new_score >= orig_score:\n",
    "                #df.loc[df_ix, 'track_id'] = p\n",
    "                labels[df_ix] = p\n",
    "                cur_track_len = cur_track_len + 1\n",
    "\n",
    "        ## extend end point\n",
    "        ns = tree.query([[c1, s1, r1]], k=min(num_neighbours, min_num_neighbours), return_distance=False)\n",
    "        ns = np.concatenate(ns)\n",
    "\n",
    "        direction = np.arctan2(r[ns] - r1, a[ns] - a1)\n",
    "        diff = 1 - np.cos(direction - direction1)\n",
    "  \n",
    "        ns = ns[(r[ns] - r1 > 0.01) & (diff < (1 - np.cos(limit)))]\n",
    "        for n in ns:  \n",
    "            df_ix = hit_ids[n] - 1\n",
    "            # Un-comment this to see if we are extending the track properly\n",
    "            #is_good = (truth.loc[df_ix, 'particle_id'] == truth_particle_id)\n",
    "\n",
    "            outlier_modifier=0.75\n",
    "            orig_label = labels[df_ix]\n",
    "            labels[df_ix] = p\n",
    "            new_score = score2.calculate_track_score(p, labels, hits, outlier_modifier=outlier_modifier, outlier_ix=df_ix)\n",
    "            labels[df_ix] = orig_label\n",
    "            if orig_label != 0:\n",
    "                orig_score = score2.calculate_track_score(orig_label, labels, hits, outlier_modifier=outlier_modifier, outlier_ix=df_ix)\n",
    "            else:\n",
    "                orig_score = 0\n",
    "\n",
    "            if new_score >= orig_score:\n",
    "                #df.loc[df_ix, 'track_id'] = p\n",
    "                labels[df_ix] = p\n",
    "                cur_track_len = cur_track_len + 1\n",
    "\n",
    "    df['track_id'] = labels\n",
    "\n",
    "    return df, count_good, count_bad, count_outlier_correct, count_outlier_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _one_cone_slice_orig(df, truth, angle, delta_angle, limit=0.04, num_neighbours=18):\n",
    "\n",
    "    df1 = df.loc[(df.arctan2>(angle - delta_angle)/180*np.pi) & (df.arctan2<(angle + delta_angle)/180*np.pi)]\n",
    "\n",
    "    min_num_neighbours = len(df1)\n",
    "    if min_num_neighbours < 3: \n",
    "        return df, 0, 0, 0, 0\n",
    "\n",
    "    hit_ids = df1.hit_id.values\n",
    "    x,y,z = df1.as_matrix(columns=['x', 'y', 'z']).T\n",
    "    r  = (x**2 + y**2)**0.5\n",
    "    r  = r/1000\n",
    "    a  = np.arctan2(y,x)\n",
    "    c = np.cos(a)\n",
    "    s = np.sin(a)\n",
    "    tree = KDTree(np.column_stack([c,s,r]), metric='euclidean')\n",
    "\n",
    "    track_ids = list(df1.track_id.unique())\n",
    "    num_track_ids = len(track_ids)\n",
    "    min_length=2\n",
    "    \n",
    "    labels = df.track_id.values\n",
    "    count_good = 0\n",
    "    count_bad = 0\n",
    "    count_outlier_correct = 0\n",
    "    count_outlier_wrong = 0\n",
    "\n",
    "    \n",
    "    for i in range(num_track_ids):\n",
    "        p = track_ids[i]\n",
    "        if p==0: continue\n",
    "\n",
    "        idx = np.where(df1.track_id==p)[0]\n",
    "        cur_track_len = len(idx)\n",
    "        if cur_track_len<min_length: continue\n",
    "\n",
    "        truth_ix = []\n",
    "        for ii in idx:\n",
    "            truth_ix.append(hit_ids[ii] - 1)\n",
    "        tdf = truth.loc[truth_ix]\n",
    "        truth_count = coll.Counter(tdf.particle_id.values).most_common(1)\n",
    "        truth_particle_id = truth_count[0][0]\n",
    "        #print('track: ' + str(p) + ', len: ' + str(len(idx)) + ', idx: ' + str(idx))\n",
    "        #print('truth particle: ' + str(truth_particle_id) + ', count:' + str(truth_count[0][1]))\n",
    "            \n",
    "        if angle>0:\n",
    "            idx = idx[np.argsort( z[idx])]\n",
    "        else:\n",
    "            idx = idx[np.argsort(-z[idx])]\n",
    "\n",
    "## start and end points  ##\n",
    "        idx0,idx1 = idx[0],idx[-1]\n",
    "        a0 = a[idx0]\n",
    "        a1 = a[idx1]\n",
    "        r0 = r[idx0]\n",
    "        r1 = r[idx1]\n",
    "        c0 = c[idx0]\n",
    "        c1 = c[idx1]\n",
    "        s0 = s[idx0]\n",
    "        s1 = s[idx1]\n",
    "\n",
    "        da0 = a[idx[1]] - a[idx[0]]  #direction\n",
    "        dr0 = r[idx[1]] - r[idx[0]]\n",
    "        direction0 = np.arctan2(dr0,da0)\n",
    "\n",
    "        da1 = a[idx[-1]] - a[idx[-2]]\n",
    "        dr1 = r[idx[-1]] - r[idx[-2]]\n",
    "        direction1 = np.arctan2(dr1,da1)\n",
    "\n",
    "        ## extend start point\n",
    "        ns = tree.query([[c0, s0, r0]], k=min(num_neighbours, min_num_neighbours), return_distance=False)\n",
    "        ns = np.concatenate(ns)\n",
    "\n",
    "        direction = np.arctan2(r0 - r[ns], a0 - a[ns])\n",
    "        diff = 1 - np.cos(direction - direction0)\n",
    "        ns = ns[(r0 - r[ns] > 0.01) & (diff < (1 - np.cos(limit)))]\n",
    "        for n in ns:\n",
    "            df_ix = hit_ids[n] - 1\n",
    "            is_good = (truth.loc[df_ix, 'particle_id'] == truth_particle_id)\n",
    "            #if truth.loc[df_ix, 'particle_id'] == truth_particle_id:\n",
    "            #    print('Found valid start extension!')\n",
    "            #else:\n",
    "            #    print('Found WRONG start extension!')\n",
    "                \n",
    "            #old_track = df.loc[df_ix, 'track_id']\n",
    "            old_track = labels[df_ix]\n",
    "            if old_track == 0:# and is_good:# and allow_overwrite:# and is_good:\n",
    "                #df.loc[df_ix, 'track_id'] = p\n",
    "                labels[df_ix] = p\n",
    "                if is_good:\n",
    "                    count_good = count_good + 1\n",
    "                else:\n",
    "                    count_bad = count_bad + 1\n",
    "                # increase cur_track_len?\n",
    "            elif old_track != 0:\n",
    "                # If the hit is already occupied by another track, only take ownership\n",
    "                # of the hit if our track is longer than the current-occupying track.\n",
    "                #existing_track_len = len(np.where(df.track_id==old_track)[0])\n",
    "                existing_track_len = len(np.where(labels==old_track)[0])\n",
    "                if cur_track_len > existing_track_len:\n",
    "                    #df.loc[df_ix, 'track_id'] = p\n",
    "                    labels[df_ix] = p\n",
    "                    if is_good:\n",
    "                        count_good = count_good + 1\n",
    "                    else:\n",
    "                        count_bad = count_bad + 1\n",
    "                else:\n",
    "                    if is_good:\n",
    "                        count_bad = count_bad + 1\n",
    "                    else:\n",
    "                        count_good = count_good + 1\n",
    "\n",
    "        ## extend end point\n",
    "        ns = tree.query([[c1, s1, r1]], k=min(num_neighbours, min_num_neighbours), return_distance=False)\n",
    "        ns = np.concatenate(ns)\n",
    "\n",
    "        direction = np.arctan2(r[ns] - r1, a[ns] - a1)\n",
    "        diff = 1 - np.cos(direction - direction1)\n",
    "  \n",
    "        ns = ns[(r[ns] - r1 > 0.01) & (diff < (1 - np.cos(limit)))]\n",
    "        for n in ns:  \n",
    "            df_ix = hit_ids[n] - 1\n",
    "            is_good = (truth.loc[df_ix, 'particle_id'] == truth_particle_id)\n",
    "            #if truth.loc[df_ix, 'particle_id'] == truth_particle_id:\n",
    "            #    print('Found valid end extension!')\n",
    "            #else:\n",
    "            #    print('Found WRONG end extension!')\n",
    "            #old_track = df.loc[df_ix, 'track_id']\n",
    "            old_track = labels[df_ix]\n",
    "            if old_track == 0:# and allow_overwrite:# and is_good:\n",
    "                #df.loc[df_ix, 'track_id'] = p\n",
    "                labels[df_ix] = p\n",
    "                if is_good:\n",
    "                    count_good = count_good + 1\n",
    "                else:\n",
    "                    count_bad = count_bad + 1\n",
    "            elif old_track != 0:\n",
    "                # If the hit is already occupied by another track, only take ownership\n",
    "                # of the hit if our track is longer than the current-occupying track.\n",
    "                #existing_track_len = len(np.where(df.track_id==old_track)[0])\n",
    "                existing_track_len = len(np.where(labels==old_track)[0])\n",
    "                if cur_track_len > existing_track_len:\n",
    "                    #df.loc[df_ix, 'track_id'] = p\n",
    "                    labels[df_ix] = p\n",
    "                    if is_good:\n",
    "                        count_good = count_good + 1\n",
    "                    else:\n",
    "                        count_bad = count_bad + 1\n",
    "                else:\n",
    "                    if is_good:\n",
    "                        count_bad = count_bad + 1\n",
    "                    else:\n",
    "                        count_good = count_good + 1\n",
    "                        \n",
    "    df['track_id'] = labels\n",
    "      \n",
    "    return df, count_good, count_bad, count_outlier_correct, count_outlier_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_proc = np.copy(labels_merged)\n",
    "labels_proc = perfect_inner_extension(labels_proc, hits, truth)\n",
    "one_submission = create_one_event_submission(event_id, hits, labels_proc)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Extension %d score for event %d: %.8f\" % (ix,event_id, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_i1 = pd.read_csv('../../best_csvs12/event_' + str(event_id) + '_labels_train_helix1_phase1_dbscan1.csv').label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_proc1 = np.copy(labels_i1)\n",
    "#track_extension_limits = [0.02, 0.04, 0.06, 0.08, 0.10]\n",
    "#for ix, limit in enumerate(track_extension_limits):\n",
    "#    labels_proc1 = extend_labelsi(ix, labels_proc1, hits, do_swap=(ix%2==1), limit=(limit))\n",
    "#    one_submission = create_one_event_submission(event_id, hits, labels_proc1)\n",
    "#    score = score_event(truth, one_submission)\n",
    "#    print(\"Extension1 %d score for event %d: %.8f\" % (ix, event_id, score))\n",
    "\n",
    "labels_proc2 = np.copy(labels_i1)\n",
    "track_extension_limits = [0.02, 0.04, 0.06, 0.08, 0.10]\n",
    "for ix, limit in enumerate(track_extension_limits):\n",
    "    labels_proc2 = extend_labelsi(ix, labels_proc2, hits, do_swap=(ix%2==1), limit=(limit))\n",
    "    one_submission = create_one_event_submission(event_id, hits, labels_proc2)\n",
    "    score = score_event(truth, one_submission)\n",
    "    print(\"Extension2 %d score for event %d: %.8f\" % (ix, event_id, score))\n",
    "# 0.56229910, 0.59463552, 0.60778460, 0.61425334, 0.61669246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_proc1 = np.copy(labels_proc2)\n",
    "track_extension_limits = [0.02, 0.04, 0.06, 0.08, 0.10]\n",
    "for ix, limit in enumerate(track_extension_limits):\n",
    "    labels_proc1 = extend_labelsi(ix, labels_proc1, hits, do_swap=(ix%2==1), limit=(limit))\n",
    "    one_submission = create_one_event_submission(event_id, hits, labels_proc1)\n",
    "    score = score_event(truth, one_submission)\n",
    "    print(\"Extension1 %d score for event %d: %.8f\" % (ix, event_id, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_proc = np.copy(labels_merged)\n",
    "track_extension_limits = [0.02, 0.04, 0.06, 0.08, 0.10]\n",
    "for ix, limit in enumerate(track_extension_limits):\n",
    "    labels_proc = extend_labelsi(ix, labels_proc, hits, do_swap=(ix%2==1), limit=(limit))\n",
    "    one_submission = create_one_event_submission(event_id, hits, labels_proc)\n",
    "    score = score_event(truth, one_submission)\n",
    "    print(\"Extension %d score for event %d: %.8f\" % (ix, event_id, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perfect_inner_extension(labels, hits, truth):\n",
    "    labels = np.copy(labels)\n",
    "    truth['z_abs'] = truth.tz.abs()\n",
    "    tracks = np.unique(labels)\n",
    "    count_extended = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 5:\n",
    "            tdf = truth.loc[track_hits]\n",
    "            tdf = tdf.sort_values('z_abs')\n",
    "            zs = tdf.z_abs.values\n",
    "            truth_count = coll.Counter(tdf.particle_id.values).most_common(1)\n",
    "            truth_particle_id = truth_count[0][0]\n",
    "            tr2_df = truth.loc[truth['particle_id'] == truth_particle_id]\n",
    "            tr2_df = tr2_df.sort_values('z_abs')\n",
    "            tr2_ixes = tr2_df.index.values\n",
    "            for tr2_ix in tr2_ixes:\n",
    "                if tr2_df.loc[tr2_ix].z_abs > zs[0] and tr2_df.loc[tr2_ix].z_abs < zs[-1]:\n",
    "                    count_extended = count_extended + 1\n",
    "                    labels[tr2_ix] = track\n",
    "\n",
    "    print('perfect count_extended: ' + str(count_extended))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_helix1_filter = merge.remove_outliers(labels_helix1, hits, smallest_track_size=2, print_counts=True)\n",
    "#labels_helix2_filter = merge.remove_outliers(labels_helix2, hits, smallest_track_size=2, print_counts=True)\n",
    "labels_merged = merge.heuristic_merge_tracks(labels_helix1_filter, labels_helix2_filter, overwrite_limit=3)\n",
    "#labels_merged = merge.heuristic_merge_tracks(labels_merged, labels_helix3_filter)\n",
    "one_submission = create_one_event_submission(event_id, hits, labels_merged)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merged score for event %d: %.8f\" % (event_id, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_submission = create_one_event_submission(1000, hits, labels_helix)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"helix score for event %d: %.8f\" % (1000, score))\n",
    "#one_submission = create_one_event_submission(1000, hits, labels_cone)\n",
    "#score = score_event(truth, one_submission)\n",
    "#print(\"cone score for event %d: %.8f\" % (1000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_simple_merge = merge.merge_tracks(labels_helix, labels_cone)\n",
    "one_submission = create_one_event_submission(1000, hits, labels_simple_merge)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Simple merged cone+helix score for event %d: %.8f\" % (1000, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Removing cone outliers')\n",
    "labels_c3 = np.copy(labels_cone)\n",
    "labels_c3 = remove_outliers(labels_c3, hits, print_counts=True)\n",
    "\n",
    "print('Removing helix outliers')\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "labels_h3 = remove_outliers(labels_h3, hits, print_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Merging tracks')\n",
    "tmp_labels = merge.heuristic_merge_tracks(labels_helix, labels_cone)\n",
    "one_submission = create_one_event_submission(1000, hits, tmp_labels)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"cone score for event %d: %.8f\" % (1000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Merging tracks')\n",
    "tmp_labels = merge.heuristic_merge_tracks(labels_h3, labels_c3)\n",
    "one_submission = create_one_event_submission(1000, hits, tmp_labels)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merged score for event %d: %.8f\" % (1000, score))\n",
    "# Baseline helix prior to outlier removal: 0.51217316\n",
    "# Score: 0.4766-->0.4799 after improved outlier removal\n",
    "# Score is 0.5037 if, after outlier removal,  we only add tracks from labels_c3 when no such track was recorded in labels_h3\n",
    "# Score is 0.51087490 if we only remove bad volumes, duplicatez, and singletons, and only add tracks from labels_c3 when no such track was recorded in labels_h3\n",
    "# Score is 0.51936460, remove badv, dupz, sings, selective track merging.\n",
    "# Score is 0.52234651, remove badv, dupz, sings, selective track merging.\n",
    "# Score is 0.52499622, remove badv, dupz, sings, selective track merging, overwrite smaller tracks of length <= 3.\n",
    "# Score is 0.52653574, same as above except overwrite tracks of length <= 4\n",
    "# Score is 0.52622554, same as above except overwrite tracks of length <= 5\n",
    "# Score is 0.52209245, full outlier removal, overwrite tracks of length <= 4\n",
    "# Score is 0.58658664 orig heuristic\n",
    "#  --> 0.58240360 with aggressive outlier removal\n",
    "#  --> 0.58621259 with aggressive cone removal, non-aggressive helix\n",
    "#  --> 0.58417970 with aggressive helix removal, non-aggressive cone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: Need to evaluate this better, seems to hurt!\n",
    "def find_invalid_volumes(track, labels, df):\n",
    "    invalid_ix = []\n",
    "\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "    df2 = df.loc[hit_ix]\n",
    "    df2 = df2.sort_values('z_abs')\n",
    "    hit_ix2 = df2.index.values\n",
    "    all_positive = np.all(df2.z.values >= 0)\n",
    "    all_negative = np.all(df2.z.values <= 0)\n",
    "    volumes = df2.volume_id.values\n",
    "    layers = df2.layer_id.values\n",
    "    last_volume = volumes[0]\n",
    "    last_layer = layers[0]\n",
    "    # Tracks with the first volume of 8, 13, and 17 are very odd, sometimes\n",
    "    # they hit in the negative way, sometimes the positive way,\n",
    "    # sometimes a mix of both. Ignore these.\n",
    "    if last_volume == 8 or last_volume == 13 or last_volume == 17:\n",
    "        all_negative = False\n",
    "        all_positive = False\n",
    "    for idx, cur_vol in enumerate(volumes):\n",
    "        cur_layer = layers[idx]\n",
    "        if all_positive:\n",
    "            # When we go from one volume to the next, we expect to see the\n",
    "            # layer drop from a large layer id to a smaller layer id.\n",
    "            # If we stay in the same volume, the layer id should not decrease.\n",
    "            #if (last_volume != cur_vol and (cur_layer > (last_layer - 4))) or (last_volume == cur_vol and cur_layer < last_layer):\n",
    "            if (last_volume == cur_vol and cur_layer < last_layer):\n",
    "                invalid_ix.append(hit_ix2[idx])\n",
    "            else:\n",
    "                last_volume = cur_vol\n",
    "                last_layer = cur_layer\n",
    "        elif all_negative:\n",
    "            # When we go from one volume to the next, we expect to see the\n",
    "            # layer increase from a small layer id to a larger layer id.\n",
    "            # If we stay in the same volume, the layer id should not increase.\n",
    "            #if (last_volume != cur_vol and (cur_layer < (last_layer + 4))) or (last_volume == cur_vol and cur_layer > last_layer):\n",
    "            if (last_volume == cur_vol and cur_layer > last_layer):\n",
    "                invalid_ix.append(hit_ix2[idx])\n",
    "            else:\n",
    "                last_volume = volumes[idx]\n",
    "                last_layer = layers[idx]\n",
    "        else:\n",
    "            last_volume = cur_vol\n",
    "            last_layer = cur_layer\n",
    "\n",
    "    return invalid_ix\n",
    "    \n",
    "def find_dimension_outlier(track, labels, df, dimension):\n",
    "    outlier_ix = []\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "\n",
    "    # Need at least 3 values to determine if any look like outliers\n",
    "    if len(hit_ix) < 3:\n",
    "        return outlier_ix\n",
    "\n",
    "    df2 = df.loc[hit_ix]        \n",
    "    df2 = df2.sort_values('z')\n",
    "    hit_ix2 = df2.index.values\n",
    "\n",
    "    # Note, diff[0] is diff between 0 and 1\n",
    "    diffs = np.diff(df2[dimension].values)\n",
    "\n",
    "    growing_trend = 0\n",
    "    shrinking_trend = 0\n",
    "    for idx, diff in enumerate(diffs):\n",
    "        if idx > 0 and diff > diffs[idx-1]:\n",
    "            growing_trend = growing_trend + 1\n",
    "        if idx > 0 and diff < diffs[idx-1]:\n",
    "            shrinking_trend = shrinking_trend + 1\n",
    "\n",
    "    check_largest_and_smallest = True\n",
    "    if growing_trend > math.ceil(0.6*len(diffs)) or shrinking_trend > math.ceil(0.6*len(diffs)):\n",
    "        check_largest_and_smallest = False\n",
    "\n",
    "    if check_largest_and_smallest:\n",
    "        # Find largest and smallest diffs, if largest is 20x larger than 2nd largest,\n",
    "        # or smallest is 20x smaller than 2nd smallest, consider them outliers.\n",
    "        top_two_ix = diffs.argsort()[-2:][::-1]\n",
    "        large1 = diffs[top_two_ix[0]]\n",
    "        large2 = diffs[top_two_ix[1]]\n",
    "        bot_two_ix = diffs.argsort()[:2]\n",
    "        small1 = diffs[bot_two_ix[0]]\n",
    "        small2 = diffs[bot_two_ix[1]]\n",
    "\n",
    "        largest_is_outlier = False\n",
    "        smallest_is_outlier = False\n",
    "        if large1 > 0 and large2 > 0 and large1 > 10.0 and large2 > 2.0 and (large2*7) < large1:\n",
    "            largest_is_outlier = True\n",
    "        if large1 < 0 and large2 < 0 and large1 < -10.0 and large2 < -2.0 and (large1*7) > large2:\n",
    "            largest_is_outlier = True\n",
    "        if small1 > 0 and small2 > 0 and small1 > 10.0 and small2 > 2.0 and (small2*7) < small1:\n",
    "            smallest_is_outlier = True\n",
    "        if small1 < 0 and small2 < 0 and small1 < -10.0 and small2 < -2.0 and (small1*7) > small2:\n",
    "            smallest_is_outlier = True\n",
    "\n",
    "        if largest_is_outlier or smallest_is_outlier:\n",
    "            hit_ix_list = hit_ix.tolist()\n",
    "            for idx, diff in enumerate(diffs):\n",
    "                if (largest_is_outlier and diff == large1) or (smallest_is_outlier and diff == small1):\n",
    "                    #print('Removing extreme outlier diff: ' + str(diff) + ', ix: ' + str(hit_ix2[idx + 1]) + ', from diffs: ' + str(diffs))\n",
    "                    outlier_ix.append(hit_ix2[idx + 1])\n",
    "                    hit_ix_list.remove(hit_ix2[idx + 1])\n",
    "\n",
    "            # Re-generate the diffs now that we've removed the extreme outliers\n",
    "            hit_ix = np.asarray(hit_ix_list)\n",
    "            if len(hit_ix) < 3:\n",
    "                return outlier_ix\n",
    "            df2 = df.loc[hit_ix]        \n",
    "            df2 = df2.sort_values('z')\n",
    "            hit_ix2 = df2.index.values\n",
    "            diffs = np.diff(df2[dimension].values)\n",
    "                \n",
    "    # Restrict to when the majority (75%+) of diffs are all in same direction\n",
    "    neg_diffs = np.where(diffs < 0)[0]\n",
    "    pos_diffs = np.where(diffs >= 0)[0]\n",
    "\n",
    "    #print(df2[dimension].values)\n",
    "    #print(hit_ix)\n",
    "    #print('trk: ' + str(track) + ', diffs: ' + str(diffs))\n",
    "    #print(neg_diffs)\n",
    "    #print(pos_diffs)\n",
    "    #print(df2)\n",
    "\n",
    "    # Restrict to when the majority of diffs are either positive or negative.\n",
    "    # (more difficult to detect outliers if diffs oscillate -ve and +ve)\n",
    "    dim_vals = df2[dimension].values\n",
    "    if len(neg_diffs) >= math.ceil(0.75*len(diffs)):\n",
    "        # remove large positive ones.\n",
    "        growing_trend = 0\n",
    "        previous_diff = 0\n",
    "        for idx, diff in enumerate(diffs):\n",
    "            # Some tracks trend from negative to positive diffs, don't eliminate\n",
    "            # positive values if it looks like we are trending that way.\n",
    "            if idx > 0 and diff > diffs[idx-1]:\n",
    "                growing_trend = growing_trend + 1\n",
    "                if growing_trend > 2:\n",
    "                    break\n",
    "            else:\n",
    "                growing_trend = 0\n",
    "            # Use absolute value > 1.0 in case there is small delta each time,\n",
    "            # we only try to find big jumps in the wrong direction.\n",
    "            #print('nidx-' + dimension + ': ' + str(idx) + ', diff: ' + str(diff) + ', df ix: ' + str(hit_ix2[idx+1]))\n",
    "            if diff > 1.0:\n",
    "                # We sometimes see cases like:\n",
    "                # diff[n-1] = -22\n",
    "                # diff[n] = 12\n",
    "                # diff[n+1] = -14\n",
    "                # In this case, we want to remove n-1 as the outlier, since if that\n",
    "                # was gone, diff[n] would be -10, which is more reasonable.\n",
    "                # In cases where we see:\n",
    "                # diff[0] = 23\n",
    "                # diff[1] = -5\n",
    "                # We want to check the dimension values directly instead of the diffs, it\n",
    "                # could be that val[0] is the outlier.\n",
    "                if idx == 0 and dim_vals[1] > dim_vals[2] and dim_vals[0] < dim_vals[2]:\n",
    "                    # The real outlier is the first entry in the list.\n",
    "                    outlier_ix.append(hit_ix2[0])\n",
    "                elif idx == 0 or idx == (len(diffs)-1) or ((diff + diffs[idx-1]) > 0) or diffs[idx+1] > 0:\n",
    "                    #print('Removing: ' + str(hit_ix2[idx+1]))\n",
    "                    outlier_ix.append(hit_ix2[idx + 1])\n",
    "                else:\n",
    "                    # The real outlier is the previous one (i.e. diff[n-1] in the example above!\n",
    "                    outlier_ix.append(hit_ix2[idx])\n",
    "    \n",
    "    elif len(pos_diffs) >= math.ceil(0.75*len(diffs)):\n",
    "        # remove large negative ones\n",
    "        shrinking_trend = 0\n",
    "        for idx, diff in enumerate(diffs):\n",
    "            # Some tracks trend from positive to negative diffs, don't eliminate\n",
    "            # negative values if it looks like we are trending that way.\n",
    "            if idx > 0 and diff < diffs[idx-1]:\n",
    "                shrinking_trend = shrinking_trend + 1\n",
    "                if shrinking_trend > 2:\n",
    "                    break\n",
    "            else:\n",
    "                shrinking_trend = 0\n",
    "            # Use absolute value > 1.0 in case there is small delta each time,\n",
    "            # we only try to find big jumps in the wrong direction.\n",
    "            #print('pidx-' + dimension + ': ' + str(idx) + ', diff: ' + str(diff) + ', df ix: ' + str(hit_ix2[idx+1]))\n",
    "            if diff < -1.0:\n",
    "                #print('Removing: ' + str(hit_ix2[idx+1]))\n",
    "                # Similar to the negative case above, make sure we remove the real\n",
    "                # outlier, in case the previous diff was misleading.\n",
    "                if idx == 0 and dim_vals[1] < dim_vals[2] and dim_vals[0] > dim_vals[2]:\n",
    "                    # The real outlier is the first entry in the list.\n",
    "                    outlier_ix.append(hit_ix2[0])\n",
    "                elif idx == 0 or idx == (len(diffs)-1) or ((diff + diffs[idx-1]) < 0) or diffs[idx+1] < 0:\n",
    "                    #print('Removing: ' + str(hit_ix2[idx+1]))\n",
    "                    outlier_ix.append(hit_ix2[idx + 1])\n",
    "                else:\n",
    "                    # The real outlier is the previous one (i.e. diff[n-1] in the example above!\n",
    "                    outlier_ix.append(hit_ix2[idx])\n",
    "\n",
    "\n",
    "\n",
    "    # Future ideas for patterns:\n",
    "    # - average positive jump + average negative jump, for values that oscillate +ve and -ve\n",
    "    # - absolute value of jump in same direction, this is hard since some tracks seem jumpy\n",
    "    #   i.e. small diffs followed by a bigger jump, then smaller diffs. May need to tie that\n",
    "    #   in with volume/layer/module ids, i.e. only allow bigger jumps between layers.\n",
    "    return outlier_ix                \n",
    "\n",
    "def find_duplicate_z(track, labels, df):\n",
    "    def number_is_between(a1, a2, a3):\n",
    "        return (a1 >= a2 and a2 >= a3) or (a1 <= a2 and a2 <= a3)\n",
    "\n",
    "    def numbers_are_between(a1, a2, a3, b1, b2, b3):\n",
    "        return number_is_between(a1, a2, a3) and number_is_between(b1, b2, b3)\n",
    "\n",
    "    duplicatez_ix = []\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "\n",
    "    # Need at least 4 values to be able to evaluate duplicate z-values.\n",
    "    if len(hit_ix) < 4:\n",
    "        return duplicatez_ix\n",
    "\n",
    "    df2 = df.loc[hit_ix]        \n",
    "    df2 = df2.sort_values('z_abs')\n",
    "    hit_ix2 = df2.index.values # remember new indexes after sorting\n",
    "    xs = df2.x.values\n",
    "    ys = df2.y.values\n",
    "    zs = df2.z.values\n",
    "    max_idx = len(zs) - 1\n",
    "\n",
    "    z_counts = coll.Counter(df2.z.values).most_common(len(df2.z.values))\n",
    "\n",
    "    if zs[0] == zs[1]:\n",
    "        # zs at the beginning\n",
    "        x1 = xs[2]\n",
    "        x2 = xs[3]\n",
    "        y1 = ys[2]\n",
    "        y2 = ys[3]\n",
    "        if numbers_are_between(xs[0], x1, x2, ys[0], y1, y2) and not numbers_are_between(xs[1], x1, x2, ys[1], y1, y2):\n",
    "            # The first one is more consistent, delete the 2nd duplicate value\n",
    "            duplicatez_ix.append(hit_ix2[1])\n",
    "            #print('xs[1] ' + str(xs[1]) + ' <= x1 ' + str(x1) + ' <= x2 ' + str(x2))\n",
    "            #print('ys[1] ' + str(ys[1]) + ' <= y1 ' + str(y1) + ' <= y2 ' + str(y2))\n",
    "        elif numbers_are_between(xs[1], x1, x2, ys[1], y1, y2) and not numbers_are_between(xs[0], x1, x2, ys[0], y1, y2):\n",
    "            # The second one is more consistent, delete the 1st duplicate value\n",
    "            duplicatez_ix.append(hit_ix2[0])\n",
    "            #print('b')\n",
    "        elif numbers_are_between(xs[0], x1, x2, ys[0], y1, y2) and numbers_are_between(xs[1], x1, x2, ys[1], y1, y2):\n",
    "            # Both z-values seem reasonable, need a tie-breaker to find out which is the right one.\n",
    "            add_code_here = True\n",
    "        # else, neither seem valid, unsure how to proceed, better off not rejecting any.\n",
    "\n",
    "    if zs[-1] == zs[-2]:\n",
    "        # zs at the end\n",
    "        x1 = xs[-4]\n",
    "        x2 = xs[-3]\n",
    "        y1 = ys[-4]\n",
    "        y2 = ys[-3]\n",
    "        if numbers_are_between(x1, x2, xs[-2], y1, y2, ys[-2]) and not numbers_are_between(x1, x2, xs[-1], y1, y2, ys[-1]):\n",
    "            # The first one is more consistent, delete the last duplicate value\n",
    "            duplicatez_ix.append(hit_ix2[-1])\n",
    "        elif numbers_are_between(x1, x2, xs[-1], y1, y2, ys[-1]) and not numbers_are_between(x1, x2, xs[-2], y1, y2, ys[-2]):\n",
    "            # The last one is more consistent, delete the 1st duplicate value\n",
    "            duplicatez_ix.append(hit_ix2[-2])\n",
    "        elif numbers_are_between(x1, x2, xs[-1], y1, y2, ys[-1]) and numbers_are_between(x1, x2, xs[-2], y1, y2, ys[-2]):\n",
    "            # Both z-values seem reasonable, need a tie-breaker to find out which is the right one.\n",
    "            add_code_here = True\n",
    "        # else, neither seem valid, unsure how to proceed, better off not rejecting any.\n",
    "        \n",
    "    # Idea: Find duplicate adjacent z-values. Remember x and y before and after the\n",
    "    # duplicates. Choose z that lies between the two. If z at beginning or end,\n",
    "    # need the two post (or pre-) x/y values to see the expected sign of the diff.\n",
    "\n",
    "    # Note max_idx is largest valid index, we already handled the case where the\n",
    "    # duplicate zs are at the beginning or end of the list.\n",
    "    for idx in range(0, max_idx):\n",
    "        if idx > 0 and (idx+2) <= max_idx and zs[idx] == zs[idx+1]:\n",
    "            x1 = xs[idx-1]\n",
    "            x2 = xs[idx+2]\n",
    "            y1 = ys[idx-1]\n",
    "            y2 = ys[idx+2]\n",
    "            # now, x1 <= z1 <= x2, and y1 <= z1 <= y2\n",
    "            if numbers_are_between(x1, xs[idx], x2, y1, ys[idx], y2) and not numbers_are_between(x1, xs[idx+1], x2, y1, ys[idx+1], y2):\n",
    "                # The first one is more consistent, delete the 2nd duplicate value\n",
    "                duplicatez_ix.append(hit_ix2[idx+1])\n",
    "            elif numbers_are_between(x1, xs[idx+1], x2, y1, ys[idx+1], y2) and not numbers_are_between(x1, xs[idx], x2, y1, ys[idx], y2):\n",
    "                # The second one is more consistent, delete the 1st duplicate value\n",
    "                duplicatez_ix.append(hit_ix2[idx])\n",
    "            elif numbers_are_between(x1, xs[idx], x2, y1, ys[idx], y2) and numbers_are_between(x1, xs[idx+1], x2, y1, ys[idx+1], y2):\n",
    "                # Both z-values seem reasonable, need a tie-breaker to find out which is the right one.\n",
    "                add_code_here = True\n",
    "            # else, neither seem valid, unsure how to proceed, better off not rejecting any.\n",
    "\n",
    "    #if z_counts[0][1] > 1:\n",
    "    #    print('Duplicatez found on track ' + str(track) + ', removed: ' + str(duplicatez_ix))\n",
    "\n",
    "    return duplicatez_ix\n",
    "\n",
    "# TODO pi, -pi discontinuity \n",
    "def remove_track_outliers_slope(track, labels, hits, debug=False):\n",
    "    final_outliers = []\n",
    "\n",
    "    hhh_ix = np.where(labels == track)\n",
    "    hhh_h = hits.loc[hhh_ix].sort_values('z')\n",
    "    \n",
    "    slopes_backward = []\n",
    "    slopes_forward = []\n",
    "\n",
    "    num_hits = len(hhh_h)\n",
    "    # Only reliable with tracks >= 5 hits\n",
    "    if num_hits < 5:\n",
    "        return final_outliers\n",
    "\n",
    "    if debug: print('backward:')\n",
    "    for i in np.arange(num_hits-1,0,-1):\n",
    "        a0 =  hhh_h.a0.values[i]\n",
    "        a1 =  hhh_h.a0.values[i-1]\n",
    "        r0 =  hhh_h.r.values[i]\n",
    "        r1 =  hhh_h.r.values[i-1]\n",
    "        if r0 == r1:\n",
    "            r0 = r0 + 1e-8\n",
    "        slope = (a0-a1)/(r0-r1) \n",
    "        slopes_backward.append(slope)\n",
    "        if debug: print(hhh_h.hit_id.values[i], slope, a0)\n",
    "        if i == 1:\n",
    "            a0 = hhh_h.a0.values[0]\n",
    "            a1 = hhh_h.a0.values[num_hits-1]\n",
    "            r0 =  hhh_h.r.values[0]\n",
    "            r1 =  hhh_h.r.values[num_hits-1]\n",
    "            if r0 == r1:\n",
    "                r0 = r0 + 1e-8\n",
    "            slope = (a0-a1)/(r0-r1)\n",
    "            slopes_backward.append(slope)\n",
    "            if debug: print(hhh_h.hit_id.values[0], slope, a1)\n",
    "\n",
    "    if debug: print('forward:')\n",
    "    for i in np.arange(0,num_hits-1,1):\n",
    "        a0 =  hhh_h.a0.values[i]\n",
    "        a1 =  hhh_h.a0.values[i+1]\n",
    "        r0 =  hhh_h.r.values[i]\n",
    "        r1 =  hhh_h.r.values[i+1]\n",
    "        if r0 == r1:\n",
    "            r1 = r1 + 1e-8\n",
    "        slope = (a1-a0)/(r1-r0) \n",
    "        slopes_forward.append(slope)\n",
    "        if debug: print(hhh_h.hit_id.values[i], slope, a0)\n",
    "\n",
    "        if i == num_hits-2:\n",
    "            a0 = hhh_h.a0.values[0]\n",
    "            a1 = hhh_h.a0.values[num_hits-1]\n",
    "            r0 =  hhh_h.r.values[0]\n",
    "            r1 =  hhh_h.r.values[num_hits-1]\n",
    "            if r0 == r1:\n",
    "                r1 = r1 + 1e-8\n",
    "            slope = (a1-a0)/(r1-r0) \n",
    "            slopes_forward.append(slope)\n",
    "            if debug: print(hhh_h.hit_id.values[num_hits-1], slope, a0)\n",
    "\n",
    "    slopes_backward = np.asarray(slopes_backward)\n",
    "    slopes_backward = np.reshape(slopes_backward, (-1, 1))\n",
    "    slopes_forward = np.asarray(slopes_forward)\n",
    "    slopes_forward = np.reshape(slopes_forward, (-1, 1))\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    X_back = ss.fit_transform(slopes_backward)\n",
    "    X_for = ss.fit_transform(slopes_forward)\n",
    "\n",
    "    cl = DBSCAN(eps=0.0033, min_samples=1)\n",
    "    outlier_labels_backward = cl.fit_predict(X_back)\n",
    "    outlier_labels_forward = cl.fit_predict(X_for)\n",
    "\n",
    "    if debug: print(outlier_labels_backward)\n",
    "    if debug: print(outlier_labels_forward)\n",
    "\n",
    "    track_counts = coll.Counter(outlier_labels_backward).most_common(1)\n",
    "    most_common_id = track_counts[0][0]\n",
    "    most_common_count = track_counts[0][1]\n",
    "\n",
    "    outlier_indices_backward = []\n",
    "    if most_common_count > 1 and len(np.unique(outlier_labels_forward)) < num_hits/2:\n",
    "        for i in np.arange(num_hits-1,-1,-1):\n",
    "            if outlier_labels_backward[i] != most_common_id:\n",
    "                if debug: print(hhh_h.index.values[num_hits-1-i])\n",
    "                outlier_indices_backward.append(hhh_h.index.values[num_hits-1-i])\n",
    "\n",
    "    track_counts = coll.Counter(outlier_labels_forward).most_common(1)\n",
    "    most_common_id = track_counts[0][0]\n",
    "    most_common_count = track_counts[0][1]\n",
    "\n",
    "\n",
    "    outlier_indices_forward = []\n",
    "    if most_common_count > 1 and len(np.unique(outlier_labels_forward)) < num_hits/2:\n",
    "        for i in np.arange(0,num_hits-1,1):\n",
    "            if outlier_labels_forward[i] != most_common_id:\n",
    "                if debug: print(hhh_h.index.values[i])\n",
    "                outlier_indices_forward.append(hhh_h.index.values[i])\n",
    "\n",
    "\n",
    "    outlier_candidates = list(set(outlier_indices_backward).intersection(outlier_indices_forward))\n",
    "\n",
    "\n",
    "    if debug: print('before removal:' + str(outlier_candidates))\n",
    "\n",
    "    for i in range(len(outlier_candidates)):\n",
    "        candidate = hhh_h.loc[outlier_candidates[i]]\n",
    "        found = False\n",
    "        for index, row in hhh_h.iterrows():\n",
    "            if np.absolute(candidate.z-row.z) == 0.5 and candidate.volume_id == row.volume_id \\\n",
    "            and candidate.layer_id == row.layer_id and candidate.module_id != row.module_id:\n",
    "                # true hits\n",
    "                if debug: print('true hit' + str(outlier_candidates[i]))\n",
    "                found = True\n",
    "        if found is False:\n",
    "            final_outliers.append(outlier_candidates[i])\n",
    "\n",
    "    if debug: print('new loutliers:' + str(final_outliers))\n",
    "\n",
    "    # If we determine that half (or more) of the hits need to be removed, we may have messed\n",
    "    # up, so do not return any outliers.\n",
    "    max_removal_threshold = math.floor(num_hits/2)\n",
    "    if len(final_outliers) >= max_removal_threshold:\n",
    "        final_outliers = []\n",
    "\n",
    "    return final_outliers\n",
    "\n",
    "    \n",
    "def remove_track_outliers(track, labels, hits, aggressive):\n",
    "    labels = np.copy(labels)\n",
    "    found_bad_volume = 0\n",
    "    found_bad_dimension = 0\n",
    "    found_bad_slope = 0\n",
    "    found_bad_z = 0\n",
    "\n",
    "    # Check if the sorted hits (on z-axis) go through the volumes\n",
    "    # and layers in the expected order\n",
    "    bad_volume_ix = find_invalid_volumes(track, labels, hits)\n",
    "    if len(bad_volume_ix) > 0:\n",
    "        #print('track ' + str(track) + ' bad volume: ' + str(bad_volume_ix))\n",
    "        found_bad_volume = found_bad_volume + len(bad_volume_ix)\n",
    "        for bvix in bad_volume_ix:\n",
    "            labels[bvix] = 0\n",
    "\n",
    "    if True:\n",
    "        # Check if the sorted hits (on z-axis) go through the volumes\n",
    "        # and layers in the expected order\n",
    "        duplicatez_ix = find_duplicate_z(track, labels, hits)\n",
    "        if len(duplicatez_ix) > 0:\n",
    "            #print('track ' + str(track) + ' duplicate z: ' + str(duplicatez_ix))\n",
    "            found_bad_z = found_bad_z + len(duplicatez_ix)\n",
    "            for bzix in duplicatez_ix:\n",
    "                labels[bzix] = 0\n",
    "\n",
    "    if True:\n",
    "        # Check the helix slope, discard hits that do not match\n",
    "        outlier_slope_ix = remove_track_outliers_slope(track, labels, hits)\n",
    "        if len(outlier_slope_ix) > 0:\n",
    "            #print('track ' + str(track) + ' slope outliers: ' + str(outlier_slope_ix))\n",
    "            found_bad_slope = found_bad_slope + len(outlier_slope_ix)\n",
    "            for oix in outlier_slope_ix:\n",
    "                labels[oix] = 0\n",
    "\n",
    "    if aggressive:\n",
    "        # Next analysis, from remaining hits, sort by 'z' (roughly time-based),\n",
    "        # check for anomolies in other dimensions.\n",
    "        outlier_ix = find_dimension_outlier(track, labels, hits, 'y')\n",
    "        if len(outlier_ix) > 0:\n",
    "            #print('track ' + str(track) + ' outlier dimension y: ' + str(outlier_ix))\n",
    "            found_bad_dimension = found_bad_dimension + len(outlier_ix)\n",
    "            for oix in outlier_ix:\n",
    "                labels[oix] = 0\n",
    "\n",
    "        # Next analysis, from remaining hits, sort by 'z' (roughly time-based),\n",
    "        # check for anomolies in z dimensions (i.e. outliers at beginning/end)\n",
    "        outlier_ix = find_dimension_outlier(track, labels, hits, 'z')\n",
    "        if len(outlier_ix) > 0:\n",
    "            #print('track ' + str(track) + ' outlier dimension z: ' + str(outlier_ix))\n",
    "            found_bad_dimension = found_bad_dimension + len(outlier_ix)\n",
    "            for oix in outlier_ix:\n",
    "                labels[oix] = 0\n",
    "            \n",
    "    return (labels, found_bad_volume, found_bad_dimension, found_bad_z, found_bad_slope)\n",
    "\n",
    "def remove_outliers(labels, hits, aggressive=False, print_counts=True):\n",
    "    tracks = np.unique(labels)\n",
    "    hits['z_abs'] = hits.z.abs()\n",
    "    hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "    hits['a0'] = np.arctan2(hits.y,hits.x)\n",
    "    count_rem_volume = 0\n",
    "    count_rem_dimension = 0\n",
    "    count_duplicatez = 0\n",
    "    count_rem_slope = 0\n",
    "    count_singletons = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 3:\n",
    "            (labels, c1, c2, c3, c4) = remove_track_outliers(track, labels, hits, aggressive)\n",
    "            count_rem_volume = count_rem_volume + c1\n",
    "            count_rem_dimension = count_rem_dimension + c2\n",
    "            count_duplicatez = count_duplicatez + c3\n",
    "            count_rem_slope = count_rem_slope + c4\n",
    "\n",
    "    # Remove singletons, we do not get any score for those. This is done\n",
    "    # last, in case removing the outliers (above) removed enough hits\n",
    "    # from a track to create a new singleton.\n",
    "    tracks = np.unique(labels)\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) == 1:\n",
    "            count_singletons = count_singletons + 1\n",
    "            labels[track_hits[0]] = 0\n",
    "\n",
    "    if print_counts:\n",
    "        print('Total removed due to bad volumes: ' + str(count_rem_volume))\n",
    "        print('Total removed due to bad dimensions: ' + str(count_rem_dimension))\n",
    "        print('Total removed due to duplicate zs: ' + str(count_duplicatez))\n",
    "        print('Total removed due to bad slopes: ' + str(count_rem_slope))\n",
    "        print('Total removed singleton hits: ' + str(count_singletons))\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_h3 = np.copy(labels_helix)\n",
    "\n",
    "#one_submission = create_one_event_submission(1000, hits, labels_h3)\n",
    "#score = score_event(truth, one_submission)\n",
    "#print(\"Initial score before outlier removal for event %d: %.8f\" % (1000, score))\n",
    "\n",
    "labels_h3 = remove_outliers(labels_h3, hits)\n",
    "one_submission = create_one_event_submission(1000, hits, labels_h3)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Initial score after outlier removal for event %d: %.8f\" % (1000, score))\n",
    "\n",
    "# before score: 0.57485796\n",
    "# after old outlier removal: 0.57471551\n",
    "# after slope removal: 0.56906582 (1006 removed due to bad slopes)\n",
    "# after slope removal + threshold: 0.56966146 (962 removed due to bad slopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with the track number to find a track that contains multiple particles\n",
    "hhh_ix = np.where(labels_helix == 5944)\n",
    "print(hhh_ix)\n",
    "hhh_t = truth.loc[hhh_ix]\n",
    "hhh_t = hhh_t.sort_values('tz')\n",
    "print(hhh_t)\n",
    "\n",
    "# Now add r/a0 as columns to the hits, and see if there's a way to detect\n",
    "# which hits belong to the track, and which should be considered outliers.\n",
    "hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "hits['a0'] = np.arctan2(hits.y,hits.x)\n",
    "hhh_h = hits.loc[hhh_ix]\n",
    "\n",
    "\n",
    "#hhh_h = hhh_h[(hhh_h.hit_id != 15459) & (hhh_h.hit_id != 16190) & (hhh_h.hit_id != 16155) ]\n",
    "\n",
    "hhh_h = hhh_h.sort_values('z')\n",
    "print(hhh_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dupz -> only remove when value ends in .0000 or .5000?\n",
    "# 59828: a: 0,025628, r: 13,745506, slope=a/r=0,001864463920062, 59825: a: 0,025181, r: 13,590172, slope=0,001852883098168\n",
    "# 67285: r: -35,053009, a: -0,072102, slope=0,002056941816322, 67284: r: -46,909088, a: -0,088433, slope=0,001885199729315\n",
    "# 59036: r: 25,316536, a: 0,041827, slope=0,001652161259344, 59007: r: 11,899605, a: 0,019226, 0,001615683881944\n",
    "# Ones that we had trouble with for slope outlier detection:\n",
    "#x 11/6 hits - track 1373 outliers: [24207, 39572, 39574, 48338, 78104, 78108] should remove [24207, 39572, 39574]\n",
    "#y 17/7 hits - track 1644 outliers: [21896, 29838, 36816, 89394, 89395, 111614, 95550] should remove: [89393, 89395]\n",
    "# 19/7 hits - track 1698 outliers: [80388, 73708, 42159, 73717, 86837, 86838, 41657] should remove: [86838, 86387, 80388, 73708, 42159]\n",
    "# 14/5 hits - track 1902 outliers: [23136, 37256, 116299, 30509, 116560] should remove: [116561]\n",
    "#x 15/9 hits - track 1963 outliers: [30432, 37195, 43502, 23025, 75475, 111219, 111222, 37180, 30367] should remove: [111222]\n",
    "#y 13/5 hits - track 2390 outliers: [76962, 84260, 84264, 45130, 45486] should remove: [45128, 45130, 45486, 76962, 84260, 84264, 91275]\n",
    "# --> actually, really good! this was a very complicated case.\n",
    "#x 20/11 hits - track 2445 outliers: [70818, 65315, 69259, 67852, 67858, 69236, 105080, 70776, 66589, 69246, 69247] should remove: [67852, 69246, 69236, 69259, 70776, 71464]\n",
    "# 16/6 hits - track 2522 outliers: [44685, 44686, 83245, 76114, 21534, 76126] should remove:  [21534, 21672, 44209, 44686, 76114, 83245, 90582]\n",
    "# 15/5 hits - track 3417 outliers: [28458, 81457, 81464, 80985, 35775] should remove: [81457, 80985]\n",
    "#x 15/8 hits - track 3427 outliers: [101154, 39335, 98506, 120427, 24496, 119921, 31986, 84925] should remove: []\n",
    "#y 15/6 hits - track 4225 outliers: [119013, 118535, 31339, 45003, 76853, 38453] should remove: [118535]\n",
    "#y 13/5 hits - track 4318 outliers: [84482, 45732, 77481, 77200, 77203] should remove: [77200, 77481, 84482, 84707, 119929]\n",
    "# 18/5 hits - track 5047 outliers: [44323, 44330, 44331, 30926, 37591] should remove: [22142, 30926, 37591, 44319, 44331, 44323, 44330]\n",
    "# 13/5 hits - track 5101 outliers: [117442, 22595, 22833, 111923, 95867] should remove   [117444]\n",
    "#xx 13/6 hits - track 5420 outliers: [86917, 92518, 86918, 92523, 108468, 108469] should remove [86918]\n",
    "#y 23/10 hits - track 5422 outliers: [94144, 81504, 88577, 94531, 74492, 88165, 43052, 110419, 21500, 110718] should remove [21650, 21512, 43052, 43060, 74492, 81504, 88165, 110718, 88577, 94531]\n",
    "# 17/5 hits - track 5826 outliers: [43176, 89005, 94905, 88988, 88991] should remove [43170, 43176, 89005, 88991, 88988, 94905, 94878, 111049]\n",
    "# 14/5 hits - track 6244 outliers: [34257, 34706, 34708, 19508, 34264] should remove [79229, 72675, 34708, 34264, 34706, 34697, 19718, 19508]\n",
    "#y 12/5 hits - track 6409 outliers: [35969, 73987, 28709, 73990, 42508] should remove [109342, 109340, 93542, 73987]\n",
    "# 15/6 hits - track 6740 outliers: [38990, 32046, 39374, 23633, 23666, 38997] should remove [23633, 23666, 32046, 38997, 39374]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_outlier_removal(labels, hits, truth):\n",
    "    tracks = np.unique(labels)\n",
    "    hits['z_abs'] = hits.z.abs()\n",
    "    hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "    hits['a0'] = np.arctan2(hits.y,hits.x)\n",
    "    count_removed = 0\n",
    "    count_not_removed = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 3:\n",
    "            outlier_slope_ix = remove_track_outliers_slope(track, labels, hits)\n",
    "            if len(outlier_slope_ix) > 0:\n",
    "                tdf = truth.loc[track_hits]\n",
    "                truth_count = coll.Counter(tdf.particle_id.values).most_common(1)\n",
    "                truth_particle_id = truth_count[0][0]\n",
    "                for out_ix in outlier_slope_ix:\n",
    "                    if tdf.loc[out_ix].particle_id != truth_particle_id:\n",
    "                        labels[out_ix] = 0\n",
    "                        count_removed = count_removed + 1\n",
    "                    else:\n",
    "                        count_not_removed = count_not_removed + 1\n",
    "\n",
    "    print('safe count_removed: ' + str(count_removed))\n",
    "    print('safe count_not_removed: ' + str(count_not_removed))\n",
    "    return labels\n",
    "\n",
    "def perfect_outlier_removal(labels, hits, truth):\n",
    "    tracks = np.unique(labels)\n",
    "    count_removed = 0\n",
    "    count_not_removed = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 3:\n",
    "            tdf = truth.loc[track_hits]\n",
    "            truth_count = coll.Counter(tdf.particle_id.values).most_common(1)\n",
    "            truth_particle_id = truth_count[0][0]\n",
    "            for hit_ix in track_hits:\n",
    "                if tdf.loc[hit_ix].particle_id != truth_particle_id:\n",
    "                    labels[hit_ix] = 0\n",
    "                    count_removed = count_removed + 1\n",
    "                else:\n",
    "                    count_not_removed = count_not_removed + 1\n",
    "\n",
    "    print('perfect count_removed: ' + str(count_removed))\n",
    "    print('perfect count_not_removed: ' + str(count_not_removed))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_h2 = np.copy(labels_helix)\n",
    "labels_h2 = remove_outliers(labels_h2, hits)\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "labels_h3 = safe_outlier_removal(labels_h3, hits, truth)\n",
    "labels_h4 = np.copy(labels_helix)\n",
    "labels_h4 = perfect_outlier_removal(labels_h4, hits, truth)\n",
    "\n",
    "one_submission = create_one_event_submission(1000, hits, labels_helix)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Initial score for event %d: %.8f\" % (1000, score))\n",
    "\n",
    "one_submission = create_one_event_submission(1000, hits, labels_h2)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Score after normal outlier removal for event %d: %.8f\" % (1000, score))\n",
    "\n",
    "one_submission = create_one_event_submission(1000, hits, labels_h3)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Score after safe outlier removal score for event %d: %.8f\" % (1000, score))\n",
    "\n",
    "one_submission = create_one_event_submission(1000, hits, labels_h4)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Score after perfect outlier removal for event %d: %.8f\" % (1000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_hh2 = np.copy(labels_hh)\n",
    "labels_hh2 = remove_outliers(labels_hh2, hits)\n",
    "labels_hh3 = np.copy(labels_hh)\n",
    "labels_hh3 = safe_outlier_removal(labels_hh3, hits, truth)\n",
    "labels_hh4 = np.copy(labels_hh)\n",
    "labels_hh4 = perfect_outlier_removal(labels_hh4, hits, truth)\n",
    "\n",
    "one_submission = create_one_event_submission(1003, hits, labels_hh)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Initial score for event %d: %.8f\" % (1003, score))\n",
    "\n",
    "one_submission = create_one_event_submission(1003, hits, labels_hh2)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Score after normal outlier removal for event %d: %.8f\" % (1003, score))\n",
    "\n",
    "one_submission = create_one_event_submission(1003, hits, labels_hh3)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Score after safe outlier removal score for event %d: %.8f\" % (1003, score))\n",
    "\n",
    "one_submission = create_one_event_submission(1003, hits, labels_hh4)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Score after perfect outlier removal for event %d: %.8f\" % (1003, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_h1 = np.copy(labels_helix1)\n",
    "labels_h2 = np.copy(labels_helix2)\n",
    "labels_h3 = np.copy(labels_helix3)\n",
    "labels_merged = merge.heuristic_merge_tracks(labels_h1, labels_h2)\n",
    "labels_merged = merge.heuristic_merge_tracks(labels_merged, labels_h3)\n",
    "one_submission = create_one_event_submission(1003, hits, labels_merged)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merge score no removal for event %d: %.8f\" % (1003, score))\n",
    "\n",
    "labels_hh1 = np.copy(labels_helix1)\n",
    "labels_hh2 = np.copy(labels_helix2)\n",
    "labels_hh3 = np.copy(labels_helix3)\n",
    "labels_hh1 = remove_outliers(labels_hh1, hits)\n",
    "labels_hh2 = remove_outliers(labels_hh2, hits)\n",
    "labels_hh3 = remove_outliers(labels_hh3, hits)\n",
    "labels_merged2 = merge.heuristic_merge_tracks(labels_hh1, labels_hh2)\n",
    "labels_merged = merge.heuristic_merge_tracks(labels_merged2, labels_hh3)\n",
    "one_submission = create_one_event_submission(1003, hits, labels_merged2)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merge score outlier removal for event %d: %.8f\" % (1003, score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_merge1 = merge.heuristic_merge_tracks(labels_helix, labels_hh)\n",
    "one_submission = create_one_event_submission(1003, hits, labels_merge1)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merge score no removal for event %d: %.8f\" % (1003, score))\n",
    "\n",
    "labels_merge2 = merge.heuristic_merge_tracks(labels_h2, labels_hh2)\n",
    "one_submission = create_one_event_submission(1003, hits, labels_merge2)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merge score normal outlier removal for event %d: %.8f\" % (1003, score))\n",
    "\n",
    "labels_merge3 = merge.heuristic_merge_tracks(labels_h3, labels_hh3)\n",
    "one_submission = create_one_event_submission(1003, hits, labels_merge3)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merge score safe removal for event %d: %.8f\" % (1003, score))\n",
    "\n",
    "labels_merge4 = merge.heuristic_merge_tracks(labels_h4, labels_hh4)\n",
    "one_submission = create_one_event_submission(1003, hits, labels_merge4)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merge score perfect removal for event %d: %.8f\" % (1003, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_q1 = np.copy(labels_helix)\n",
    "labels_q1 = remove_outliers(labels_q1, hits)\n",
    "\n",
    "labels_q2 = np.copy(labels_hh)\n",
    "labels_q2 = remove_outliers(labels_q2, hits)\n",
    "\n",
    "labels_mergeq1 = merge.heuristic_merge_tracks(labels_q1, labels_q2)\n",
    "one_submission = create_one_event_submission(1003, hits, labels_mergeq1)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merge score normal no-slope removal for event %d: %.8f\" % (1003, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO pi, -pi discontinuity \n",
    "def remove_track_outliers_slope(track, labels, hits, debug=False):\n",
    "    hhh_ix = np.where(labels == track)\n",
    "    hhh_h = hits.loc[hhh_ix].sort_values('z')\n",
    "    \n",
    "    slopes_backward = []\n",
    "    slopes_forward = []\n",
    "\n",
    "\n",
    "    num_hits = len(hhh_h)\n",
    "    if debug: print('backward:')\n",
    "    for i in np.arange(num_hits-1,0,-1):\n",
    "        a0 =  hhh_h.a0.values[i]\n",
    "        a1 =  hhh_h.a0.values[i-1]\n",
    "        r0 =  hhh_h.r.values[i]\n",
    "        r1 =  hhh_h.r.values[i-1]\n",
    "        if r0 == r1:\n",
    "            r0 = r0 + 1e-8\n",
    "        slope = (a0-a1)/(r0-r1) \n",
    "        slopes_backward.append(slope)\n",
    "        if debug: print(hhh_h.hit_id.values[i], slope, a0)\n",
    "        if i == 1:\n",
    "            a0 = hhh_h.a0.values[0]\n",
    "            a1 = hhh_h.a0.values[num_hits-1]\n",
    "            r0 =  hhh_h.r.values[0]\n",
    "            r1 =  hhh_h.r.values[num_hits-1]\n",
    "            if r0 == r1:\n",
    "                r0 = r0 + 1e-8\n",
    "            slope = (a0-a1)/(r0-r1)\n",
    "            slopes_backward.append(slope)\n",
    "            if debug: print(hhh_h.hit_id.values[0], slope, a1)\n",
    "\n",
    "    if debug: print('forward:')\n",
    "    for i in np.arange(0,num_hits-1,1):\n",
    "        a0 =  hhh_h.a0.values[i]\n",
    "        a1 =  hhh_h.a0.values[i+1]\n",
    "        r0 =  hhh_h.r.values[i]\n",
    "        r1 =  hhh_h.r.values[i+1]\n",
    "        if r0 == r1:\n",
    "            r1 = r1 + 1e-8\n",
    "        slope = (a1-a0)/(r1-r0) \n",
    "        slopes_forward.append(slope)\n",
    "        if debug: print(hhh_h.hit_id.values[i], slope, a0)\n",
    "\n",
    "        if i == num_hits-2:\n",
    "            a0 = hhh_h.a0.values[0]\n",
    "            a1 = hhh_h.a0.values[num_hits-1]\n",
    "            r0 =  hhh_h.r.values[0]\n",
    "            r1 =  hhh_h.r.values[num_hits-1]\n",
    "            if r0 == r1:\n",
    "                r1 = r1 + 1e-8\n",
    "            slope = (a1-a0)/(r1-r0) \n",
    "            slopes_forward.append(slope)\n",
    "            if debug: print(hhh_h.hit_id.values[num_hits-1], slope, a0)\n",
    "\n",
    "    slopes_backward = np.asarray(slopes_backward)\n",
    "    slopes_backward = np.reshape(slopes_backward, (-1, 1))\n",
    "    slopes_forward = np.asarray(slopes_forward)\n",
    "    slopes_forward = np.reshape(slopes_forward, (-1, 1))\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    X_back = ss.fit_transform(slopes_backward)\n",
    "    X_for = ss.fit_transform(slopes_forward)\n",
    "\n",
    "    cl = DBSCAN(eps=0.0033, min_samples=1)\n",
    "    outlier_labels_backward = cl.fit_predict(X_back)\n",
    "    outlier_labels_forward = cl.fit_predict(X_for)\n",
    "\n",
    "    if debug: print(outlier_labels_backward)\n",
    "    if debug: print(outlier_labels_forward)\n",
    "\n",
    "    track_counts = coll.Counter(outlier_labels_backward).most_common(1)\n",
    "    most_common_id = track_counts[0][0]\n",
    "    most_common_count = track_counts[0][1]\n",
    "\n",
    "    outlier_indices_backward = []\n",
    "    if most_common_count > 1 and len(np.unique(outlier_labels_forward)) < num_hits/2:\n",
    "        for i in np.arange(num_hits-1,-1,-1):\n",
    "            if outlier_labels_backward[i] != most_common_id:\n",
    "                if debug: print(hhh_h.index.values[num_hits-1-i])\n",
    "                outlier_indices_backward.append(hhh_h.index.values[num_hits-1-i])\n",
    "\n",
    "    track_counts = coll.Counter(outlier_labels_forward).most_common(1)\n",
    "    most_common_id = track_counts[0][0]\n",
    "    most_common_count = track_counts[0][1]\n",
    "\n",
    "\n",
    "    outlier_indices_forward = []\n",
    "    if most_common_count > 1 and len(np.unique(outlier_labels_forward)) < num_hits/2:\n",
    "        for i in np.arange(0,num_hits-1,1):\n",
    "            if outlier_labels_forward[i] != most_common_id:\n",
    "                if debug: print(hhh_h.index.values[i])\n",
    "                outlier_indices_forward.append(hhh_h.index.values[i])\n",
    "\n",
    "\n",
    "    outlier_candidates = list(set(outlier_indices_backward).intersection(outlier_indices_forward))\n",
    "    final_outliers = []\n",
    "\n",
    "    if debug: print('before removal:' + str(outlier_candidates))\n",
    "\n",
    "    for i in range(len(outlier_candidates)):\n",
    "        candidate = hhh_h.loc[outlier_candidates[i]]\n",
    "        found = False\n",
    "        for index, row in hhh_h.iterrows():\n",
    "            if np.absolute(candidate.z-row.z) == 0.5 and candidate.volume_id == row.volume_id \\\n",
    "            and candidate.layer_id == row.layer_id and candidate.module_id != row.module_id:\n",
    "                # true hits\n",
    "                if debug: print('true hit' + str(outlier_candidates[i]))\n",
    "                found = True\n",
    "        if found is False:\n",
    "            final_outliers.append(outlier_candidates[i])\n",
    "\n",
    "    if debug: print('new loutliers:' + str(final_outliers))\n",
    "\n",
    "    return final_outliers\n",
    "\n",
    "\n",
    "def remove_outliers_slope(labels, hits):\n",
    "    tracks = np.unique(labels)\n",
    "    count_rem_volume = 0\n",
    "    count_rem_dimension = 0\n",
    "    count_duplicatez = 0\n",
    "    count_singletons = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 4:\n",
    "            outliers = remove_track_outliers_slope(track, labels, hits)\n",
    "            if len(outliers) > 0:\n",
    "                do_something = True\n",
    "                # filter out the outliers\n",
    "            \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_h3 = np.copy(labels_helix)\n",
    "outliers = remove_track_outliers_slope(14, labels_h3, hits)\n",
    "print('Slope outliers: ' + str(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Removing helix outliers')\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "labels_h3 = remove_outliers_slope(labels_h3, hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with the track number to find a track that contains multiple particles\n",
    "hhh_ix = np.where(labels_helix == 14)\n",
    "print(hhh_ix)\n",
    "hhh_t = truth.loc[hhh_ix]\n",
    "hhh_t = hhh_t.sort_values('tz')\n",
    "print(hhh_t)\n",
    "\n",
    "# Now add r/a0 as columns to the hits, and see if there's a way to detect\n",
    "# which hits belong to the track, and which should be considered outliers.\n",
    "hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "hits['a0'] = np.arctan2(hits.y,hits.x)\n",
    "hhh_h = hits.loc[hhh_ix]\n",
    "\n",
    "\n",
    "#hhh_h = hhh_h[(hhh_h.hit_id != 15459) & (hhh_h.hit_id != 16190) & (hhh_h.hit_id != 16155) ]\n",
    "\n",
    "hhh_h = hhh_h.sort_values('z')\n",
    "print(hhh_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Removing cone outliers')\n",
    "labels_c3 = np.copy(labels_cone)\n",
    "labels_c3 = remove_outliers_slope(labels_c3, hits)\n",
    "\n",
    "print('Removing helix outliers')\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "labels_h3 = remove_outliers_slope(labels_h3, hits)\n",
    "\n",
    "print('Merging tracks')\n",
    "tmp_labels = merge.heuristic_merge_tracks(labels_h3, labels_c3)\n",
    "one_submission = create_one_event_submission(1000, hits, tmp_labels)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merged score for event %d: %.8f\" % (1000, score))\n",
    "\n",
    "#Merged score for event 1000: 0.58701361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_c2 = np.copy(labels_cone)\n",
    "labels_c2 = remove_outliers(labels_c2, hits)\n",
    "\n",
    "labels_simple_merge = merge.merge_tracks(labels_helix, labels_c2)\n",
    "one_submission = create_one_event_submission(1000, hits, labels_simple_merge)\n",
    "#one_submission = create_one_event_submission(1000, hits, labels_c2)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Outlier removal score for cone event %d: %.8f\" % (1000, score))\n",
    "# Orig Score no removals: 0.26152679\n",
    "# With outlier removal:\n",
    "# volume: 1231\n",
    "# bad dim: 6733\n",
    "# score: 0.18735180\n",
    "# With volume removal: 3520, score: 0.23268937\n",
    "# With volume removal, treat -ve+8--> +ve: 2841, score: 0.23394451\n",
    "# With ignore of 8, otherwise full checks: 1056, score: 0.25499520\n",
    "# With ignore of 8, light checks: 1, score: 0.26152679\n",
    "# Bad volume removal: 1, Bad dims: 222, score: 0.25587569, merged: 0.48291748\n",
    "# Bad volume removal: 1, Bad dims: 220, duplicatez: 1675, score: 0.25657711, merged: 0.48742414\n",
    "# HELIX: Bad vol: 15, score: 0.51204521\n",
    "# HELIX: Bad vol: 15, duplicatez: 399, score: 0.51065321\n",
    "# HELIX: Bad vol: 15, duplicatez: 399, bad dim: 286, score: 0.49635631\n",
    "# HELIX: Bad vol: 15, duplicatez: 399, bad dim: 1518, score: 0.50364853\n",
    "# CONE: Bad vol: 0, duplicatez: 1675, bad dim: 1239, score: 0.25871803, merged: 0.48569633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_labels = merge.heuristic_merge_tracks(labels_helix, labels_c2)\n",
    "one_submission = create_one_event_submission(1000, hits, tmp_labels)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"cone score for event %d: %.8f\" % (1000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_h2 = np.copy(labels_helix)\n",
    "labels_h2 = remove_outliers(labels_h2, hits)#, aggressive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_to_remove = 157\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "xxx_ix = np.where(labels_h3 == track_to_remove)[0]\n",
    "print('xxx_ix: ' + str(xxx_ix))\n",
    "(labels_h3x, _, _, _) = remove_track_outliers(track_to_remove, labels_h3, hits, False)#, True)\n",
    "xxx_ixx = np.where(labels_h3x == track_to_remove)[0]\n",
    "print('xxx_ixx: ' + str(xxx_ixx))\n",
    "#(labels_h3, cx, cy, cz) = remove_track_outliers(track_to_remove, labels_c3, hits)\n",
    "#labels_c3x = np.copy(labels_c3)\n",
    "#one_submission = create_one_event_submission(1000, hits, labels_c3)\n",
    "#score = score_event(truth, one_submission)\n",
    "#print(\"Outlier removal score for cone event %d: %.8f\" % (1000, score))\n",
    "# rem 0,0, score 0.26152679\n",
    "#xxx_ix = np.where(labels_c3 == track_to_remove)[0]\n",
    "#print('xxx_ix: ' + str(xxx_ix))\n",
    "xxx_df = truth.loc[xxx_ix]\n",
    "xxx_df = xxx_df.sort_values('tz')\n",
    "xxx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_to_remove = 157\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "xxx_ix = np.where(labels_h3 == track_to_remove)[0]\n",
    "print('xxx_ix: ' + str(xxx_ix))\n",
    "(labels_h3x, _, _, _) = remove_track_outliers(track_to_remove, labels_h3, hits, False)#, True)\n",
    "xxx_ixx = np.where(labels_h3x == track_to_remove)[0]\n",
    "print('xxx_ixx: ' + str(xxx_ixx))\n",
    "#(labels_h3, cx, cy, cz) = remove_track_outliers(track_to_remove, labels_c3, hits)\n",
    "#labels_c3x = np.copy(labels_c3)\n",
    "#one_submission = create_one_event_submission(1000, hits, labels_c3)\n",
    "#score = score_event(truth, one_submission)\n",
    "#print(\"Outlier removal score for cone event %d: %.8f\" % (1000, score))\n",
    "# rem 0,0, score 0.26152679\n",
    "#xxx_ix = np.where(labels_c3 == track_to_remove)[0]\n",
    "#print('xxx_ix: ' + str(xxx_ix))\n",
    "xxx_df = hits.loc[xxx_ix]\n",
    "xxx_df = xxx_df.sort_values('z')\n",
    "xxx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr2_df = truth.loc[truth['particle_id'] == 306250134780379136]\n",
    "tr2_df = tr2_df.sort_values('tz')\n",
    "tr2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr2_df = truth.loc[truth['particle_id'] == 49542619558051840]\n",
    "tr2_df = tr2_df.sort_values('tz')\n",
    "tr2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helidx = np.where(labels_helix==7095)[0]\n",
    "print(helidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_submission = create_one_event_submission(1000, hits, labels_cone)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Outlier removal score for cone event %d: %.8f\" % (1000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_c2 = np.copy(labels_cone)\n",
    "#trks2 = np.unique(labels_c2)\n",
    "#count_rem_volume = 0\n",
    "#count_rem_dimension = 0\n",
    "#for trk2 in trks2:\n",
    "#    if trk2 == 0:\n",
    "#        continue\n",
    "#    trk2_hits = np.where(labels_c2 == trk2)[0]\n",
    "#    if len(trk2_hits) > 3:\n",
    "#        (labels_c2, c1, c2) = remove_track_outliers(trk2, labels_c2, hits)\n",
    "#        count_rem_volume = count_rem_volume + c1\n",
    "#        count_rem_dimension = count_rem_dimension + c2\n",
    "\n",
    "#print('Total removed due to bad volumes: ' + str(count_rem_volume))\n",
    "#print('Total removed due to bad dimensions: ' + str(count_rem_dimension))\n",
    "\n",
    "track_to_remove = 63542\n",
    "labels_c3 = np.copy(labels_cone)\n",
    "xxx_ix = np.where(labels_c3 == track_to_remove)[0]\n",
    "print('xxx_ix: ' + str(xxx_ix))\n",
    "(labels_c3, cx, cy, cz) = remove_track_outliers(track_to_remove, labels_c3, hits)\n",
    "labels_c3x = np.copy(labels_c3)\n",
    "one_submission = create_one_event_submission(1000, hits, labels_c3)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Outlier removal score for cone event %d: %.8f\" % (1000, score))\n",
    "# rem 0,0, score 0.26152679\n",
    "xxx_ix = np.where(labels_c3 == track_to_remove)[0]\n",
    "print('xxx_ix: ' + str(xxx_ix))\n",
    "xxx_df = hits.loc[xxx_ix]\n",
    "xxx_df = xxx_df.sort_values('z')\n",
    "# Bad indexes: 59855, 61697\n",
    "print(xxx_df)\n",
    "xxx_ix = np.where(labels_cone == 63542)\n",
    "xxx_df = hits.loc[xxx_ix]\n",
    "xxx_df = xxx_df.sort_values('z')\n",
    "# indexes in question: ???\n",
    "xxx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_helix = merge.renumber_labels(labels_helix)\n",
    "max_track = np.amax(labels_helix)\n",
    "labels_cone[labels_cone != 0] = labels_cone[labels_cone != 0] + max_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsx = np.copy(labels_cone)\n",
    "trackx = 63949\n",
    "outx = find_dimension_outlier(trackx, labelsx, hits, 'y')\n",
    "print('outx: ' + str(outx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx_ix = np.where(labels_cone == 63542)\n",
    "xxx_df = hits.loc[xxx_ix]\n",
    "xxx_df = xxx_df.sort_values('z')\n",
    "# indexes in question: ???\n",
    "xxx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = truth.loc[xxx_ix]\n",
    "tr_df = tr_df.sort_values('tz')\n",
    "# BAD: 59855, 61697\n",
    "# sorted z, see steps for 'y', ensure they are consistent (good for 59855, ok for 61697)\n",
    "# --> if most in same direction except one/two outliers, and outlier magnitude is large....\n",
    "all_y1 = tr_df.ty.values\n",
    "all_y = np.diff(all_y1)\n",
    "print('mean: ' + str(all_y.mean()))\n",
    "print('max: ' + str(all_y.max()))\n",
    "print('min: ' + str(all_y.min()))\n",
    "#[49193, 59886, 61710]\n",
    "tr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr2_df = truth.loc[truth['particle_id'] == 166642325903114240]\n",
    "tr2_df = tr2_df.sort_values('tz')\n",
    "tr2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track 150 outlier dimension y: [100706]\n",
    "trk large -: 160, large1: -1.592, large2: -61.533\n",
    "track 160 outlier dimension y: [41261]\n",
    "track 169 outlier dimension y: [17867]\n",
    "track 181 outlier dimension y: [103556]\n",
    "track 188 outlier dimension y: [661]\n",
    "track 194 outlier dimension y: [40343]\n",
    "trk large -: 208, large1: -1.2258, large2: -30.6159\n",
    "track 208 outlier dimension y: [23693]\n",
    "trk large -: 223, large1: -1.3697, large2: -36.1324\n",
    "track 223 outlier dimension y: [29081]\n",
    "trk large +: 263, large1: 38.0123, large2: 1.7837\n",
    "track 263 outlier dimension y: [19765]\n",
    "trk large +: 281, large1: 102.727, large2: 3.78752\n",
    "track 281 outlier dimension y: [100657]\n",
    "track 286 outlier dimension y: [62478]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhh_ix = np.where(labels_helix == 5549)\n",
    "hhh_df = truth.loc[hhh_ix]\n",
    "hhh_df = hhh_df.sort_values('tz')\n",
    "# indexes in question: ???\n",
    "hhh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = truth.loc[hhh_ix]\n",
    "tr_df = tr_df.sort_values('tz')\n",
    "# BAD: 59855, 61697\n",
    "# sorted z, see steps for 'y', ensure they are consistent (good for 59855, ok for 61697)\n",
    "# --> if most in same direction except one/two outliers, and outlier magnitude is large....\n",
    "all_y1 = tr_df.ty.values\n",
    "all_y = np.diff(all_y1)\n",
    "print('mean: ' + str(all_y.mean()))\n",
    "print('max: ' + str(all_y.max()))\n",
    "print('min: ' + str(all_y.min()))\n",
    "#[48366, 48517, 51460, 54360, 54471, 57091, 57133]\n",
    "#[6596, 6539, 4232, 4180]\n",
    "tr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_x = np.array([\n",
    "    [\n",
    "        [-12.166300, -156.817993, 962.000],\n",
    "        [20.349701, -242.162003, 1498.500],\n",
    "        [48.221401, -284.333008, 1795.500],\n",
    "        [48.529099, -284.743988, 1798.500],\n",
    "        [122.458000, -404.139008, 2554.500]\n",
    "    ],\n",
    "    [\n",
    "        [-7.17245, -44.973099, -1098.0],\n",
    "        [-6.67366, -39.278999, -962.5],\n",
    "        [-6.65474, -39.091202, -958.0],\n",
    "        [-6.01885, -33.464001, -822.5],\n",
    "        [-5.99555, -33.277901, -818.0]\n",
    "    ],\n",
    "    [\n",
    "        [-6.829120, 30.917900, 40.626202],\n",
    "        [-28.525200, 169.742004, 212.766998],\n",
    "        [-34.125198, 258.694000, 325.647003],\n",
    "        [-31.138201, 361.954987, 453.306000],\n",
    "        [-12.355800, 503.367004, 629.344971]\n",
    "    ]\n",
    "])\n",
    "\n",
    "track_y = np.array([\n",
    "    [1, 1, 1, 1, 0],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [0, 0, 1, 1, 1]\n",
    "])\n",
    "print(track_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = np.unique(labels_h3)\n",
    "track_x = np.zeros((len(tracks), 10, 3))\n",
    "track_y = np.zeros((len(tracks), 10, 1))\n",
    "max_xval = np.amax(hits.x.values)\n",
    "print(max_xval)\n",
    "max_yval = np.amax(hits.y.values)\n",
    "print(max_yval)\n",
    "max_zval = np.amax(hits.z.values)\n",
    "print(max_zval)\n",
    "for idx, track in enumerate(tracks):\n",
    "    if track == 0:\n",
    "        continue\n",
    "    hit_ix = np.where(labels_h3 == track)[0]\n",
    "    df2 = hits.loc[hit_ix]\n",
    "    df2 = df2.sort_values('z_abs')\n",
    "    hit_ix2 = df2.index.values # remember new indexes after sorting\n",
    "    #print(df2)\n",
    "    xs = df2.x.values\n",
    "    ys = df2.y.values\n",
    "    zs = df2.z.values\n",
    "\n",
    "    # From this track, figure out the most common particle ID from the truth.\n",
    "    # Any hits from our track that belong to that particle will be set to '1'\n",
    "    # (correctly predicted hit), otherwise '0' (outlier).\n",
    "    tf2 = truth.loc[hit_ix]\n",
    "    counters = coll.Counter\n",
    "    track_counts = coll.Counter(tf2.particle_id.values).most_common(len(hit_ix2))\n",
    "    track_particle_id = track_counts[0][0]\n",
    "\n",
    "    for i in range(len(hit_ix2)):\n",
    "        if i < 10:\n",
    "            track_x[idx][i][0] = xs[i] / max_xval\n",
    "            track_x[idx][i][1] = ys[i] / max_yval\n",
    "            track_x[idx][i][2] = zs[i] / max_zval\n",
    "            if (truth.loc[hit_ix2[i]].particle_id == track_particle_id):\n",
    "                track_y[idx][i][0] = 3\n",
    "            else:\n",
    "                track_y[idx][i][0] = 30\n",
    "            #track_y[idx][i][0] = (truth.loc[hit_ix2[i]].particle_id == track_particle_id)\n",
    "            \n",
    "    #if idx < 10:\n",
    "    #    ignore_it = True\n",
    "    #elif idx < 20:\n",
    "    #    print(tf2)\n",
    "    #    print(df2)\n",
    "    #    print(track_x[idx])\n",
    "    #    print(track_y[idx])\n",
    "    #else:\n",
    "    #    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as L\n",
    "import keras.models as M\n",
    "\n",
    "# The inputs to the model.\n",
    "# We will create two data points, just for the example.\n",
    "data_x = np.array([\n",
    "    # Datapoint 1\n",
    "#    [\n",
    "#        # Input features at timestep 1\n",
    "#        [1, 2, 3],\n",
    "#        # Input features at timestep 2\n",
    "#        [4, 5, 6]\n",
    "#    ],\n",
    "    # Datapoint 2\n",
    "    [\n",
    "        # Features at timestep 1\n",
    "        [7, 8, 9],\n",
    "        # Features at timestep 2\n",
    "        [10, 11, 12]\n",
    "    ]\n",
    "])\n",
    "\n",
    "# The desired model outputs.\n",
    "# We will create two data points, just for the example.\n",
    "data_y = np.array([\n",
    "    # Datapoint 1\n",
    "    # Target features at timestep 2\n",
    "#    [105, 106, 107, 108, 109],\n",
    "    # Datapoint 2\n",
    "    # Target features at timestep 2\n",
    "    [205, 206, 207, 208, 209]\n",
    "])\n",
    "\n",
    "# Each input data point has 2 timesteps, each with 3 features.\n",
    "# So the input shape (excluding batch_size) is (2, 3), which\n",
    "# matches the shape of each data point in data_x above.\n",
    "model_input = L.Input(shape=(10, 3))\n",
    "\n",
    "# This RNN will return timesteps with 4 features each.\n",
    "# Because return_sequences=True, it will output 2 timesteps, each\n",
    "# with 4 features. So the output shape (excluding batch size) is\n",
    "# (2, 4), which matches the shape of each data point in data_y above.\n",
    "#model_output = L.LSTM(4, return_sequences=False)(model_input)\n",
    "#model_output = L.LSTM(100, return_sequences=True)(model_input)\n",
    "#model_output = L.Dense(100, activation='linear')(model_output)\n",
    "#model_output = L.Dense(4, activation='linear')(model_output)\n",
    "#model_output = L.LSTM(4, return_sequences=False)(model_output)\n",
    "model_output = L.Bidirectional(L.LSTM(10, return_sequences=True, stateful=False))(model_input)\n",
    "#model_output = L.LSTM(30, return_sequences=True, stateful=False)(model_output)\n",
    "model_output = L.Bidirectional(L.LSTM(10, return_sequences=True, stateful=False))(model_output)\n",
    "model_output = L.TimeDistributed(L.Dense(1, activation='linear'))(model_output)\n",
    "#model_output = L.Dense(100, activation='linear')(model_output)\n",
    "#model_output = L.Dense(30, activation='linear')(model_output)\n",
    "# Create the model.\n",
    "model = M.Model(inputs=model_input, outputs=model_output)\n",
    "\n",
    "# You need to pick appropriate loss/optimizers for your problem.\n",
    "# I'm just using these to make the example compile.\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train\n",
    "#model.fit(data_x, data_y)\n",
    "#model.fit(track_x, track_y)\n",
    "\n",
    "# batch_size=3\n",
    "# num_steps=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for e in range(100):\n",
    "#    #for i in range(track_x.shape[0]):\n",
    "#    #    tx = np.expand_dims(track_x[i], axis=0)\n",
    "#    #    ty = np.expand_dims(track_y[i], axis=0)\n",
    "model.fit(track_x, track_y, batch_size=1, epochs=5, verbose=1)\n",
    "#for i in range(track_x.shape[0]):\n",
    "#        tx = np.expand_dims(track_x[i], axis=0)\n",
    "\n",
    "for i in range(track_x.shape[0]):\n",
    "    tx = np.expand_dims(track_x[i], axis=0)\n",
    "    yhat = model.predict(tx)\n",
    "    ty = track_y[i]\n",
    "    print(ty)\n",
    "    print(yhat)\n",
    "    if i > 10:\n",
    "        break\n",
    "# old - (3 LSTM Layers): Loss: 0.0810\n",
    "# old - (2 LSTM, 2 Dense (100, 30)): Loss ~ 0.08\n",
    "# New TimeDistributed Loss: 0.2072\n",
    "# New normalized TimeDistributed loss: 0.3196\n",
    "# New normalized Bidi-LSTM TimeDistributed loss: 0.3100, 0.1920, 0.1362, 0.1273, 0.1193\n",
    "#  -> same, but with 10-hit track input: 0.5413, 0.4646, 0.4242, 0.3943, 0.3550\n",
    "#  -> 3 for right hit, 10 for outlier, 0 for ignore: 5.1141, 5.0738, 5.0602, 5.0038, 4.6757,\n",
    "#                                                    4.1964, 3.9969, 3.8963, 3.8313, 3.7737\n",
    "#  -> 3 for right hit, 30 for outlier, 0 for ignore: 62.5914"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(track_x.shape[0]):\n",
    "    if i < 1000:\n",
    "        continue\n",
    "    tx = np.expand_dims(track_x[i], axis=0)\n",
    "    yhat = model.predict(tx)\n",
    "    ty = track_y[i]\n",
    "    print('Prediction: ' + str(i))\n",
    "    print(ty)\n",
    "    print(yhat)\n",
    "    if i > 1020:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hits.head())\n",
    "print(len(hits))\n",
    "\n",
    "print(particles.head())\n",
    "print(len(particles))\n",
    "\n",
    "print(cells.head())\n",
    "print(len(cells))\n",
    "\n",
    "print(truth.head())\n",
    "print(len(truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the XY plane\n",
    "g = sns.jointplot(hits.x, hits.y, size=12)\n",
    "\n",
    "#Clear the axes containing the scatter plot\n",
    "g.ax_joint.cla()\n",
    "# Set the current axis to the parent of ax\n",
    "plt.sca(g.ax_joint)\n",
    "\n",
    "volumes = hits.volume_id.unique()\n",
    "for volume in volumes:\n",
    "    v = hits[hits.volume_id == volume]\n",
    "    # scattering the hit coordinates with the particle size = 1\n",
    "    plt.scatter(v.x, v.y, s=1, label='volume {}'.format(volume))\n",
    "\n",
    "plt.xlabel('X (mm)')\n",
    "plt.ylabel('Y (mm)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the YZ plane\n",
    "g = sns.jointplot(hits.z, hits.y, s=1, size=12)\n",
    "g.ax_joint.cla()\n",
    "plt.sca(g.ax_joint)\n",
    "\n",
    "volumes = hits.volume_id.unique()\n",
    "for volume in volumes:\n",
    "    v = hits[hits.volume_id == volume]\n",
    "    plt.scatter(v.z, v.y, s=1, label='volume {}'.format(volume))\n",
    "\n",
    "plt.xlabel('Z (mm)')\n",
    "plt.ylabel('Y (mm)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From XYZ 3D perspective\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for volume in volumes:\n",
    "    v = hits[hits.volume_id == volume]\n",
    "    ax.scatter(v.z, v.x, v.y, s=1, label='volume {}'.format(volume), alpha=0.5)\n",
    "ax.set_title('SHit Locations')\n",
    "ax.set_xlabel('Z (millimeters)')\n",
    "ax.set_ylabel('X (millimeters)')\n",
    "ax.set_zlabel('Y (millimeters)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(particles.nhits.values, axlabel='Hits/Particle', bins=50)\n",
    "plt.title('Distribution of number of hits per particle for event 1000.')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(particles.groupby('q')['vx'].count(),\n",
    "        labels=['negative', 'positive'],\n",
    "        autopct='%.0f%%',\n",
    "        shadow=True,\n",
    "        radius=1)\n",
    "plt.title('Distribution of particle charges.')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the original particles of tracks, most particle collisions are generated from the origin\n",
    "\n",
    "g = sns.jointplot(particles.vz, particles.vy,  s=3, size=12)\n",
    "g.ax_joint.cla()\n",
    "plt.sca(g.ax_joint)\n",
    "\n",
    "n_hits = particles.nhits.unique()\n",
    "for n_hit in n_hits:\n",
    "    p = particles[particles.nhits == n_hit]\n",
    "    plt.scatter(p.vz, p.vy, s=1, label='Hits {}'.format(n_hit))\n",
    "\n",
    "plt.xlabel('Z (mm)')\n",
    "plt.ylabel('Y (mm)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "\n",
    "for charge in [-1, 1]:\n",
    "    q = particles[particles.q == charge]\n",
    "    ax.scatter(q.vz, q.vx, q.vy, s=1, label='Charge {}'.format(charge), alpha=0.5)\n",
    "ax.set_title('Sample of 1000 Particle initial location')\n",
    "ax.set_xlabel('Z (millimeters)')\n",
    "ax.set_ylabel('X (millimeters)')\n",
    "ax.set_zlabel('Y (millimeters)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIT_COUNT = 12\n",
    "particle1 = particles.loc[particles.nhits == HIT_COUNT].iloc[0]\n",
    "particle2 = particles.loc[particles.nhits == HIT_COUNT].iloc[1]\n",
    "particle3 = particles.loc[particles.nhits == HIT_COUNT].iloc[2]\n",
    "\n",
    "\n",
    "p_traj_surface1 = truth[truth.particle_id == particle1.particle_id][['tx', 'ty', 'tz']]\n",
    "p_traj_surface2 = truth[truth.particle_id == particle2.particle_id][['tx', 'ty', 'tz']]\n",
    "p_traj_surface3 = truth[truth.particle_id == particle3.particle_id][['tx', 'ty', 'tz']]\n",
    "\n",
    "\n",
    "\n",
    "p_traj1 = (p_traj_surface1\n",
    "          .append({'tx': particle1.vx, 'ty': particle1.vy, 'tz': particle1.vz}, ignore_index=True)\n",
    "          .sort_values(by='tz'))\n",
    "\n",
    "p_traj2 = (p_traj_surface2\n",
    "          .append({'tx': particle2.vx, 'ty': particle2.vy, 'tz': particle2.vz}, ignore_index=True)\n",
    "          .sort_values(by='tz'))\n",
    "\n",
    "p_traj3 = (p_traj_surface3\n",
    "          .append({'tx': particle3.vx, 'ty': particle3.vy, 'tz': particle3.vz}, ignore_index=True)\n",
    "          .sort_values(by='tz'))\n",
    "\n",
    "\n",
    "# Visualize XY projection to the Z-axis\n",
    "\n",
    "plt.plot(p_traj1.tz, p_traj1.ty, '-o', label='hits')\n",
    "plt.plot(p_traj2.tz, p_traj2.ty, '-o', label='hits')\n",
    "plt.plot(p_traj3.tz, p_traj3.ty, '-o', label='hits')\n",
    "plt.xlabel('Z (mm)')\n",
    "plt.ylabel('Y (mm)')\n",
    "plt.title('ZY projection to the X-axis')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot(\n",
    "    xs=p_traj1.tx,\n",
    "    ys=p_traj1.ty,\n",
    "    zs=p_traj1.tz, marker='o')\n",
    "ax.plot(\n",
    "    xs=p_traj2.tx,\n",
    "    ys=p_traj2.ty,\n",
    "    zs=p_traj2.tz, marker='o')\n",
    "ax.plot(\n",
    "    xs=p_traj3.tx,\n",
    "    ys=p_traj3.ty,\n",
    "    zs=p_traj3.tz, marker='o')\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xlabel('X (mm)')\n",
    "ax.set_ylabel('Y (mm)')\n",
    "ax.set_zlabel('Z  (mm) -- Detection layers')\n",
    "plt.title('Trajectories of two particles as they cross the detection surface ($Z$ axis).')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
