{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from trackml.dataset import load_event\n",
    "from trackml.randomize import shuffle_hits\n",
    "from trackml.score import score_event\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import collections as coll\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "import merge as merge\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../../../input/train_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event000001000 memory usage 18.46 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "event_prefix = 'event000001000'\n",
    "hits, cells, particles, truth = load_event(os.path.join(TRAIN_PATH, event_prefix))\n",
    "\n",
    "mem_bytes = (hits.memory_usage(index=True).sum() \n",
    "             + cells.memory_usage(index=True).sum() \n",
    "             + particles.memory_usage(index=True).sum() \n",
    "             + truth.memory_usage(index=True).sum())\n",
    "print('{} memory usage {:.2f} MB'.format(event_prefix, mem_bytes / 2**20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_cone = pd.read_csv('../../event_1003_labels_train_cone.csv').label.values\n",
    "labels_helix = pd.read_csv('../../event_1000_labels_train_helix1.csv').label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2 ..., 7084 7085 7086]\n"
     ]
    }
   ],
   "source": [
    "#uniq_cone = np.unique(labels_cone)\n",
    "uniq_helix = np.unique(labels_helix)\n",
    "#print(uniq_cone)\n",
    "print(uniq_helix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_event_submission(event_id, hits, labels):\n",
    "    sub_data = np.column_stack(([event_id]*len(hits), hits.hit_id.values, labels))\n",
    "    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helix score for event 1000: 0.57485796\n"
     ]
    }
   ],
   "source": [
    "one_submission = create_one_event_submission(1000, hits, labels_helix)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"helix score for event %d: %.8f\" % (1000, score))\n",
    "#one_submission = create_one_event_submission(1000, hits, labels_cone)\n",
    "#score = score_event(truth, one_submission)\n",
    "#print(\"cone score for event %d: %.8f\" % (1000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_simple_merge = merge.merge_tracks(labels_helix, labels_cone)\n",
    "one_submission = create_one_event_submission(1000, hits, labels_simple_merge)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Simple merged cone+helix score for event %d: %.8f\" % (1000, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Removing cone outliers')\n",
    "labels_c3 = np.copy(labels_cone)\n",
    "labels_c3 = remove_outliers(labels_c3, hits, print_counts=True)\n",
    "\n",
    "print('Removing helix outliers')\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "labels_h3 = remove_outliers(labels_h3, hits, print_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Merging tracks')\n",
    "tmp_labels = merge.heuristic_merge_tracks(labels_helix, labels_cone)\n",
    "one_submission = create_one_event_submission(1000, hits, tmp_labels)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"cone score for event %d: %.8f\" % (1000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Merging tracks')\n",
    "tmp_labels = merge.heuristic_merge_tracks(labels_h3, labels_c3)\n",
    "one_submission = create_one_event_submission(1000, hits, tmp_labels)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merged score for event %d: %.8f\" % (1000, score))\n",
    "# Baseline helix prior to outlier removal: 0.51217316\n",
    "# Score: 0.4766-->0.4799 after improved outlier removal\n",
    "# Score is 0.5037 if, after outlier removal,  we only add tracks from labels_c3 when no such track was recorded in labels_h3\n",
    "# Score is 0.51087490 if we only remove bad volumes, duplicatez, and singletons, and only add tracks from labels_c3 when no such track was recorded in labels_h3\n",
    "# Score is 0.51936460, remove badv, dupz, sings, selective track merging.\n",
    "# Score is 0.52234651, remove badv, dupz, sings, selective track merging.\n",
    "# Score is 0.52499622, remove badv, dupz, sings, selective track merging, overwrite smaller tracks of length <= 3.\n",
    "# Score is 0.52653574, same as above except overwrite tracks of length <= 4\n",
    "# Score is 0.52622554, same as above except overwrite tracks of length <= 5\n",
    "# Score is 0.52209245, full outlier removal, overwrite tracks of length <= 4\n",
    "# Score is 0.58658664 orig heuristic\n",
    "#  --> 0.58240360 with aggressive outlier removal\n",
    "#  --> 0.58621259 with aggressive cone removal, non-aggressive helix\n",
    "#  --> 0.58417970 with aggressive helix removal, non-aggressive cone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: Need to evaluate this better, seems to hurt!\n",
    "def find_invalid_volumes(track, labels, df):\n",
    "    invalid_ix = []\n",
    "\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "    df2 = df.loc[hit_ix]\n",
    "    df2 = df2.sort_values('z_abs')\n",
    "    hit_ix2 = df2.index.values\n",
    "    all_positive = np.all(df2.z.values >= 0)\n",
    "    all_negative = np.all(df2.z.values <= 0)\n",
    "    volumes = df2.volume_id.values\n",
    "    layers = df2.layer_id.values\n",
    "    last_volume = volumes[0]\n",
    "    last_layer = layers[0]\n",
    "    # Tracks with the first volume of 8, 13, and 17 are very odd, sometimes\n",
    "    # they hit in the negative way, sometimes the positive way,\n",
    "    # sometimes a mix of both. Ignore these.\n",
    "    if last_volume == 8 or last_volume == 13 or last_volume == 17:\n",
    "        all_negative = False\n",
    "        all_positive = False\n",
    "    for idx, cur_vol in enumerate(volumes):\n",
    "        cur_layer = layers[idx]\n",
    "        if all_positive:\n",
    "            # When we go from one volume to the next, we expect to see the\n",
    "            # layer drop from a large layer id to a smaller layer id.\n",
    "            # If we stay in the same volume, the layer id should not decrease.\n",
    "            #if (last_volume != cur_vol and (cur_layer > (last_layer - 4))) or (last_volume == cur_vol and cur_layer < last_layer):\n",
    "            if (last_volume == cur_vol and cur_layer < last_layer):\n",
    "                invalid_ix.append(hit_ix2[idx])\n",
    "            else:\n",
    "                last_volume = cur_vol\n",
    "                last_layer = cur_layer\n",
    "        elif all_negative:\n",
    "            # When we go from one volume to the next, we expect to see the\n",
    "            # layer increase from a small layer id to a larger layer id.\n",
    "            # If we stay in the same volume, the layer id should not increase.\n",
    "            #if (last_volume != cur_vol and (cur_layer < (last_layer + 4))) or (last_volume == cur_vol and cur_layer > last_layer):\n",
    "            if (last_volume == cur_vol and cur_layer > last_layer):\n",
    "                invalid_ix.append(hit_ix2[idx])\n",
    "            else:\n",
    "                last_volume = volumes[idx]\n",
    "                last_layer = layers[idx]\n",
    "        else:\n",
    "            last_volume = cur_vol\n",
    "            last_layer = cur_layer\n",
    "\n",
    "    return invalid_ix\n",
    "    \n",
    "def find_dimension_outlier(track, labels, df, dimension):\n",
    "    outlier_ix = []\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "\n",
    "    # Need at least 3 values to determine if any look like outliers\n",
    "    if len(hit_ix) < 3:\n",
    "        return outlier_ix\n",
    "\n",
    "    df2 = df.loc[hit_ix]        \n",
    "    df2 = df2.sort_values('z')\n",
    "    hit_ix2 = df2.index.values\n",
    "\n",
    "    # Note, diff[0] is diff between 0 and 1\n",
    "    diffs = np.diff(df2[dimension].values)\n",
    "\n",
    "    growing_trend = 0\n",
    "    shrinking_trend = 0\n",
    "    for idx, diff in enumerate(diffs):\n",
    "        if idx > 0 and diff > diffs[idx-1]:\n",
    "            growing_trend = growing_trend + 1\n",
    "        if idx > 0 and diff < diffs[idx-1]:\n",
    "            shrinking_trend = shrinking_trend + 1\n",
    "\n",
    "    check_largest_and_smallest = True\n",
    "    if growing_trend > math.ceil(0.6*len(diffs)) or shrinking_trend > math.ceil(0.6*len(diffs)):\n",
    "        check_largest_and_smallest = False\n",
    "\n",
    "    if check_largest_and_smallest:\n",
    "        # Find largest and smallest diffs, if largest is 20x larger than 2nd largest,\n",
    "        # or smallest is 20x smaller than 2nd smallest, consider them outliers.\n",
    "        top_two_ix = diffs.argsort()[-2:][::-1]\n",
    "        large1 = diffs[top_two_ix[0]]\n",
    "        large2 = diffs[top_two_ix[1]]\n",
    "        bot_two_ix = diffs.argsort()[:2]\n",
    "        small1 = diffs[bot_two_ix[0]]\n",
    "        small2 = diffs[bot_two_ix[1]]\n",
    "\n",
    "        largest_is_outlier = False\n",
    "        smallest_is_outlier = False\n",
    "        if large1 > 0 and large2 > 0 and large1 > 10.0 and large2 > 2.0 and (large2*7) < large1:\n",
    "            largest_is_outlier = True\n",
    "        if large1 < 0 and large2 < 0 and large1 < -10.0 and large2 < -2.0 and (large1*7) > large2:\n",
    "            largest_is_outlier = True\n",
    "        if small1 > 0 and small2 > 0 and small1 > 10.0 and small2 > 2.0 and (small2*7) < small1:\n",
    "            smallest_is_outlier = True\n",
    "        if small1 < 0 and small2 < 0 and small1 < -10.0 and small2 < -2.0 and (small1*7) > small2:\n",
    "            smallest_is_outlier = True\n",
    "\n",
    "        if largest_is_outlier or smallest_is_outlier:\n",
    "            hit_ix_list = hit_ix.tolist()\n",
    "            for idx, diff in enumerate(diffs):\n",
    "                if (largest_is_outlier and diff == large1) or (smallest_is_outlier and diff == small1):\n",
    "                    #print('Removing extreme outlier diff: ' + str(diff) + ', ix: ' + str(hit_ix2[idx + 1]) + ', from diffs: ' + str(diffs))\n",
    "                    outlier_ix.append(hit_ix2[idx + 1])\n",
    "                    hit_ix_list.remove(hit_ix2[idx + 1])\n",
    "\n",
    "            # Re-generate the diffs now that we've removed the extreme outliers\n",
    "            hit_ix = np.asarray(hit_ix_list)\n",
    "            if len(hit_ix) < 3:\n",
    "                return outlier_ix\n",
    "            df2 = df.loc[hit_ix]        \n",
    "            df2 = df2.sort_values('z')\n",
    "            hit_ix2 = df2.index.values\n",
    "            diffs = np.diff(df2[dimension].values)\n",
    "                \n",
    "    # Restrict to when the majority (75%+) of diffs are all in same direction\n",
    "    neg_diffs = np.where(diffs < 0)[0]\n",
    "    pos_diffs = np.where(diffs >= 0)[0]\n",
    "\n",
    "    #print(df2[dimension].values)\n",
    "    #print(hit_ix)\n",
    "    #print('trk: ' + str(track) + ', diffs: ' + str(diffs))\n",
    "    #print(neg_diffs)\n",
    "    #print(pos_diffs)\n",
    "    #print(df2)\n",
    "\n",
    "    # Restrict to when the majority of diffs are either positive or negative.\n",
    "    # (more difficult to detect outliers if diffs oscillate -ve and +ve)\n",
    "    dim_vals = df2[dimension].values\n",
    "    if len(neg_diffs) >= math.ceil(0.75*len(diffs)):\n",
    "        # remove large positive ones.\n",
    "        growing_trend = 0\n",
    "        previous_diff = 0\n",
    "        for idx, diff in enumerate(diffs):\n",
    "            # Some tracks trend from negative to positive diffs, don't eliminate\n",
    "            # positive values if it looks like we are trending that way.\n",
    "            if idx > 0 and diff > diffs[idx-1]:\n",
    "                growing_trend = growing_trend + 1\n",
    "                if growing_trend > 2:\n",
    "                    break\n",
    "            else:\n",
    "                growing_trend = 0\n",
    "            # Use absolute value > 1.0 in case there is small delta each time,\n",
    "            # we only try to find big jumps in the wrong direction.\n",
    "            #print('nidx-' + dimension + ': ' + str(idx) + ', diff: ' + str(diff) + ', df ix: ' + str(hit_ix2[idx+1]))\n",
    "            if diff > 1.0:\n",
    "                # We sometimes see cases like:\n",
    "                # diff[n-1] = -22\n",
    "                # diff[n] = 12\n",
    "                # diff[n+1] = -14\n",
    "                # In this case, we want to remove n-1 as the outlier, since if that\n",
    "                # was gone, diff[n] would be -10, which is more reasonable.\n",
    "                # In cases where we see:\n",
    "                # diff[0] = 23\n",
    "                # diff[1] = -5\n",
    "                # We want to check the dimension values directly instead of the diffs, it\n",
    "                # could be that val[0] is the outlier.\n",
    "                if idx == 0 and dim_vals[1] > dim_vals[2] and dim_vals[0] < dim_vals[2]:\n",
    "                    # The real outlier is the first entry in the list.\n",
    "                    outlier_ix.append(hit_ix2[0])\n",
    "                elif idx == 0 or idx == (len(diffs)-1) or ((diff + diffs[idx-1]) > 0) or diffs[idx+1] > 0:\n",
    "                    #print('Removing: ' + str(hit_ix2[idx+1]))\n",
    "                    outlier_ix.append(hit_ix2[idx + 1])\n",
    "                else:\n",
    "                    # The real outlier is the previous one (i.e. diff[n-1] in the example above!\n",
    "                    outlier_ix.append(hit_ix2[idx])\n",
    "    \n",
    "    elif len(pos_diffs) >= math.ceil(0.75*len(diffs)):\n",
    "        # remove large negative ones\n",
    "        shrinking_trend = 0\n",
    "        for idx, diff in enumerate(diffs):\n",
    "            # Some tracks trend from positive to negative diffs, don't eliminate\n",
    "            # negative values if it looks like we are trending that way.\n",
    "            if idx > 0 and diff < diffs[idx-1]:\n",
    "                shrinking_trend = shrinking_trend + 1\n",
    "                if shrinking_trend > 2:\n",
    "                    break\n",
    "            else:\n",
    "                shrinking_trend = 0\n",
    "            # Use absolute value > 1.0 in case there is small delta each time,\n",
    "            # we only try to find big jumps in the wrong direction.\n",
    "            #print('pidx-' + dimension + ': ' + str(idx) + ', diff: ' + str(diff) + ', df ix: ' + str(hit_ix2[idx+1]))\n",
    "            if diff < -1.0:\n",
    "                #print('Removing: ' + str(hit_ix2[idx+1]))\n",
    "                # Similar to the negative case above, make sure we remove the real\n",
    "                # outlier, in case the previous diff was misleading.\n",
    "                if idx == 0 and dim_vals[1] < dim_vals[2] and dim_vals[0] > dim_vals[2]:\n",
    "                    # The real outlier is the first entry in the list.\n",
    "                    outlier_ix.append(hit_ix2[0])\n",
    "                elif idx == 0 or idx == (len(diffs)-1) or ((diff + diffs[idx-1]) < 0) or diffs[idx+1] < 0:\n",
    "                    #print('Removing: ' + str(hit_ix2[idx+1]))\n",
    "                    outlier_ix.append(hit_ix2[idx + 1])\n",
    "                else:\n",
    "                    # The real outlier is the previous one (i.e. diff[n-1] in the example above!\n",
    "                    outlier_ix.append(hit_ix2[idx])\n",
    "\n",
    "\n",
    "\n",
    "    # Future ideas for patterns:\n",
    "    # - average positive jump + average negative jump, for values that oscillate +ve and -ve\n",
    "    # - absolute value of jump in same direction, this is hard since some tracks seem jumpy\n",
    "    #   i.e. small diffs followed by a bigger jump, then smaller diffs. May need to tie that\n",
    "    #   in with volume/layer/module ids, i.e. only allow bigger jumps between layers.\n",
    "    return outlier_ix                \n",
    "\n",
    "def find_duplicate_z(track, labels, df):\n",
    "    def number_is_between(a1, a2, a3):\n",
    "        return (a1 >= a2 and a2 >= a3) or (a1 <= a2 and a2 <= a3)\n",
    "\n",
    "    def numbers_are_between(a1, a2, a3, b1, b2, b3):\n",
    "        return number_is_between(a1, a2, a3) and number_is_between(b1, b2, b3)\n",
    "\n",
    "    duplicatez_ix = []\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "\n",
    "    # Need at least 4 values to be able to evaluate duplicate z-values.\n",
    "    if len(hit_ix) < 4:\n",
    "        return duplicatez_ix\n",
    "\n",
    "    df2 = df.loc[hit_ix]        \n",
    "    df2 = df2.sort_values('z_abs')\n",
    "    hit_ix2 = df2.index.values # remember new indexes after sorting\n",
    "    xs = df2.x.values\n",
    "    ys = df2.y.values\n",
    "    zs = df2.z.values\n",
    "    max_idx = len(zs) - 1\n",
    "\n",
    "    z_counts = coll.Counter(df2.z.values).most_common(len(df2.z.values))\n",
    "\n",
    "    if zs[0] == zs[1]:\n",
    "        # zs at the beginning\n",
    "        x1 = xs[2]\n",
    "        x2 = xs[3]\n",
    "        y1 = ys[2]\n",
    "        y2 = ys[3]\n",
    "        if numbers_are_between(xs[0], x1, x2, ys[0], y1, y2) and not numbers_are_between(xs[1], x1, x2, ys[1], y1, y2):\n",
    "            # The first one is more consistent, delete the 2nd duplicate value\n",
    "            duplicatez_ix.append(hit_ix2[1])\n",
    "            #print('xs[1] ' + str(xs[1]) + ' <= x1 ' + str(x1) + ' <= x2 ' + str(x2))\n",
    "            #print('ys[1] ' + str(ys[1]) + ' <= y1 ' + str(y1) + ' <= y2 ' + str(y2))\n",
    "        elif numbers_are_between(xs[1], x1, x2, ys[1], y1, y2) and not numbers_are_between(xs[0], x1, x2, ys[0], y1, y2):\n",
    "            # The second one is more consistent, delete the 1st duplicate value\n",
    "            duplicatez_ix.append(hit_ix2[0])\n",
    "            #print('b')\n",
    "        elif numbers_are_between(xs[0], x1, x2, ys[0], y1, y2) and numbers_are_between(xs[1], x1, x2, ys[1], y1, y2):\n",
    "            # Both z-values seem reasonable, need a tie-breaker to find out which is the right one.\n",
    "            add_code_here = True\n",
    "        # else, neither seem valid, unsure how to proceed, better off not rejecting any.\n",
    "\n",
    "    if zs[-1] == zs[-2]:\n",
    "        # zs at the end\n",
    "        x1 = xs[-4]\n",
    "        x2 = xs[-3]\n",
    "        y1 = ys[-4]\n",
    "        y2 = ys[-3]\n",
    "        if numbers_are_between(x1, x2, xs[-2], y1, y2, ys[-2]) and not numbers_are_between(x1, x2, xs[-1], y1, y2, ys[-1]):\n",
    "            # The first one is more consistent, delete the last duplicate value\n",
    "            duplicatez_ix.append(hit_ix2[-1])\n",
    "        elif numbers_are_between(x1, x2, xs[-1], y1, y2, ys[-1]) and not numbers_are_between(x1, x2, xs[-2], y1, y2, ys[-2]):\n",
    "            # The last one is more consistent, delete the 1st duplicate value\n",
    "            duplicatez_ix.append(hit_ix2[-2])\n",
    "        elif numbers_are_between(x1, x2, xs[-1], y1, y2, ys[-1]) and numbers_are_between(x1, x2, xs[-2], y1, y2, ys[-2]):\n",
    "            # Both z-values seem reasonable, need a tie-breaker to find out which is the right one.\n",
    "            add_code_here = True\n",
    "        # else, neither seem valid, unsure how to proceed, better off not rejecting any.\n",
    "        \n",
    "    # Idea: Find duplicate adjacent z-values. Remember x and y before and after the\n",
    "    # duplicates. Choose z that lies between the two. If z at beginning or end,\n",
    "    # need the two post (or pre-) x/y values to see the expected sign of the diff.\n",
    "\n",
    "    # Note max_idx is largest valid index, we already handled the case where the\n",
    "    # duplicate zs are at the beginning or end of the list.\n",
    "    for idx in range(0, max_idx):\n",
    "        if idx > 0 and (idx+2) <= max_idx and zs[idx] == zs[idx+1]:\n",
    "            x1 = xs[idx-1]\n",
    "            x2 = xs[idx+2]\n",
    "            y1 = ys[idx-1]\n",
    "            y2 = ys[idx+2]\n",
    "            # now, x1 <= z1 <= x2, and y1 <= z1 <= y2\n",
    "            if numbers_are_between(x1, xs[idx], x2, y1, ys[idx], y2) and not numbers_are_between(x1, xs[idx+1], x2, y1, ys[idx+1], y2):\n",
    "                # The first one is more consistent, delete the 2nd duplicate value\n",
    "                duplicatez_ix.append(hit_ix2[idx+1])\n",
    "            elif numbers_are_between(x1, xs[idx+1], x2, y1, ys[idx+1], y2) and not numbers_are_between(x1, xs[idx], x2, y1, ys[idx], y2):\n",
    "                # The second one is more consistent, delete the 1st duplicate value\n",
    "                duplicatez_ix.append(hit_ix2[idx])\n",
    "            elif numbers_are_between(x1, xs[idx], x2, y1, ys[idx], y2) and numbers_are_between(x1, xs[idx+1], x2, y1, ys[idx+1], y2):\n",
    "                # Both z-values seem reasonable, need a tie-breaker to find out which is the right one.\n",
    "                add_code_here = True\n",
    "            # else, neither seem valid, unsure how to proceed, better off not rejecting any.\n",
    "\n",
    "    if z_counts[0][1] > 1:\n",
    "        print('Duplicatez found on track ' + str(track) + ', removed: ' + str(duplicatez_ix))\n",
    "\n",
    "    return duplicatez_ix\n",
    "\n",
    "# TODO pi, -pi discontinuity \n",
    "def remove_track_outliers_slope(track, labels, hits, debug=False):\n",
    "    final_outliers = []\n",
    "\n",
    "    hhh_ix = np.where(labels == track)\n",
    "    hhh_h = hits.loc[hhh_ix].sort_values('z')\n",
    "    \n",
    "    slopes_backward = []\n",
    "    slopes_forward = []\n",
    "\n",
    "    num_hits = len(hhh_h)\n",
    "    # Only reliable with tracks >= 5 hits\n",
    "    if num_hits < 5:\n",
    "        return final_outliers\n",
    "\n",
    "    if debug: print('backward:')\n",
    "    for i in np.arange(num_hits-1,0,-1):\n",
    "        a0 =  hhh_h.a0.values[i]\n",
    "        a1 =  hhh_h.a0.values[i-1]\n",
    "        r0 =  hhh_h.r.values[i]\n",
    "        r1 =  hhh_h.r.values[i-1]\n",
    "        if r0 == r1:\n",
    "            r0 = r0 + 1e-8\n",
    "        slope = (a0-a1)/(r0-r1) \n",
    "        slopes_backward.append(slope)\n",
    "        if debug: print(hhh_h.hit_id.values[i], slope, a0)\n",
    "        if i == 1:\n",
    "            a0 = hhh_h.a0.values[0]\n",
    "            a1 = hhh_h.a0.values[num_hits-1]\n",
    "            r0 =  hhh_h.r.values[0]\n",
    "            r1 =  hhh_h.r.values[num_hits-1]\n",
    "            if r0 == r1:\n",
    "                r0 = r0 + 1e-8\n",
    "            slope = (a0-a1)/(r0-r1)\n",
    "            slopes_backward.append(slope)\n",
    "            if debug: print(hhh_h.hit_id.values[0], slope, a1)\n",
    "\n",
    "    if debug: print('forward:')\n",
    "    for i in np.arange(0,num_hits-1,1):\n",
    "        a0 =  hhh_h.a0.values[i]\n",
    "        a1 =  hhh_h.a0.values[i+1]\n",
    "        r0 =  hhh_h.r.values[i]\n",
    "        r1 =  hhh_h.r.values[i+1]\n",
    "        if r0 == r1:\n",
    "            r1 = r1 + 1e-8\n",
    "        slope = (a1-a0)/(r1-r0) \n",
    "        slopes_forward.append(slope)\n",
    "        if debug: print(hhh_h.hit_id.values[i], slope, a0)\n",
    "\n",
    "        if i == num_hits-2:\n",
    "            a0 = hhh_h.a0.values[0]\n",
    "            a1 = hhh_h.a0.values[num_hits-1]\n",
    "            r0 =  hhh_h.r.values[0]\n",
    "            r1 =  hhh_h.r.values[num_hits-1]\n",
    "            if r0 == r1:\n",
    "                r1 = r1 + 1e-8\n",
    "            slope = (a1-a0)/(r1-r0) \n",
    "            slopes_forward.append(slope)\n",
    "            if debug: print(hhh_h.hit_id.values[num_hits-1], slope, a0)\n",
    "\n",
    "    slopes_backward = np.asarray(slopes_backward)\n",
    "    slopes_backward = np.reshape(slopes_backward, (-1, 1))\n",
    "    slopes_forward = np.asarray(slopes_forward)\n",
    "    slopes_forward = np.reshape(slopes_forward, (-1, 1))\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    X_back = ss.fit_transform(slopes_backward)\n",
    "    X_for = ss.fit_transform(slopes_forward)\n",
    "\n",
    "    cl = DBSCAN(eps=0.0033, min_samples=1)\n",
    "    outlier_labels_backward = cl.fit_predict(X_back)\n",
    "    outlier_labels_forward = cl.fit_predict(X_for)\n",
    "\n",
    "    if debug: print(outlier_labels_backward)\n",
    "    if debug: print(outlier_labels_forward)\n",
    "\n",
    "    track_counts = coll.Counter(outlier_labels_backward).most_common(1)\n",
    "    most_common_id = track_counts[0][0]\n",
    "    most_common_count = track_counts[0][1]\n",
    "\n",
    "    outlier_indices_backward = []\n",
    "    if most_common_count > 1 and len(np.unique(outlier_labels_forward)) < num_hits/2:\n",
    "        for i in np.arange(num_hits-1,-1,-1):\n",
    "            if outlier_labels_backward[i] != most_common_id:\n",
    "                if debug: print(hhh_h.index.values[num_hits-1-i])\n",
    "                outlier_indices_backward.append(hhh_h.index.values[num_hits-1-i])\n",
    "\n",
    "    track_counts = coll.Counter(outlier_labels_forward).most_common(1)\n",
    "    most_common_id = track_counts[0][0]\n",
    "    most_common_count = track_counts[0][1]\n",
    "\n",
    "\n",
    "    outlier_indices_forward = []\n",
    "    if most_common_count > 1 and len(np.unique(outlier_labels_forward)) < num_hits/2:\n",
    "        for i in np.arange(0,num_hits-1,1):\n",
    "            if outlier_labels_forward[i] != most_common_id:\n",
    "                if debug: print(hhh_h.index.values[i])\n",
    "                outlier_indices_forward.append(hhh_h.index.values[i])\n",
    "\n",
    "\n",
    "    outlier_candidates = list(set(outlier_indices_backward).intersection(outlier_indices_forward))\n",
    "\n",
    "\n",
    "    if debug: print('before removal:' + str(outlier_candidates))\n",
    "\n",
    "    for i in range(len(outlier_candidates)):\n",
    "        candidate = hhh_h.loc[outlier_candidates[i]]\n",
    "        found = False\n",
    "        for index, row in hhh_h.iterrows():\n",
    "            if np.absolute(candidate.z-row.z) == 0.5 and candidate.volume_id == row.volume_id \\\n",
    "            and candidate.layer_id == row.layer_id and candidate.module_id != row.module_id:\n",
    "                # true hits\n",
    "                if debug: print('true hit' + str(outlier_candidates[i]))\n",
    "                found = True\n",
    "        if found is False:\n",
    "            final_outliers.append(outlier_candidates[i])\n",
    "\n",
    "    if debug: print('new loutliers:' + str(final_outliers))\n",
    "\n",
    "    # If we determine that half (or more) of the hits need to be removed, we may have messed\n",
    "    # up, so do not return any outliers.\n",
    "    max_removal_threshold = math.floor(num_hits/2)\n",
    "    if len(final_outliers) >= max_removal_threshold:\n",
    "        final_outliers = []\n",
    "\n",
    "    return final_outliers\n",
    "\n",
    "    \n",
    "def remove_track_outliers(track, labels, hits, aggressive):\n",
    "    labels = np.copy(labels)\n",
    "    found_bad_volume = 0\n",
    "    found_bad_dimension = 0\n",
    "    found_bad_slope = 0\n",
    "    found_bad_z = 0\n",
    "\n",
    "    # Check if the sorted hits (on z-axis) go through the volumes\n",
    "    # and layers in the expected order\n",
    "    bad_volume_ix = find_invalid_volumes(track, labels, hits)\n",
    "    if len(bad_volume_ix) > 0:\n",
    "        #print('track ' + str(track) + ' bad volume: ' + str(bad_volume_ix))\n",
    "        found_bad_volume = found_bad_volume + len(bad_volume_ix)\n",
    "        for bvix in bad_volume_ix:\n",
    "            labels[bvix] = 0\n",
    "\n",
    "    if True:\n",
    "        # Check if the sorted hits (on z-axis) go through the volumes\n",
    "        # and layers in the expected order\n",
    "        duplicatez_ix = find_duplicate_z(track, labels, hits)\n",
    "        if len(duplicatez_ix) > 0:\n",
    "            #print('track ' + str(track) + ' duplicate z: ' + str(duplicatez_ix))\n",
    "            found_bad_z = found_bad_z + len(duplicatez_ix)\n",
    "            for bzix in duplicatez_ix:\n",
    "                labels[bzix] = 0\n",
    "\n",
    "    if True:\n",
    "        # Check the helix slope, discard hits that do not match\n",
    "        outlier_slope_ix = remove_track_outliers_slope(track, labels, hits)\n",
    "        if len(outlier_slope_ix) > 0:\n",
    "            #print('track ' + str(track) + ' slope outliers: ' + str(outlier_slope_ix))\n",
    "            found_bad_slope = found_bad_slope + len(outlier_slope_ix)\n",
    "            for oix in outlier_slope_ix:\n",
    "                labels[oix] = 0\n",
    "\n",
    "    if aggressive:\n",
    "        # Next analysis, from remaining hits, sort by 'z' (roughly time-based),\n",
    "        # check for anomolies in other dimensions.\n",
    "        outlier_ix = find_dimension_outlier(track, labels, hits, 'y')\n",
    "        if len(outlier_ix) > 0:\n",
    "            #print('track ' + str(track) + ' outlier dimension y: ' + str(outlier_ix))\n",
    "            found_bad_dimension = found_bad_dimension + len(outlier_ix)\n",
    "            for oix in outlier_ix:\n",
    "                labels[oix] = 0\n",
    "\n",
    "        # Next analysis, from remaining hits, sort by 'z' (roughly time-based),\n",
    "        # check for anomolies in z dimensions (i.e. outliers at beginning/end)\n",
    "        outlier_ix = find_dimension_outlier(track, labels, hits, 'z')\n",
    "        if len(outlier_ix) > 0:\n",
    "            #print('track ' + str(track) + ' outlier dimension z: ' + str(outlier_ix))\n",
    "            found_bad_dimension = found_bad_dimension + len(outlier_ix)\n",
    "            for oix in outlier_ix:\n",
    "                labels[oix] = 0\n",
    "            \n",
    "    return (labels, found_bad_volume, found_bad_dimension, found_bad_z, found_bad_slope)\n",
    "\n",
    "def remove_outliers(labels, hits, aggressive=False, print_counts=True):\n",
    "    tracks = np.unique(labels)\n",
    "    hits['z_abs'] = hits.z.abs()\n",
    "    hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "    hits['a0'] = np.arctan2(hits.y,hits.x)\n",
    "    count_rem_volume = 0\n",
    "    count_rem_dimension = 0\n",
    "    count_duplicatez = 0\n",
    "    count_rem_slope = 0\n",
    "    count_singletons = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 3:\n",
    "            (labels, c1, c2, c3, c4) = remove_track_outliers(track, labels, hits, aggressive)\n",
    "            count_rem_volume = count_rem_volume + c1\n",
    "            count_rem_dimension = count_rem_dimension + c2\n",
    "            count_duplicatez = count_duplicatez + c3\n",
    "            count_rem_slope = count_rem_slope + c4\n",
    "\n",
    "    # Remove singletons, we do not get any score for those. This is done\n",
    "    # last, in case removing the outliers (above) removed enough hits\n",
    "    # from a track to create a new singleton.\n",
    "    tracks = np.unique(labels)\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) == 1:\n",
    "            count_singletons = count_singletons + 1\n",
    "            labels[track_hits[0]] = 0\n",
    "\n",
    "    if print_counts:\n",
    "        print('Total removed due to bad volumes: ' + str(count_rem_volume))\n",
    "        print('Total removed due to bad dimensions: ' + str(count_rem_dimension))\n",
    "        print('Total removed due to duplicate zs: ' + str(count_duplicatez))\n",
    "        print('Total removed due to bad slopes: ' + str(count_rem_slope))\n",
    "        print('Total removed singleton hits: ' + str(count_singletons))\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicatez found on track 4, removed: []\n",
      "Duplicatez found on track 19, removed: []\n",
      "Duplicatez found on track 21, removed: []\n",
      "Duplicatez found on track 24, removed: []\n",
      "Duplicatez found on track 35, removed: []\n",
      "Duplicatez found on track 38, removed: []\n",
      "Duplicatez found on track 40, removed: []\n",
      "Duplicatez found on track 49, removed: []\n",
      "Duplicatez found on track 51, removed: []\n",
      "Duplicatez found on track 61, removed: []\n",
      "Duplicatez found on track 65, removed: []\n",
      "Duplicatez found on track 84, removed: []\n",
      "Duplicatez found on track 90, removed: []\n",
      "Duplicatez found on track 93, removed: []\n",
      "Duplicatez found on track 102, removed: []\n",
      "Duplicatez found on track 108, removed: []\n",
      "Duplicatez found on track 109, removed: [50020]\n",
      "track 109 duplicate z: [50020]\n",
      "Duplicatez found on track 120, removed: []\n",
      "Duplicatez found on track 121, removed: [236, 11267]\n",
      "track 121 duplicate z: [236, 11267]\n",
      "Duplicatez found on track 125, removed: []\n",
      "Duplicatez found on track 126, removed: []\n",
      "Duplicatez found on track 133, removed: []\n",
      "Duplicatez found on track 151, removed: []\n",
      "Duplicatez found on track 157, removed: []\n",
      "Duplicatez found on track 158, removed: []\n",
      "Duplicatez found on track 179, removed: []\n",
      "Duplicatez found on track 196, removed: []\n",
      "Duplicatez found on track 207, removed: []\n",
      "Duplicatez found on track 216, removed: []\n",
      "Duplicatez found on track 221, removed: []\n",
      "Duplicatez found on track 236, removed: []\n",
      "Duplicatez found on track 264, removed: []\n",
      "Duplicatez found on track 270, removed: []\n",
      "Duplicatez found on track 280, removed: []\n",
      "Duplicatez found on track 288, removed: []\n",
      "Duplicatez found on track 290, removed: []\n",
      "Duplicatez found on track 295, removed: []\n",
      "Duplicatez found on track 298, removed: []\n",
      "Duplicatez found on track 304, removed: []\n",
      "Duplicatez found on track 343, removed: []\n",
      "Duplicatez found on track 345, removed: []\n",
      "Duplicatez found on track 355, removed: []\n",
      "Duplicatez found on track 377, removed: []\n",
      "Duplicatez found on track 380, removed: []\n",
      "Duplicatez found on track 397, removed: []\n",
      "Duplicatez found on track 416, removed: []\n",
      "Duplicatez found on track 419, removed: []\n",
      "Duplicatez found on track 420, removed: []\n",
      "Duplicatez found on track 423, removed: []\n",
      "Duplicatez found on track 425, removed: []\n",
      "Duplicatez found on track 426, removed: []\n",
      "Duplicatez found on track 492, removed: []\n",
      "Duplicatez found on track 497, removed: []\n",
      "Duplicatez found on track 507, removed: []\n",
      "Duplicatez found on track 512, removed: []\n",
      "Duplicatez found on track 516, removed: []\n",
      "Duplicatez found on track 530, removed: []\n",
      "Duplicatez found on track 534, removed: []\n",
      "Duplicatez found on track 538, removed: [102024]\n",
      "track 538 duplicate z: [102024]\n",
      "Duplicatez found on track 547, removed: []\n",
      "Duplicatez found on track 554, removed: []\n",
      "Duplicatez found on track 568, removed: []\n",
      "Duplicatez found on track 569, removed: []\n",
      "Duplicatez found on track 584, removed: []\n",
      "Duplicatez found on track 588, removed: []\n",
      "Duplicatez found on track 596, removed: []\n",
      "Duplicatez found on track 599, removed: []\n",
      "Duplicatez found on track 603, removed: []\n",
      "Duplicatez found on track 623, removed: [2729]\n",
      "track 623 duplicate z: [2729]\n",
      "Duplicatez found on track 634, removed: []\n",
      "Duplicatez found on track 647, removed: [253]\n",
      "track 647 duplicate z: [253]\n",
      "Duplicatez found on track 650, removed: []\n",
      "Duplicatez found on track 653, removed: []\n",
      "Duplicatez found on track 658, removed: []\n",
      "Duplicatez found on track 662, removed: []\n",
      "Duplicatez found on track 672, removed: []\n",
      "Duplicatez found on track 677, removed: [104974]\n",
      "track 677 duplicate z: [104974]\n",
      "Duplicatez found on track 690, removed: []\n",
      "Duplicatez found on track 707, removed: []\n",
      "Duplicatez found on track 708, removed: []\n",
      "Duplicatez found on track 719, removed: []\n",
      "Duplicatez found on track 724, removed: []\n",
      "Duplicatez found on track 751, removed: []\n",
      "Duplicatez found on track 760, removed: []\n",
      "Duplicatez found on track 766, removed: []\n",
      "Duplicatez found on track 769, removed: []\n",
      "Duplicatez found on track 795, removed: []\n",
      "Duplicatez found on track 799, removed: []\n",
      "Duplicatez found on track 804, removed: []\n",
      "Duplicatez found on track 819, removed: []\n",
      "Duplicatez found on track 823, removed: []\n",
      "Duplicatez found on track 841, removed: []\n",
      "Duplicatez found on track 849, removed: []\n",
      "Duplicatez found on track 876, removed: []\n",
      "Duplicatez found on track 887, removed: []\n",
      "Duplicatez found on track 891, removed: []\n",
      "Duplicatez found on track 894, removed: []\n",
      "Duplicatez found on track 898, removed: []\n",
      "Duplicatez found on track 899, removed: []\n",
      "Duplicatez found on track 916, removed: []\n",
      "Duplicatez found on track 928, removed: []\n",
      "Duplicatez found on track 934, removed: []\n",
      "Duplicatez found on track 936, removed: []\n",
      "Duplicatez found on track 939, removed: []\n",
      "Duplicatez found on track 942, removed: []\n",
      "Duplicatez found on track 961, removed: []\n",
      "Duplicatez found on track 965, removed: []\n",
      "Duplicatez found on track 983, removed: []\n",
      "Duplicatez found on track 984, removed: []\n",
      "Duplicatez found on track 985, removed: [51445]\n",
      "track 985 duplicate z: [51445]\n",
      "Duplicatez found on track 987, removed: []\n",
      "Duplicatez found on track 995, removed: []\n",
      "Duplicatez found on track 996, removed: []\n",
      "Duplicatez found on track 1002, removed: []\n",
      "Duplicatez found on track 1010, removed: []\n",
      "Duplicatez found on track 1011, removed: []\n",
      "Duplicatez found on track 1012, removed: []\n",
      "Duplicatez found on track 1016, removed: []\n",
      "Duplicatez found on track 1029, removed: []\n",
      "Duplicatez found on track 1033, removed: []\n",
      "Duplicatez found on track 1034, removed: []\n",
      "Duplicatez found on track 1054, removed: []\n",
      "Duplicatez found on track 1058, removed: []\n",
      "Duplicatez found on track 1059, removed: []\n",
      "Duplicatez found on track 1064, removed: [63179]\n",
      "track 1064 duplicate z: [63179]\n",
      "Duplicatez found on track 1076, removed: []\n",
      "Duplicatez found on track 1078, removed: []\n",
      "Duplicatez found on track 1079, removed: []\n",
      "Duplicatez found on track 1080, removed: []\n",
      "Duplicatez found on track 1087, removed: []\n",
      "Duplicatez found on track 1090, removed: []\n",
      "Duplicatez found on track 1095, removed: []\n",
      "Duplicatez found on track 1098, removed: []\n",
      "Duplicatez found on track 1108, removed: []\n",
      "Duplicatez found on track 1122, removed: []\n",
      "Duplicatez found on track 1130, removed: []\n",
      "Duplicatez found on track 1131, removed: []\n",
      "Duplicatez found on track 1132, removed: []\n",
      "Duplicatez found on track 1144, removed: []\n",
      "Duplicatez found on track 1159, removed: []\n",
      "Duplicatez found on track 1161, removed: []\n",
      "Duplicatez found on track 1167, removed: []\n",
      "Duplicatez found on track 1169, removed: [66660]\n",
      "track 1169 duplicate z: [66660]\n",
      "Duplicatez found on track 1175, removed: []\n",
      "Duplicatez found on track 1176, removed: []\n",
      "Duplicatez found on track 1181, removed: []\n",
      "Duplicatez found on track 1183, removed: []\n",
      "Duplicatez found on track 1186, removed: []\n",
      "Duplicatez found on track 1187, removed: []\n",
      "Duplicatez found on track 1197, removed: []\n",
      "Duplicatez found on track 1199, removed: []\n",
      "Duplicatez found on track 1202, removed: []\n",
      "Duplicatez found on track 1209, removed: []\n",
      "Duplicatez found on track 1222, removed: []\n",
      "Duplicatez found on track 1224, removed: []\n",
      "Duplicatez found on track 1229, removed: []\n",
      "Duplicatez found on track 1255, removed: []\n",
      "Duplicatez found on track 1262, removed: []\n",
      "Duplicatez found on track 1271, removed: []\n",
      "Duplicatez found on track 1278, removed: []\n",
      "Duplicatez found on track 1280, removed: []\n",
      "Duplicatez found on track 1308, removed: []\n",
      "Duplicatez found on track 1309, removed: []\n",
      "Duplicatez found on track 1326, removed: [5330]\n",
      "track 1326 duplicate z: [5330]\n",
      "Duplicatez found on track 1334, removed: []\n",
      "Duplicatez found on track 1338, removed: []\n",
      "Duplicatez found on track 1353, removed: []\n",
      "Duplicatez found on track 1367, removed: []\n",
      "Duplicatez found on track 1372, removed: []\n",
      "Duplicatez found on track 1374, removed: []\n",
      "Duplicatez found on track 1377, removed: []\n",
      "Duplicatez found on track 1378, removed: []\n",
      "Duplicatez found on track 1389, removed: []\n",
      "Duplicatez found on track 1396, removed: []\n",
      "Duplicatez found on track 1400, removed: []\n",
      "Duplicatez found on track 1401, removed: []\n",
      "Duplicatez found on track 1412, removed: []\n",
      "Duplicatez found on track 1413, removed: []\n",
      "Duplicatez found on track 1421, removed: []\n",
      "Duplicatez found on track 1423, removed: []\n",
      "Duplicatez found on track 1425, removed: [107246]\n",
      "track 1425 duplicate z: [107246]\n",
      "Duplicatez found on track 1426, removed: [118274]\n",
      "track 1426 duplicate z: [118274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicatez found on track 1437, removed: []\n",
      "Duplicatez found on track 1440, removed: []\n",
      "Duplicatez found on track 1451, removed: [97155]\n",
      "track 1451 duplicate z: [97155]\n",
      "Duplicatez found on track 1453, removed: []\n",
      "Duplicatez found on track 1462, removed: []\n",
      "Duplicatez found on track 1463, removed: []\n",
      "Duplicatez found on track 1464, removed: []\n",
      "Duplicatez found on track 1467, removed: []\n",
      "Duplicatez found on track 1480, removed: []\n",
      "Duplicatez found on track 1482, removed: []\n",
      "Duplicatez found on track 1488, removed: []\n",
      "Duplicatez found on track 1503, removed: []\n",
      "Duplicatez found on track 1504, removed: []\n",
      "Duplicatez found on track 1505, removed: []\n",
      "Duplicatez found on track 1507, removed: []\n",
      "Duplicatez found on track 1519, removed: []\n",
      "Duplicatez found on track 1524, removed: []\n",
      "Duplicatez found on track 1529, removed: []\n",
      "Duplicatez found on track 1533, removed: []\n",
      "Duplicatez found on track 1538, removed: []\n",
      "Duplicatez found on track 1548, removed: []\n",
      "Duplicatez found on track 1550, removed: [69396]\n",
      "track 1550 duplicate z: [69396]\n",
      "Duplicatez found on track 1561, removed: []\n",
      "Duplicatez found on track 1563, removed: []\n",
      "Duplicatez found on track 1571, removed: []\n",
      "Duplicatez found on track 1579, removed: []\n",
      "Duplicatez found on track 1588, removed: []\n",
      "Duplicatez found on track 1590, removed: []\n",
      "Duplicatez found on track 1592, removed: []\n",
      "Duplicatez found on track 1593, removed: []\n",
      "Duplicatez found on track 1605, removed: []\n",
      "Duplicatez found on track 1606, removed: []\n",
      "Duplicatez found on track 1616, removed: []\n",
      "Duplicatez found on track 1619, removed: []\n",
      "Duplicatez found on track 1625, removed: []\n",
      "Duplicatez found on track 1637, removed: []\n",
      "Duplicatez found on track 1641, removed: []\n",
      "Duplicatez found on track 1646, removed: []\n",
      "Duplicatez found on track 1647, removed: []\n",
      "Duplicatez found on track 1660, removed: []\n",
      "Duplicatez found on track 1675, removed: []\n",
      "Duplicatez found on track 1685, removed: []\n",
      "Duplicatez found on track 1688, removed: []\n",
      "Duplicatez found on track 1691, removed: []\n",
      "Duplicatez found on track 1692, removed: []\n",
      "Duplicatez found on track 1696, removed: [22279]\n",
      "track 1696 duplicate z: [22279]\n",
      "Duplicatez found on track 1709, removed: []\n",
      "Duplicatez found on track 1714, removed: []\n",
      "Duplicatez found on track 1719, removed: []\n",
      "Duplicatez found on track 1723, removed: []\n",
      "Duplicatez found on track 1738, removed: []\n",
      "Duplicatez found on track 1746, removed: []\n",
      "Duplicatez found on track 1748, removed: []\n",
      "Duplicatez found on track 1757, removed: []\n",
      "Duplicatez found on track 1765, removed: []\n",
      "Duplicatez found on track 1768, removed: []\n",
      "Duplicatez found on track 1775, removed: []\n",
      "Duplicatez found on track 1781, removed: []\n",
      "Duplicatez found on track 1782, removed: []\n",
      "Duplicatez found on track 1784, removed: []\n",
      "Duplicatez found on track 1792, removed: [99060]\n",
      "track 1792 duplicate z: [99060]\n",
      "Duplicatez found on track 1800, removed: []\n",
      "Duplicatez found on track 1808, removed: []\n",
      "Duplicatez found on track 1815, removed: []\n",
      "Duplicatez found on track 1834, removed: []\n",
      "Duplicatez found on track 1839, removed: []\n",
      "Duplicatez found on track 1848, removed: []\n",
      "Duplicatez found on track 1849, removed: []\n",
      "Duplicatez found on track 1855, removed: []\n",
      "Duplicatez found on track 1860, removed: []\n",
      "Duplicatez found on track 1865, removed: []\n",
      "Duplicatez found on track 1869, removed: []\n",
      "Duplicatez found on track 1883, removed: []\n",
      "Duplicatez found on track 1887, removed: []\n",
      "Duplicatez found on track 1893, removed: []\n",
      "Duplicatez found on track 1894, removed: []\n",
      "Duplicatez found on track 1902, removed: []\n",
      "Duplicatez found on track 1903, removed: []\n",
      "Duplicatez found on track 1904, removed: []\n",
      "Duplicatez found on track 1921, removed: []\n",
      "Duplicatez found on track 1926, removed: []\n",
      "Duplicatez found on track 1950, removed: []\n",
      "Duplicatez found on track 1954, removed: []\n",
      "Duplicatez found on track 1956, removed: []\n",
      "Duplicatez found on track 1957, removed: [65711]\n",
      "track 1957 duplicate z: [65711]\n",
      "Duplicatez found on track 2001, removed: []\n",
      "Duplicatez found on track 2008, removed: []\n",
      "Duplicatez found on track 2009, removed: []\n",
      "Duplicatez found on track 2011, removed: []\n",
      "Duplicatez found on track 2019, removed: [115371]\n",
      "track 2019 duplicate z: [115371]\n",
      "Duplicatez found on track 2022, removed: []\n",
      "Duplicatez found on track 2025, removed: []\n",
      "Duplicatez found on track 2029, removed: []\n",
      "Duplicatez found on track 2032, removed: []\n",
      "Duplicatez found on track 2038, removed: []\n",
      "Duplicatez found on track 2041, removed: []\n",
      "Duplicatez found on track 2047, removed: []\n",
      "Duplicatez found on track 2056, removed: []\n",
      "Duplicatez found on track 2064, removed: []\n",
      "Duplicatez found on track 2067, removed: []\n",
      "Duplicatez found on track 2072, removed: []\n",
      "Duplicatez found on track 2078, removed: []\n",
      "Duplicatez found on track 2083, removed: []\n",
      "Duplicatez found on track 2088, removed: []\n",
      "Duplicatez found on track 2095, removed: []\n",
      "Duplicatez found on track 2096, removed: []\n",
      "Duplicatez found on track 2105, removed: []\n",
      "Duplicatez found on track 2120, removed: []\n",
      "Duplicatez found on track 2122, removed: []\n",
      "Duplicatez found on track 2125, removed: []\n",
      "Duplicatez found on track 2129, removed: []\n",
      "Duplicatez found on track 2138, removed: []\n",
      "Duplicatez found on track 2140, removed: []\n",
      "Duplicatez found on track 2153, removed: []\n",
      "Duplicatez found on track 2156, removed: []\n",
      "Duplicatez found on track 2169, removed: []\n",
      "Duplicatez found on track 2171, removed: []\n",
      "Duplicatez found on track 2178, removed: []\n",
      "Duplicatez found on track 2179, removed: []\n",
      "Duplicatez found on track 2202, removed: []\n",
      "Duplicatez found on track 2203, removed: []\n",
      "Duplicatez found on track 2206, removed: []\n",
      "Duplicatez found on track 2214, removed: []\n",
      "Duplicatez found on track 2228, removed: []\n",
      "Duplicatez found on track 2233, removed: []\n",
      "Duplicatez found on track 2238, removed: []\n",
      "Duplicatez found on track 2260, removed: []\n",
      "Duplicatez found on track 2263, removed: []\n",
      "Duplicatez found on track 2264, removed: []\n",
      "Duplicatez found on track 2268, removed: []\n",
      "Duplicatez found on track 2275, removed: []\n",
      "Duplicatez found on track 2276, removed: []\n",
      "Duplicatez found on track 2279, removed: []\n",
      "Duplicatez found on track 2308, removed: []\n",
      "Duplicatez found on track 2310, removed: []\n",
      "Duplicatez found on track 2312, removed: []\n",
      "Duplicatez found on track 2314, removed: []\n",
      "Duplicatez found on track 2323, removed: []\n",
      "Duplicatez found on track 2327, removed: []\n",
      "Duplicatez found on track 2329, removed: []\n",
      "Duplicatez found on track 2334, removed: []\n",
      "Duplicatez found on track 2338, removed: [119664]\n",
      "track 2338 duplicate z: [119664]\n",
      "Duplicatez found on track 2345, removed: []\n",
      "Duplicatez found on track 2348, removed: [115814]\n",
      "track 2348 duplicate z: [115814]\n",
      "Duplicatez found on track 2355, removed: []\n",
      "Duplicatez found on track 2359, removed: []\n",
      "Duplicatez found on track 2373, removed: []\n",
      "Duplicatez found on track 2374, removed: []\n",
      "Duplicatez found on track 2382, removed: []\n",
      "Duplicatez found on track 2383, removed: []\n",
      "Duplicatez found on track 2393, removed: []\n",
      "Duplicatez found on track 2398, removed: []\n",
      "Duplicatez found on track 2399, removed: []\n",
      "Duplicatez found on track 2404, removed: []\n",
      "Duplicatez found on track 2409, removed: []\n",
      "Duplicatez found on track 2411, removed: []\n",
      "Duplicatez found on track 2420, removed: []\n",
      "Duplicatez found on track 2431, removed: []\n",
      "Duplicatez found on track 2435, removed: []\n",
      "Duplicatez found on track 2436, removed: []\n",
      "Duplicatez found on track 2438, removed: []\n",
      "Duplicatez found on track 2445, removed: []\n",
      "Duplicatez found on track 2447, removed: []\n",
      "Duplicatez found on track 2457, removed: []\n",
      "Duplicatez found on track 2462, removed: []\n",
      "Duplicatez found on track 2465, removed: []\n",
      "Duplicatez found on track 2466, removed: []\n",
      "Duplicatez found on track 2470, removed: []\n",
      "Duplicatez found on track 2472, removed: []\n",
      "Duplicatez found on track 2482, removed: []\n",
      "Duplicatez found on track 2489, removed: []\n",
      "Duplicatez found on track 2501, removed: []\n",
      "Duplicatez found on track 2507, removed: []\n",
      "Duplicatez found on track 2508, removed: []\n",
      "Duplicatez found on track 2509, removed: []\n",
      "Duplicatez found on track 2514, removed: []\n",
      "Duplicatez found on track 2529, removed: []\n",
      "Duplicatez found on track 2531, removed: []\n",
      "Duplicatez found on track 2534, removed: []\n",
      "Duplicatez found on track 2539, removed: []\n",
      "Duplicatez found on track 2554, removed: [57346]\n",
      "track 2554 duplicate z: [57346]\n",
      "Duplicatez found on track 2561, removed: []\n",
      "Duplicatez found on track 2569, removed: []\n",
      "Duplicatez found on track 2570, removed: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicatez found on track 2579, removed: []\n",
      "Duplicatez found on track 2581, removed: [76031]\n",
      "track 2581 duplicate z: [76031]\n",
      "Duplicatez found on track 2585, removed: []\n",
      "Duplicatez found on track 2588, removed: []\n",
      "Duplicatez found on track 2593, removed: []\n",
      "Duplicatez found on track 2598, removed: []\n",
      "Duplicatez found on track 2602, removed: []\n",
      "Duplicatez found on track 2605, removed: []\n",
      "Duplicatez found on track 2611, removed: []\n",
      "Duplicatez found on track 2617, removed: []\n",
      "Duplicatez found on track 2618, removed: []\n",
      "Duplicatez found on track 2620, removed: []\n",
      "Duplicatez found on track 2626, removed: []\n",
      "Duplicatez found on track 2631, removed: []\n",
      "Duplicatez found on track 2633, removed: []\n",
      "Duplicatez found on track 2641, removed: []\n",
      "Duplicatez found on track 2642, removed: []\n",
      "Duplicatez found on track 2645, removed: []\n",
      "Duplicatez found on track 2647, removed: [99500]\n",
      "track 2647 duplicate z: [99500]\n",
      "Duplicatez found on track 2649, removed: []\n",
      "Duplicatez found on track 2651, removed: []\n",
      "Duplicatez found on track 2655, removed: []\n",
      "Duplicatez found on track 2662, removed: []\n",
      "Duplicatez found on track 2663, removed: []\n",
      "Duplicatez found on track 2675, removed: []\n",
      "Duplicatez found on track 2676, removed: []\n",
      "Duplicatez found on track 2687, removed: []\n",
      "Duplicatez found on track 2688, removed: []\n",
      "Duplicatez found on track 2694, removed: []\n",
      "Duplicatez found on track 2695, removed: []\n",
      "Duplicatez found on track 2707, removed: []\n",
      "Duplicatez found on track 2728, removed: []\n",
      "Duplicatez found on track 2729, removed: []\n",
      "Duplicatez found on track 2730, removed: []\n",
      "Duplicatez found on track 2734, removed: []\n",
      "Duplicatez found on track 2736, removed: []\n",
      "Duplicatez found on track 2740, removed: []\n",
      "Duplicatez found on track 2741, removed: []\n",
      "Duplicatez found on track 2743, removed: []\n",
      "Duplicatez found on track 2748, removed: []\n",
      "Duplicatez found on track 2749, removed: []\n",
      "Duplicatez found on track 2758, removed: []\n",
      "Duplicatez found on track 2773, removed: []\n",
      "Duplicatez found on track 2775, removed: []\n",
      "Duplicatez found on track 2787, removed: []\n",
      "Duplicatez found on track 2788, removed: []\n",
      "Duplicatez found on track 2789, removed: []\n",
      "Duplicatez found on track 2802, removed: []\n",
      "Duplicatez found on track 2805, removed: []\n",
      "Duplicatez found on track 2806, removed: []\n",
      "Duplicatez found on track 2822, removed: []\n",
      "Duplicatez found on track 2827, removed: []\n",
      "Duplicatez found on track 2831, removed: []\n",
      "Duplicatez found on track 2832, removed: []\n",
      "Duplicatez found on track 2833, removed: []\n",
      "Duplicatez found on track 2834, removed: []\n",
      "Duplicatez found on track 2845, removed: []\n",
      "Duplicatez found on track 2849, removed: []\n",
      "Duplicatez found on track 2851, removed: []\n",
      "Duplicatez found on track 2857, removed: []\n",
      "Duplicatez found on track 2858, removed: []\n",
      "Duplicatez found on track 2861, removed: [102855]\n",
      "track 2861 duplicate z: [102855]\n",
      "Duplicatez found on track 2868, removed: []\n",
      "Duplicatez found on track 2888, removed: []\n",
      "Duplicatez found on track 2891, removed: []\n",
      "Duplicatez found on track 2904, removed: [103877]\n",
      "track 2904 duplicate z: [103877]\n",
      "Duplicatez found on track 2907, removed: []\n",
      "Duplicatez found on track 2910, removed: []\n",
      "Duplicatez found on track 2911, removed: []\n",
      "Duplicatez found on track 2913, removed: []\n",
      "Duplicatez found on track 2914, removed: []\n",
      "Duplicatez found on track 2923, removed: []\n",
      "Duplicatez found on track 2946, removed: []\n",
      "Duplicatez found on track 2957, removed: []\n",
      "Duplicatez found on track 2960, removed: []\n",
      "Duplicatez found on track 2962, removed: []\n",
      "Duplicatez found on track 2963, removed: []\n",
      "Duplicatez found on track 2966, removed: []\n",
      "Duplicatez found on track 2972, removed: []\n",
      "Duplicatez found on track 2973, removed: []\n",
      "Duplicatez found on track 2980, removed: []\n",
      "Duplicatez found on track 2981, removed: []\n",
      "Duplicatez found on track 2985, removed: []\n",
      "Duplicatez found on track 2996, removed: []\n",
      "Duplicatez found on track 2998, removed: []\n",
      "Duplicatez found on track 3003, removed: []\n",
      "Duplicatez found on track 3008, removed: []\n",
      "Duplicatez found on track 3010, removed: []\n",
      "Duplicatez found on track 3018, removed: []\n",
      "Duplicatez found on track 3029, removed: []\n",
      "Duplicatez found on track 3034, removed: []\n",
      "Duplicatez found on track 3035, removed: []\n",
      "Duplicatez found on track 3038, removed: [10482]\n",
      "track 3038 duplicate z: [10482]\n",
      "Duplicatez found on track 3039, removed: []\n",
      "Duplicatez found on track 3040, removed: []\n",
      "Duplicatez found on track 3041, removed: []\n",
      "Duplicatez found on track 3044, removed: []\n",
      "Duplicatez found on track 3048, removed: []\n",
      "Duplicatez found on track 3051, removed: []\n",
      "Duplicatez found on track 3052, removed: []\n",
      "Duplicatez found on track 3056, removed: []\n",
      "Duplicatez found on track 3057, removed: []\n",
      "Duplicatez found on track 3058, removed: [107376]\n",
      "track 3058 duplicate z: [107376]\n",
      "Duplicatez found on track 3062, removed: []\n",
      "Duplicatez found on track 3065, removed: []\n",
      "Duplicatez found on track 3073, removed: []\n",
      "Duplicatez found on track 3076, removed: []\n",
      "Duplicatez found on track 3079, removed: []\n",
      "Duplicatez found on track 3093, removed: []\n",
      "Duplicatez found on track 3094, removed: []\n",
      "Duplicatez found on track 3105, removed: []\n",
      "Duplicatez found on track 3107, removed: []\n",
      "Duplicatez found on track 3118, removed: []\n",
      "Duplicatez found on track 3127, removed: []\n",
      "Duplicatez found on track 3128, removed: []\n",
      "Duplicatez found on track 3136, removed: []\n",
      "Duplicatez found on track 3141, removed: []\n",
      "Duplicatez found on track 3144, removed: []\n",
      "Duplicatez found on track 3154, removed: []\n",
      "Duplicatez found on track 3161, removed: []\n",
      "Duplicatez found on track 3183, removed: []\n",
      "Duplicatez found on track 3187, removed: []\n",
      "Duplicatez found on track 3191, removed: []\n",
      "Duplicatez found on track 3212, removed: []\n",
      "Duplicatez found on track 3223, removed: []\n",
      "Duplicatez found on track 3247, removed: []\n",
      "Duplicatez found on track 3248, removed: []\n",
      "Duplicatez found on track 3255, removed: []\n",
      "Duplicatez found on track 3314, removed: []\n",
      "Duplicatez found on track 3315, removed: []\n",
      "Duplicatez found on track 3322, removed: []\n",
      "Duplicatez found on track 3337, removed: []\n",
      "Duplicatez found on track 3343, removed: []\n",
      "Duplicatez found on track 3365, removed: []\n",
      "Duplicatez found on track 3371, removed: []\n",
      "Duplicatez found on track 3377, removed: []\n",
      "Duplicatez found on track 3385, removed: []\n",
      "Duplicatez found on track 3387, removed: []\n",
      "Duplicatez found on track 3409, removed: []\n",
      "Duplicatez found on track 3414, removed: []\n",
      "Duplicatez found on track 3415, removed: []\n",
      "Duplicatez found on track 3422, removed: []\n",
      "Duplicatez found on track 3426, removed: []\n",
      "Duplicatez found on track 3443, removed: []\n",
      "Duplicatez found on track 3449, removed: []\n",
      "Duplicatez found on track 3450, removed: []\n",
      "Duplicatez found on track 3488, removed: []\n",
      "Duplicatez found on track 3492, removed: []\n",
      "Duplicatez found on track 3495, removed: []\n",
      "Duplicatez found on track 3497, removed: []\n",
      "Duplicatez found on track 3501, removed: []\n",
      "Duplicatez found on track 3504, removed: [65364]\n",
      "track 3504 duplicate z: [65364]\n",
      "Duplicatez found on track 3505, removed: []\n",
      "Duplicatez found on track 3507, removed: []\n",
      "Duplicatez found on track 3509, removed: []\n",
      "Duplicatez found on track 3519, removed: []\n",
      "Duplicatez found on track 3528, removed: []\n",
      "Duplicatez found on track 3529, removed: []\n",
      "Duplicatez found on track 3534, removed: []\n",
      "Duplicatez found on track 3538, removed: []\n",
      "Duplicatez found on track 3543, removed: []\n",
      "Duplicatez found on track 3551, removed: []\n",
      "Duplicatez found on track 3553, removed: []\n",
      "Duplicatez found on track 3558, removed: []\n",
      "Duplicatez found on track 3561, removed: []\n",
      "Duplicatez found on track 3586, removed: []\n",
      "Duplicatez found on track 3596, removed: []\n",
      "Duplicatez found on track 3603, removed: []\n",
      "Duplicatez found on track 3604, removed: []\n",
      "Duplicatez found on track 3608, removed: []\n",
      "Duplicatez found on track 3628, removed: []\n",
      "Duplicatez found on track 3630, removed: [111872]\n",
      "track 3630 duplicate z: [111872]\n",
      "Duplicatez found on track 3639, removed: []\n",
      "Duplicatez found on track 3640, removed: []\n",
      "Duplicatez found on track 3642, removed: []\n",
      "Duplicatez found on track 3643, removed: []\n",
      "Duplicatez found on track 3649, removed: []\n",
      "Duplicatez found on track 3653, removed: []\n",
      "Duplicatez found on track 3656, removed: []\n",
      "Duplicatez found on track 3666, removed: []\n",
      "Duplicatez found on track 3674, removed: []\n",
      "Duplicatez found on track 3682, removed: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicatez found on track 3683, removed: []\n",
      "Duplicatez found on track 3695, removed: []\n",
      "Duplicatez found on track 3701, removed: [68435]\n",
      "track 3701 duplicate z: [68435]\n",
      "Duplicatez found on track 3705, removed: []\n",
      "Duplicatez found on track 3716, removed: []\n",
      "Duplicatez found on track 3718, removed: []\n",
      "Duplicatez found on track 3723, removed: []\n",
      "Duplicatez found on track 3725, removed: []\n",
      "Duplicatez found on track 3726, removed: []\n",
      "Duplicatez found on track 3730, removed: []\n",
      "Duplicatez found on track 3738, removed: []\n",
      "Duplicatez found on track 3742, removed: []\n",
      "Duplicatez found on track 3743, removed: []\n",
      "Duplicatez found on track 3745, removed: []\n",
      "Duplicatez found on track 3746, removed: []\n",
      "Duplicatez found on track 3748, removed: []\n",
      "Duplicatez found on track 3762, removed: []\n",
      "Duplicatez found on track 3768, removed: []\n",
      "Duplicatez found on track 3770, removed: []\n",
      "Duplicatez found on track 3775, removed: []\n",
      "Duplicatez found on track 3782, removed: []\n",
      "Duplicatez found on track 3785, removed: []\n",
      "Duplicatez found on track 3789, removed: []\n",
      "Duplicatez found on track 3790, removed: []\n",
      "Duplicatez found on track 3792, removed: []\n",
      "Duplicatez found on track 3805, removed: []\n",
      "Duplicatez found on track 3808, removed: []\n",
      "Duplicatez found on track 3812, removed: []\n",
      "Duplicatez found on track 3823, removed: []\n",
      "Duplicatez found on track 3830, removed: []\n",
      "Duplicatez found on track 3844, removed: []\n",
      "Duplicatez found on track 3849, removed: []\n",
      "Duplicatez found on track 3860, removed: []\n",
      "Duplicatez found on track 3869, removed: []\n",
      "Duplicatez found on track 3882, removed: []\n",
      "Duplicatez found on track 3893, removed: []\n",
      "Duplicatez found on track 3898, removed: []\n",
      "Duplicatez found on track 3899, removed: [81717]\n",
      "track 3899 duplicate z: [81717]\n",
      "Duplicatez found on track 3904, removed: []\n",
      "Duplicatez found on track 3907, removed: []\n",
      "Duplicatez found on track 3912, removed: []\n",
      "Duplicatez found on track 3914, removed: []\n",
      "Duplicatez found on track 3917, removed: []\n",
      "Duplicatez found on track 3925, removed: []\n",
      "Duplicatez found on track 3930, removed: []\n",
      "Duplicatez found on track 3945, removed: []\n",
      "Duplicatez found on track 3949, removed: []\n",
      "Duplicatez found on track 3951, removed: []\n",
      "Duplicatez found on track 3956, removed: []\n",
      "Duplicatez found on track 3959, removed: []\n",
      "Duplicatez found on track 3965, removed: []\n",
      "Duplicatez found on track 3970, removed: []\n",
      "Duplicatez found on track 3979, removed: []\n",
      "Duplicatez found on track 3991, removed: []\n",
      "Duplicatez found on track 4003, removed: []\n",
      "Duplicatez found on track 4007, removed: []\n",
      "Duplicatez found on track 4013, removed: []\n",
      "Duplicatez found on track 4027, removed: []\n",
      "Duplicatez found on track 4028, removed: []\n",
      "Duplicatez found on track 4031, removed: []\n",
      "Duplicatez found on track 4033, removed: []\n",
      "Duplicatez found on track 4052, removed: []\n",
      "Duplicatez found on track 4057, removed: []\n",
      "Duplicatez found on track 4062, removed: []\n",
      "Duplicatez found on track 4063, removed: []\n",
      "Duplicatez found on track 4064, removed: [105918]\n",
      "track 4064 duplicate z: [105918]\n",
      "Duplicatez found on track 4065, removed: []\n",
      "Duplicatez found on track 4075, removed: []\n",
      "Duplicatez found on track 4081, removed: []\n",
      "Duplicatez found on track 4084, removed: []\n",
      "Duplicatez found on track 4090, removed: []\n",
      "Duplicatez found on track 4103, removed: []\n",
      "Duplicatez found on track 4106, removed: []\n",
      "Duplicatez found on track 4125, removed: []\n",
      "Duplicatez found on track 4126, removed: []\n",
      "Duplicatez found on track 4127, removed: []\n",
      "Duplicatez found on track 4135, removed: []\n",
      "Duplicatez found on track 4138, removed: []\n",
      "Duplicatez found on track 4144, removed: []\n",
      "Duplicatez found on track 4152, removed: []\n",
      "Duplicatez found on track 4154, removed: []\n",
      "Duplicatez found on track 4155, removed: []\n",
      "Duplicatez found on track 4156, removed: []\n",
      "Duplicatez found on track 4158, removed: []\n",
      "Duplicatez found on track 4167, removed: []\n",
      "Duplicatez found on track 4170, removed: []\n",
      "Duplicatez found on track 4171, removed: []\n",
      "Duplicatez found on track 4172, removed: []\n",
      "Duplicatez found on track 4173, removed: []\n",
      "Duplicatez found on track 4174, removed: []\n",
      "Duplicatez found on track 4179, removed: []\n",
      "Duplicatez found on track 4184, removed: []\n",
      "Duplicatez found on track 4196, removed: []\n",
      "Duplicatez found on track 4205, removed: [119616]\n",
      "track 4205 duplicate z: [119616]\n",
      "Duplicatez found on track 4212, removed: []\n",
      "Duplicatez found on track 4217, removed: []\n",
      "Duplicatez found on track 4225, removed: []\n",
      "Duplicatez found on track 4232, removed: []\n",
      "Duplicatez found on track 4233, removed: []\n",
      "Duplicatez found on track 4242, removed: []\n",
      "Duplicatez found on track 4254, removed: []\n",
      "Duplicatez found on track 4258, removed: []\n",
      "Duplicatez found on track 4279, removed: []\n",
      "Duplicatez found on track 4280, removed: []\n",
      "Duplicatez found on track 4299, removed: []\n",
      "Duplicatez found on track 4302, removed: []\n",
      "Duplicatez found on track 4304, removed: []\n",
      "Duplicatez found on track 4305, removed: []\n",
      "Duplicatez found on track 4311, removed: []\n",
      "Duplicatez found on track 4317, removed: []\n",
      "Duplicatez found on track 4318, removed: []\n",
      "Duplicatez found on track 4343, removed: []\n",
      "Duplicatez found on track 4355, removed: []\n",
      "Duplicatez found on track 4357, removed: []\n",
      "Duplicatez found on track 4362, removed: []\n",
      "Duplicatez found on track 4368, removed: []\n",
      "Duplicatez found on track 4369, removed: []\n",
      "Duplicatez found on track 4371, removed: []\n",
      "Duplicatez found on track 4376, removed: []\n",
      "Duplicatez found on track 4380, removed: []\n",
      "Duplicatez found on track 4386, removed: []\n",
      "Duplicatez found on track 4388, removed: []\n",
      "Duplicatez found on track 4389, removed: []\n",
      "Duplicatez found on track 4394, removed: []\n",
      "Duplicatez found on track 4397, removed: []\n",
      "Duplicatez found on track 4404, removed: []\n",
      "Duplicatez found on track 4421, removed: []\n",
      "Duplicatez found on track 4423, removed: []\n",
      "Duplicatez found on track 4436, removed: []\n",
      "Duplicatez found on track 4445, removed: []\n",
      "Duplicatez found on track 4451, removed: []\n",
      "Duplicatez found on track 4453, removed: []\n",
      "Duplicatez found on track 4458, removed: []\n",
      "Duplicatez found on track 4459, removed: []\n",
      "Duplicatez found on track 4473, removed: []\n",
      "Duplicatez found on track 4475, removed: []\n",
      "Duplicatez found on track 4492, removed: []\n",
      "Duplicatez found on track 4499, removed: []\n",
      "Duplicatez found on track 4507, removed: []\n",
      "Duplicatez found on track 4509, removed: []\n",
      "Duplicatez found on track 4514, removed: []\n",
      "Duplicatez found on track 4516, removed: []\n",
      "Duplicatez found on track 4522, removed: []\n",
      "Duplicatez found on track 4523, removed: []\n",
      "Duplicatez found on track 4540, removed: []\n",
      "Duplicatez found on track 4546, removed: []\n",
      "Duplicatez found on track 4564, removed: []\n",
      "Duplicatez found on track 4565, removed: []\n",
      "Duplicatez found on track 4574, removed: []\n",
      "Duplicatez found on track 4582, removed: []\n",
      "Duplicatez found on track 4584, removed: []\n",
      "Duplicatez found on track 4590, removed: []\n",
      "Duplicatez found on track 4593, removed: []\n",
      "Duplicatez found on track 4594, removed: []\n",
      "Duplicatez found on track 4606, removed: []\n",
      "Duplicatez found on track 4618, removed: []\n",
      "Duplicatez found on track 4619, removed: []\n",
      "Duplicatez found on track 4621, removed: []\n",
      "Duplicatez found on track 4622, removed: []\n",
      "Duplicatez found on track 4624, removed: []\n",
      "Duplicatez found on track 4625, removed: []\n",
      "Duplicatez found on track 4628, removed: []\n",
      "Duplicatez found on track 4633, removed: []\n",
      "Duplicatez found on track 4634, removed: []\n",
      "Duplicatez found on track 4637, removed: []\n",
      "Duplicatez found on track 4641, removed: []\n",
      "Duplicatez found on track 4642, removed: []\n",
      "Duplicatez found on track 4657, removed: []\n",
      "Duplicatez found on track 4662, removed: []\n",
      "Duplicatez found on track 4663, removed: []\n",
      "Duplicatez found on track 4669, removed: []\n",
      "Duplicatez found on track 4670, removed: []\n",
      "Duplicatez found on track 4673, removed: [61915]\n",
      "track 4673 duplicate z: [61915]\n",
      "Duplicatez found on track 4674, removed: []\n",
      "Duplicatez found on track 4677, removed: []\n",
      "Duplicatez found on track 4683, removed: []\n",
      "Duplicatez found on track 4685, removed: [102294]\n",
      "track 4685 duplicate z: [102294]\n",
      "Duplicatez found on track 4686, removed: []\n",
      "Duplicatez found on track 4694, removed: []\n",
      "Duplicatez found on track 4702, removed: []\n",
      "Duplicatez found on track 4706, removed: []\n",
      "Duplicatez found on track 4709, removed: []\n",
      "Duplicatez found on track 4728, removed: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicatez found on track 4740, removed: []\n",
      "Duplicatez found on track 4743, removed: []\n",
      "Duplicatez found on track 4744, removed: []\n",
      "Duplicatez found on track 4745, removed: []\n",
      "Duplicatez found on track 4748, removed: []\n",
      "Duplicatez found on track 4753, removed: []\n",
      "Duplicatez found on track 4760, removed: []\n",
      "Duplicatez found on track 4761, removed: []\n",
      "Duplicatez found on track 4770, removed: [8590]\n",
      "track 4770 duplicate z: [8590]\n",
      "Duplicatez found on track 4776, removed: []\n",
      "Duplicatez found on track 4796, removed: []\n",
      "Duplicatez found on track 4810, removed: []\n",
      "Duplicatez found on track 4816, removed: []\n",
      "Duplicatez found on track 4830, removed: []\n",
      "Duplicatez found on track 4838, removed: []\n",
      "Duplicatez found on track 4841, removed: []\n",
      "Duplicatez found on track 4843, removed: []\n",
      "Duplicatez found on track 4845, removed: []\n",
      "Duplicatez found on track 4850, removed: []\n",
      "Duplicatez found on track 4856, removed: []\n",
      "Duplicatez found on track 4861, removed: []\n",
      "Duplicatez found on track 4864, removed: []\n",
      "Duplicatez found on track 4865, removed: []\n",
      "Duplicatez found on track 4866, removed: []\n",
      "Duplicatez found on track 4877, removed: []\n",
      "Duplicatez found on track 4886, removed: []\n",
      "Duplicatez found on track 4888, removed: []\n",
      "Duplicatez found on track 4889, removed: []\n",
      "Duplicatez found on track 4900, removed: []\n",
      "Duplicatez found on track 4910, removed: []\n",
      "Duplicatez found on track 4922, removed: []\n",
      "Duplicatez found on track 4924, removed: []\n",
      "Duplicatez found on track 4925, removed: []\n",
      "Duplicatez found on track 4927, removed: []\n",
      "Duplicatez found on track 4942, removed: []\n",
      "Duplicatez found on track 4947, removed: []\n",
      "Duplicatez found on track 4948, removed: []\n",
      "Duplicatez found on track 4956, removed: []\n",
      "Duplicatez found on track 4957, removed: []\n",
      "Duplicatez found on track 4960, removed: []\n",
      "Duplicatez found on track 4962, removed: []\n",
      "Duplicatez found on track 4966, removed: []\n",
      "Duplicatez found on track 4968, removed: [107482]\n",
      "track 4968 duplicate z: [107482]\n",
      "Duplicatez found on track 4972, removed: []\n",
      "Duplicatez found on track 4977, removed: []\n",
      "Duplicatez found on track 4978, removed: []\n",
      "Duplicatez found on track 4982, removed: []\n",
      "Duplicatez found on track 4984, removed: []\n",
      "Duplicatez found on track 4989, removed: []\n",
      "Duplicatez found on track 4990, removed: []\n",
      "Duplicatez found on track 4993, removed: []\n",
      "Duplicatez found on track 4998, removed: []\n",
      "Duplicatez found on track 5001, removed: []\n",
      "Duplicatez found on track 5006, removed: []\n",
      "Duplicatez found on track 5008, removed: []\n",
      "Duplicatez found on track 5010, removed: []\n",
      "Duplicatez found on track 5012, removed: []\n",
      "Duplicatez found on track 5014, removed: []\n",
      "Duplicatez found on track 5019, removed: []\n",
      "Duplicatez found on track 5020, removed: []\n",
      "Duplicatez found on track 5021, removed: []\n",
      "Duplicatez found on track 5022, removed: []\n",
      "Duplicatez found on track 5026, removed: []\n",
      "Duplicatez found on track 5027, removed: []\n",
      "Duplicatez found on track 5045, removed: []\n",
      "Duplicatez found on track 5048, removed: []\n",
      "Duplicatez found on track 5065, removed: [5132]\n",
      "track 5065 duplicate z: [5132]\n",
      "Duplicatez found on track 5070, removed: []\n",
      "Duplicatez found on track 5075, removed: []\n",
      "Duplicatez found on track 5076, removed: []\n",
      "Duplicatez found on track 5078, removed: []\n",
      "Duplicatez found on track 5081, removed: []\n",
      "Duplicatez found on track 5082, removed: []\n",
      "Duplicatez found on track 5084, removed: []\n",
      "Duplicatez found on track 5089, removed: []\n",
      "Duplicatez found on track 5092, removed: []\n",
      "Duplicatez found on track 5094, removed: []\n",
      "Duplicatez found on track 5097, removed: [102623]\n",
      "track 5097 duplicate z: [102623]\n",
      "Duplicatez found on track 5104, removed: []\n",
      "Duplicatez found on track 5108, removed: []\n",
      "Duplicatez found on track 5110, removed: []\n",
      "Duplicatez found on track 5116, removed: []\n",
      "Duplicatez found on track 5118, removed: []\n",
      "Duplicatez found on track 5122, removed: []\n",
      "Duplicatez found on track 5124, removed: []\n",
      "Duplicatez found on track 5130, removed: []\n",
      "Duplicatez found on track 5150, removed: []\n",
      "Duplicatez found on track 5151, removed: []\n",
      "Duplicatez found on track 5167, removed: []\n",
      "Duplicatez found on track 5176, removed: []\n",
      "Duplicatez found on track 5181, removed: []\n",
      "Duplicatez found on track 5182, removed: []\n",
      "Duplicatez found on track 5191, removed: []\n",
      "Duplicatez found on track 5201, removed: [63066]\n",
      "track 5201 duplicate z: [63066]\n",
      "Duplicatez found on track 5216, removed: []\n",
      "Duplicatez found on track 5217, removed: []\n",
      "Duplicatez found on track 5219, removed: []\n",
      "Duplicatez found on track 5222, removed: []\n",
      "Duplicatez found on track 5227, removed: []\n",
      "Duplicatez found on track 5228, removed: []\n",
      "Duplicatez found on track 5238, removed: []\n",
      "Duplicatez found on track 5239, removed: []\n",
      "Duplicatez found on track 5241, removed: []\n",
      "Duplicatez found on track 5246, removed: []\n",
      "Duplicatez found on track 5247, removed: []\n",
      "Duplicatez found on track 5248, removed: []\n",
      "Duplicatez found on track 5274, removed: []\n",
      "Duplicatez found on track 5293, removed: []\n",
      "Duplicatez found on track 5294, removed: []\n",
      "Duplicatez found on track 5295, removed: []\n",
      "Duplicatez found on track 5305, removed: []\n",
      "Duplicatez found on track 5309, removed: []\n",
      "Duplicatez found on track 5311, removed: []\n",
      "Duplicatez found on track 5318, removed: []\n",
      "Duplicatez found on track 5324, removed: []\n",
      "Duplicatez found on track 5329, removed: []\n",
      "Duplicatez found on track 5331, removed: []\n",
      "Duplicatez found on track 5341, removed: []\n",
      "Duplicatez found on track 5360, removed: []\n",
      "Duplicatez found on track 5362, removed: []\n",
      "Duplicatez found on track 5367, removed: []\n",
      "Duplicatez found on track 5368, removed: []\n",
      "Duplicatez found on track 5369, removed: []\n",
      "Duplicatez found on track 5371, removed: []\n",
      "Duplicatez found on track 5374, removed: []\n",
      "Duplicatez found on track 5383, removed: []\n",
      "Duplicatez found on track 5389, removed: []\n",
      "Duplicatez found on track 5397, removed: []\n",
      "Duplicatez found on track 5400, removed: [6777]\n",
      "track 5400 duplicate z: [6777]\n",
      "Duplicatez found on track 5405, removed: []\n",
      "Duplicatez found on track 5406, removed: []\n",
      "Duplicatez found on track 5414, removed: []\n",
      "Duplicatez found on track 5418, removed: []\n",
      "Duplicatez found on track 5420, removed: []\n",
      "Duplicatez found on track 5421, removed: []\n",
      "Duplicatez found on track 5422, removed: []\n",
      "Duplicatez found on track 5444, removed: []\n",
      "Duplicatez found on track 5446, removed: []\n",
      "Duplicatez found on track 5449, removed: []\n",
      "Duplicatez found on track 5454, removed: []\n",
      "Duplicatez found on track 5455, removed: []\n",
      "Duplicatez found on track 5464, removed: []\n",
      "Duplicatez found on track 5468, removed: []\n",
      "Duplicatez found on track 5483, removed: []\n",
      "Duplicatez found on track 5486, removed: []\n",
      "Duplicatez found on track 5501, removed: []\n",
      "Duplicatez found on track 5505, removed: []\n",
      "Duplicatez found on track 5509, removed: []\n",
      "Duplicatez found on track 5510, removed: []\n",
      "Duplicatez found on track 5512, removed: []\n",
      "Duplicatez found on track 5519, removed: []\n",
      "Duplicatez found on track 5522, removed: []\n",
      "Duplicatez found on track 5524, removed: []\n",
      "Duplicatez found on track 5526, removed: []\n",
      "Duplicatez found on track 5541, removed: []\n",
      "Duplicatez found on track 5555, removed: []\n",
      "Duplicatez found on track 5565, removed: []\n",
      "Duplicatez found on track 5567, removed: []\n",
      "Duplicatez found on track 5568, removed: []\n",
      "Duplicatez found on track 5573, removed: []\n",
      "Duplicatez found on track 5577, removed: []\n",
      "Duplicatez found on track 5587, removed: []\n",
      "Duplicatez found on track 5594, removed: []\n",
      "Duplicatez found on track 5595, removed: []\n",
      "Duplicatez found on track 5596, removed: []\n",
      "Duplicatez found on track 5600, removed: []\n",
      "Duplicatez found on track 5604, removed: []\n",
      "Duplicatez found on track 5605, removed: []\n",
      "Duplicatez found on track 5619, removed: []\n",
      "Duplicatez found on track 5624, removed: []\n",
      "Duplicatez found on track 5629, removed: []\n",
      "Duplicatez found on track 5631, removed: [66460]\n",
      "track 5631 duplicate z: [66460]\n",
      "Duplicatez found on track 5635, removed: []\n",
      "Duplicatez found on track 5637, removed: []\n",
      "Duplicatez found on track 5639, removed: []\n",
      "Duplicatez found on track 5647, removed: []\n",
      "Duplicatez found on track 5650, removed: []\n",
      "Duplicatez found on track 5654, removed: []\n",
      "Duplicatez found on track 5657, removed: []\n",
      "Duplicatez found on track 5663, removed: []\n",
      "Duplicatez found on track 5674, removed: []\n",
      "Duplicatez found on track 5679, removed: []\n",
      "Duplicatez found on track 5689, removed: []\n",
      "Duplicatez found on track 5696, removed: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicatez found on track 5706, removed: [97462]\n",
      "track 5706 duplicate z: [97462]\n",
      "Duplicatez found on track 5711, removed: []\n",
      "Duplicatez found on track 5715, removed: []\n",
      "Duplicatez found on track 5726, removed: []\n",
      "Duplicatez found on track 5728, removed: [68932]\n",
      "track 5728 duplicate z: [68932]\n",
      "Duplicatez found on track 5732, removed: []\n",
      "Duplicatez found on track 5744, removed: []\n",
      "Duplicatez found on track 5764, removed: []\n",
      "Duplicatez found on track 5766, removed: []\n",
      "Duplicatez found on track 5769, removed: []\n",
      "Duplicatez found on track 5773, removed: []\n",
      "Duplicatez found on track 5779, removed: []\n",
      "Duplicatez found on track 5782, removed: []\n",
      "Duplicatez found on track 5784, removed: []\n",
      "Duplicatez found on track 5786, removed: []\n",
      "Duplicatez found on track 5789, removed: []\n",
      "Duplicatez found on track 5791, removed: []\n",
      "Duplicatez found on track 5823, removed: []\n",
      "Duplicatez found on track 5835, removed: []\n",
      "Duplicatez found on track 5850, removed: []\n",
      "Duplicatez found on track 5858, removed: []\n",
      "Duplicatez found on track 5877, removed: []\n",
      "Duplicatez found on track 5902, removed: []\n",
      "Duplicatez found on track 5906, removed: []\n",
      "Duplicatez found on track 5922, removed: []\n",
      "Duplicatez found on track 5927, removed: []\n",
      "Duplicatez found on track 5929, removed: []\n",
      "Duplicatez found on track 5930, removed: []\n",
      "Duplicatez found on track 5933, removed: []\n",
      "Duplicatez found on track 5944, removed: [101079, 103426]\n",
      "track 5944 duplicate z: [101079, 103426]\n",
      "Duplicatez found on track 5950, removed: []\n",
      "Duplicatez found on track 5952, removed: []\n",
      "Duplicatez found on track 5956, removed: []\n",
      "Duplicatez found on track 5960, removed: []\n",
      "Duplicatez found on track 5962, removed: []\n",
      "Duplicatez found on track 5967, removed: []\n",
      "Duplicatez found on track 5969, removed: []\n",
      "Duplicatez found on track 5973, removed: [74385]\n",
      "track 5973 duplicate z: [74385]\n",
      "Duplicatez found on track 5982, removed: []\n",
      "Duplicatez found on track 5992, removed: []\n",
      "Duplicatez found on track 5995, removed: []\n",
      "Duplicatez found on track 5997, removed: []\n",
      "Duplicatez found on track 6017, removed: []\n",
      "Duplicatez found on track 6036, removed: []\n",
      "Duplicatez found on track 6045, removed: []\n",
      "Duplicatez found on track 6059, removed: []\n",
      "Duplicatez found on track 6066, removed: []\n",
      "Duplicatez found on track 6072, removed: []\n",
      "Duplicatez found on track 6102, removed: []\n",
      "Duplicatez found on track 6107, removed: []\n",
      "Duplicatez found on track 6109, removed: []\n",
      "Duplicatez found on track 6118, removed: []\n",
      "Duplicatez found on track 6126, removed: []\n",
      "Duplicatez found on track 6128, removed: []\n",
      "Duplicatez found on track 6146, removed: []\n",
      "Duplicatez found on track 6158, removed: []\n",
      "Duplicatez found on track 6162, removed: []\n",
      "Duplicatez found on track 6169, removed: []\n",
      "Duplicatez found on track 6179, removed: []\n",
      "Duplicatez found on track 6189, removed: []\n",
      "Duplicatez found on track 6193, removed: []\n",
      "Duplicatez found on track 6194, removed: []\n",
      "Duplicatez found on track 6205, removed: []\n",
      "Duplicatez found on track 6209, removed: []\n",
      "Duplicatez found on track 6211, removed: [55871]\n",
      "track 6211 duplicate z: [55871]\n",
      "Duplicatez found on track 6213, removed: []\n",
      "Duplicatez found on track 6231, removed: []\n",
      "Duplicatez found on track 6236, removed: []\n",
      "Duplicatez found on track 6238, removed: []\n",
      "Duplicatez found on track 6253, removed: []\n",
      "Duplicatez found on track 6254, removed: []\n",
      "Duplicatez found on track 6257, removed: []\n",
      "Duplicatez found on track 6261, removed: []\n",
      "Duplicatez found on track 6263, removed: []\n",
      "Duplicatez found on track 6268, removed: []\n",
      "Duplicatez found on track 6279, removed: []\n",
      "Duplicatez found on track 6292, removed: [13627]\n",
      "track 6292 duplicate z: [13627]\n",
      "Duplicatez found on track 6295, removed: [94426]\n",
      "track 6295 duplicate z: [94426]\n",
      "Duplicatez found on track 6300, removed: []\n",
      "Duplicatez found on track 6303, removed: []\n",
      "Duplicatez found on track 6313, removed: []\n",
      "Duplicatez found on track 6319, removed: []\n",
      "Duplicatez found on track 6322, removed: []\n",
      "Duplicatez found on track 6323, removed: []\n",
      "Duplicatez found on track 6329, removed: [62619]\n",
      "track 6329 duplicate z: [62619]\n",
      "Duplicatez found on track 6335, removed: [232]\n",
      "track 6335 duplicate z: [232]\n",
      "Duplicatez found on track 6338, removed: []\n",
      "Duplicatez found on track 6351, removed: []\n",
      "Duplicatez found on track 6352, removed: []\n",
      "Duplicatez found on track 6354, removed: []\n",
      "Duplicatez found on track 6375, removed: []\n",
      "Duplicatez found on track 6376, removed: []\n",
      "Duplicatez found on track 6383, removed: []\n",
      "Duplicatez found on track 6393, removed: []\n",
      "Duplicatez found on track 6399, removed: []\n",
      "Duplicatez found on track 6406, removed: []\n",
      "Duplicatez found on track 6407, removed: []\n",
      "Duplicatez found on track 6409, removed: []\n",
      "Duplicatez found on track 6411, removed: []\n",
      "Duplicatez found on track 6418, removed: []\n",
      "Duplicatez found on track 6420, removed: []\n",
      "Duplicatez found on track 6460, removed: []\n",
      "Duplicatez found on track 6462, removed: [66534]\n",
      "track 6462 duplicate z: [66534]\n",
      "Duplicatez found on track 6480, removed: []\n",
      "Duplicatez found on track 6489, removed: []\n",
      "Duplicatez found on track 6500, removed: []\n",
      "Duplicatez found on track 6509, removed: []\n",
      "Duplicatez found on track 6510, removed: []\n",
      "Duplicatez found on track 6519, removed: []\n",
      "Duplicatez found on track 6520, removed: []\n",
      "Duplicatez found on track 6547, removed: []\n",
      "Duplicatez found on track 6550, removed: []\n",
      "Duplicatez found on track 6557, removed: []\n",
      "Duplicatez found on track 6558, removed: []\n",
      "Duplicatez found on track 6568, removed: [67056]\n",
      "track 6568 duplicate z: [67056]\n",
      "Duplicatez found on track 6569, removed: []\n",
      "Duplicatez found on track 6579, removed: []\n",
      "Duplicatez found on track 6588, removed: []\n",
      "Duplicatez found on track 6595, removed: []\n",
      "Duplicatez found on track 6609, removed: [51634]\n",
      "track 6609 duplicate z: [51634]\n",
      "Duplicatez found on track 6610, removed: []\n",
      "Duplicatez found on track 6615, removed: []\n",
      "Duplicatez found on track 6620, removed: []\n",
      "Duplicatez found on track 6628, removed: []\n",
      "Duplicatez found on track 6629, removed: []\n",
      "Duplicatez found on track 6637, removed: []\n",
      "Duplicatez found on track 6642, removed: []\n",
      "Duplicatez found on track 6644, removed: []\n",
      "Duplicatez found on track 6648, removed: []\n",
      "Duplicatez found on track 6654, removed: []\n",
      "Duplicatez found on track 6656, removed: [100893]\n",
      "track 6656 duplicate z: [100893]\n",
      "Duplicatez found on track 6658, removed: []\n",
      "Duplicatez found on track 6667, removed: [58688]\n",
      "track 6667 duplicate z: [58688]\n",
      "Duplicatez found on track 6669, removed: []\n",
      "Duplicatez found on track 6674, removed: []\n",
      "Duplicatez found on track 6675, removed: []\n",
      "Duplicatez found on track 6698, removed: []\n",
      "Duplicatez found on track 6700, removed: []\n",
      "Duplicatez found on track 6701, removed: []\n",
      "Duplicatez found on track 6702, removed: []\n",
      "Duplicatez found on track 6706, removed: [12550]\n",
      "track 6706 duplicate z: [12550]\n",
      "Duplicatez found on track 6707, removed: []\n",
      "Duplicatez found on track 6716, removed: []\n",
      "Duplicatez found on track 6737, removed: []\n",
      "Duplicatez found on track 6748, removed: [6746]\n",
      "track 6748 duplicate z: [6746]\n",
      "Duplicatez found on track 6754, removed: []\n",
      "Duplicatez found on track 6757, removed: []\n",
      "Duplicatez found on track 6760, removed: [60647]\n",
      "track 6760 duplicate z: [60647]\n",
      "Duplicatez found on track 6761, removed: [449]\n",
      "track 6761 duplicate z: [449]\n",
      "Duplicatez found on track 6778, removed: []\n",
      "Duplicatez found on track 6801, removed: []\n",
      "Duplicatez found on track 6809, removed: []\n",
      "Duplicatez found on track 6819, removed: []\n",
      "Duplicatez found on track 6820, removed: []\n",
      "Duplicatez found on track 6851, removed: []\n",
      "Duplicatez found on track 6877, removed: []\n",
      "Duplicatez found on track 6879, removed: []\n",
      "Duplicatez found on track 6880, removed: []\n",
      "Duplicatez found on track 6883, removed: []\n",
      "Duplicatez found on track 6885, removed: []\n",
      "Duplicatez found on track 6887, removed: []\n",
      "Duplicatez found on track 6888, removed: []\n",
      "Duplicatez found on track 6927, removed: []\n",
      "Duplicatez found on track 6933, removed: []\n",
      "Duplicatez found on track 6936, removed: []\n",
      "Duplicatez found on track 6947, removed: []\n",
      "Duplicatez found on track 6978, removed: []\n",
      "Duplicatez found on track 6985, removed: []\n",
      "Duplicatez found on track 6997, removed: []\n",
      "Duplicatez found on track 7010, removed: []\n",
      "Duplicatez found on track 7020, removed: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicatez found on track 7031, removed: []\n",
      "Duplicatez found on track 7032, removed: []\n",
      "Duplicatez found on track 7037, removed: []\n",
      "Duplicatez found on track 7044, removed: []\n",
      "Duplicatez found on track 7045, removed: []\n",
      "Duplicatez found on track 7051, removed: []\n",
      "Duplicatez found on track 7064, removed: []\n",
      "Duplicatez found on track 7070, removed: []\n",
      "Duplicatez found on track 7079, removed: [115169]\n",
      "track 7079 duplicate z: [115169]\n",
      "Duplicatez found on track 7080, removed: []\n",
      "Duplicatez found on track 7084, removed: [110407, 115286]\n",
      "track 7084 duplicate z: [110407, 115286]\n",
      "Total removed due to bad volumes: 0\n",
      "Total removed due to bad dimensions: 0\n",
      "Total removed due to duplicate zs: 65\n",
      "Total removed due to bad slopes: 962\n",
      "Total removed singleton hits: 49\n",
      "Initial score after outlier removal for event 1000: 0.56966146\n"
     ]
    }
   ],
   "source": [
    "labels_h3 = np.copy(labels_helix)\n",
    "\n",
    "#one_submission = create_one_event_submission(1000, hits, labels_h3)\n",
    "#score = score_event(truth, one_submission)\n",
    "#print(\"Initial score before outlier removal for event %d: %.8f\" % (1000, score))\n",
    "\n",
    "labels_h3 = remove_outliers(labels_h3, hits)\n",
    "one_submission = create_one_event_submission(1000, hits, labels_h3)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Initial score after outlier removal for event %d: %.8f\" % (1000, score))\n",
    "\n",
    "# before score: 0.57485796\n",
    "# after old outlier removal: 0.57471551\n",
    "# after slope removal: 0.56906582 (1006 removed due to bad slopes)\n",
    "# after slope removal + threshold: 0.56966146 (962 removed due to bad slopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([48926, 48987, 52069, 55050, 57628, 59825, 59828]),)\n",
      "       hit_id         particle_id          tx       ty      tz       tpx  \\\n",
      "48987   48988  941257957117526016  -58.671299  7.62641   597.5 -0.174186   \n",
      "48926   48927  941257957117526016  -59.121101  7.63531   602.0 -0.173795   \n",
      "52069   52070  941257957117526016  -68.662102  7.68017   697.5 -0.173923   \n",
      "55050   55051  941257957117526016  -80.685402  7.08299   817.5 -0.173570   \n",
      "57628   57629  941257957117526016  -94.600098  5.84833   957.5 -0.171999   \n",
      "59825   59826  941257957117526016 -108.307999  3.99822  1098.0 -0.168786   \n",
      "59828   59829  630505047343497216 -108.446999  3.92813  1098.0 -0.750007   \n",
      "\n",
      "            tpy      tpz    weight  \n",
      "48987  0.003858  1.73910  0.000015  \n",
      "48926  0.003710  1.73855  0.000012  \n",
      "52069 -0.002642  1.73850  0.000010  \n",
      "55050 -0.011636  1.73788  0.000008  \n",
      "57628 -0.020179  1.73737  0.000006  \n",
      "59825 -0.027291  1.73704  0.000004  \n",
      "59828 -0.003918  7.63164  0.000006  \n",
      "       hit_id           x        y       z  volume_id  layer_id  module_id  \\\n",
      "48987   48988  -58.679199  7.61525   597.5          9         2        106   \n",
      "48926   48927  -59.128502  7.63935   602.0          9         2        104   \n",
      "52069   52070  -68.674202  7.68681   697.5          9         4        106   \n",
      "55050   55051  -80.684097  7.07722   817.5          9         6        106   \n",
      "57628   57629  -94.603302  5.86557   957.5          9         8        106   \n",
      "59825   59826 -108.302002  3.98100  1098.0          9        10        108   \n",
      "59828   59829 -108.459000  3.93819  1098.0          9        10        108   \n",
      "\n",
      "                r        a0   z_abs  \n",
      "48987   59.171280  3.012536   597.5  \n",
      "48926   59.619957  3.013105   602.0  \n",
      "52069   69.103058  3.030125   697.5  \n",
      "55050   80.993889  3.054101   817.5  \n",
      "57628   94.784973  3.079670   957.5  \n",
      "59825  108.375145  3.104851  1098.0  \n",
      "59828  108.530479  3.105298  1098.0  \n"
     ]
    }
   ],
   "source": [
    "# Play around with the track number to find a track that contains multiple particles\n",
    "hhh_ix = np.where(labels_helix == 24)\n",
    "print(hhh_ix)\n",
    "hhh_t = truth.loc[hhh_ix]\n",
    "hhh_t = hhh_t.sort_values('tz')\n",
    "print(hhh_t)\n",
    "\n",
    "# Now add r/a0 as columns to the hits, and see if there's a way to detect\n",
    "# which hits belong to the track, and which should be considered outliers.\n",
    "hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "hits['a0'] = np.arctan2(hits.y,hits.x)\n",
    "hhh_h = hits.loc[hhh_ix]\n",
    "\n",
    "\n",
    "#hhh_h = hhh_h[(hhh_h.hit_id != 15459) & (hhh_h.hit_id != 16190) & (hhh_h.hit_id != 16155) ]\n",
    "\n",
    "hhh_h = hhh_h.sort_values('z')\n",
    "print(hhh_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ones that we had trouble with for slope outlier detection:\n",
    "#x 11/6 hits - track 1373 outliers: [24207, 39572, 39574, 48338, 78104, 78108] should remove [24207, 39572, 39574]\n",
    "#y 17/7 hits - track 1644 outliers: [21896, 29838, 36816, 89394, 89395, 111614, 95550] should remove: [89393, 89395]\n",
    "# 19/7 hits - track 1698 outliers: [80388, 73708, 42159, 73717, 86837, 86838, 41657] should remove: [86838, 86387, 80388, 73708, 42159]\n",
    "# 14/5 hits - track 1902 outliers: [23136, 37256, 116299, 30509, 116560] should remove: [116561]\n",
    "#x 15/9 hits - track 1963 outliers: [30432, 37195, 43502, 23025, 75475, 111219, 111222, 37180, 30367] should remove: [111222]\n",
    "#y 13/5 hits - track 2390 outliers: [76962, 84260, 84264, 45130, 45486] should remove: [45128, 45130, 45486, 76962, 84260, 84264, 91275]\n",
    "# --> actually, really good! this was a very complicated case.\n",
    "#x 20/11 hits - track 2445 outliers: [70818, 65315, 69259, 67852, 67858, 69236, 105080, 70776, 66589, 69246, 69247] should remove: [67852, 69246, 69236, 69259, 70776, 71464]\n",
    "# 16/6 hits - track 2522 outliers: [44685, 44686, 83245, 76114, 21534, 76126] should remove:  [21534, 21672, 44209, 44686, 76114, 83245, 90582]\n",
    "# 15/5 hits - track 3417 outliers: [28458, 81457, 81464, 80985, 35775] should remove: [81457, 80985]\n",
    "#x 15/8 hits - track 3427 outliers: [101154, 39335, 98506, 120427, 24496, 119921, 31986, 84925] should remove: []\n",
    "#y 15/6 hits - track 4225 outliers: [119013, 118535, 31339, 45003, 76853, 38453] should remove: [118535]\n",
    "#y 13/5 hits - track 4318 outliers: [84482, 45732, 77481, 77200, 77203] should remove: [77200, 77481, 84482, 84707, 119929]\n",
    "# 18/5 hits - track 5047 outliers: [44323, 44330, 44331, 30926, 37591] should remove: [22142, 30926, 37591, 44319, 44331, 44323, 44330]\n",
    "# 13/5 hits - track 5101 outliers: [117442, 22595, 22833, 111923, 95867] should remove   [117444]\n",
    "#xx 13/6 hits - track 5420 outliers: [86917, 92518, 86918, 92523, 108468, 108469] should remove [86918]\n",
    "#y 23/10 hits - track 5422 outliers: [94144, 81504, 88577, 94531, 74492, 88165, 43052, 110419, 21500, 110718] should remove [21650, 21512, 43052, 43060, 74492, 81504, 88165, 110718, 88577, 94531]\n",
    "# 17/5 hits - track 5826 outliers: [43176, 89005, 94905, 88988, 88991] should remove [43170, 43176, 89005, 88991, 88988, 94905, 94878, 111049]\n",
    "# 14/5 hits - track 6244 outliers: [34257, 34706, 34708, 19508, 34264] should remove [79229, 72675, 34708, 34264, 34706, 34697, 19718, 19508]\n",
    "#y 12/5 hits - track 6409 outliers: [35969, 73987, 28709, 73990, 42508] should remove [109342, 109340, 93542, 73987]\n",
    "# 15/6 hits - track 6740 outliers: [38990, 32046, 39374, 23633, 23666, 38997] should remove [23633, 23666, 32046, 38997, 39374]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO pi, -pi discontinuity \n",
    "def remove_track_outliers_slope(track, labels, hits, debug=False):\n",
    "    hhh_ix = np.where(labels == track)\n",
    "    hhh_h = hits.loc[hhh_ix].sort_values('z')\n",
    "    \n",
    "    slopes_backward = []\n",
    "    slopes_forward = []\n",
    "\n",
    "\n",
    "    num_hits = len(hhh_h)\n",
    "    if debug: print('backward:')\n",
    "    for i in np.arange(num_hits-1,0,-1):\n",
    "        a0 =  hhh_h.a0.values[i]\n",
    "        a1 =  hhh_h.a0.values[i-1]\n",
    "        r0 =  hhh_h.r.values[i]\n",
    "        r1 =  hhh_h.r.values[i-1]\n",
    "        if r0 == r1:\n",
    "            r0 = r0 + 1e-8\n",
    "        slope = (a0-a1)/(r0-r1) \n",
    "        slopes_backward.append(slope)\n",
    "        if debug: print(hhh_h.hit_id.values[i], slope, a0)\n",
    "        if i == 1:\n",
    "            a0 = hhh_h.a0.values[0]\n",
    "            a1 = hhh_h.a0.values[num_hits-1]\n",
    "            r0 =  hhh_h.r.values[0]\n",
    "            r1 =  hhh_h.r.values[num_hits-1]\n",
    "            if r0 == r1:\n",
    "                r0 = r0 + 1e-8\n",
    "            slope = (a0-a1)/(r0-r1)\n",
    "            slopes_backward.append(slope)\n",
    "            if debug: print(hhh_h.hit_id.values[0], slope, a1)\n",
    "\n",
    "    if debug: print('forward:')\n",
    "    for i in np.arange(0,num_hits-1,1):\n",
    "        a0 =  hhh_h.a0.values[i]\n",
    "        a1 =  hhh_h.a0.values[i+1]\n",
    "        r0 =  hhh_h.r.values[i]\n",
    "        r1 =  hhh_h.r.values[i+1]\n",
    "        if r0 == r1:\n",
    "            r1 = r1 + 1e-8\n",
    "        slope = (a1-a0)/(r1-r0) \n",
    "        slopes_forward.append(slope)\n",
    "        if debug: print(hhh_h.hit_id.values[i], slope, a0)\n",
    "\n",
    "        if i == num_hits-2:\n",
    "            a0 = hhh_h.a0.values[0]\n",
    "            a1 = hhh_h.a0.values[num_hits-1]\n",
    "            r0 =  hhh_h.r.values[0]\n",
    "            r1 =  hhh_h.r.values[num_hits-1]\n",
    "            if r0 == r1:\n",
    "                r1 = r1 + 1e-8\n",
    "            slope = (a1-a0)/(r1-r0) \n",
    "            slopes_forward.append(slope)\n",
    "            if debug: print(hhh_h.hit_id.values[num_hits-1], slope, a0)\n",
    "\n",
    "    slopes_backward = np.asarray(slopes_backward)\n",
    "    slopes_backward = np.reshape(slopes_backward, (-1, 1))\n",
    "    slopes_forward = np.asarray(slopes_forward)\n",
    "    slopes_forward = np.reshape(slopes_forward, (-1, 1))\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    X_back = ss.fit_transform(slopes_backward)\n",
    "    X_for = ss.fit_transform(slopes_forward)\n",
    "\n",
    "    cl = DBSCAN(eps=0.0033, min_samples=1)\n",
    "    outlier_labels_backward = cl.fit_predict(X_back)\n",
    "    outlier_labels_forward = cl.fit_predict(X_for)\n",
    "\n",
    "    if debug: print(outlier_labels_backward)\n",
    "    if debug: print(outlier_labels_forward)\n",
    "\n",
    "    track_counts = coll.Counter(outlier_labels_backward).most_common(1)\n",
    "    most_common_id = track_counts[0][0]\n",
    "    most_common_count = track_counts[0][1]\n",
    "\n",
    "    outlier_indices_backward = []\n",
    "    if most_common_count > 1 and len(np.unique(outlier_labels_forward)) < num_hits/2:\n",
    "        for i in np.arange(num_hits-1,-1,-1):\n",
    "            if outlier_labels_backward[i] != most_common_id:\n",
    "                if debug: print(hhh_h.index.values[num_hits-1-i])\n",
    "                outlier_indices_backward.append(hhh_h.index.values[num_hits-1-i])\n",
    "\n",
    "    track_counts = coll.Counter(outlier_labels_forward).most_common(1)\n",
    "    most_common_id = track_counts[0][0]\n",
    "    most_common_count = track_counts[0][1]\n",
    "\n",
    "\n",
    "    outlier_indices_forward = []\n",
    "    if most_common_count > 1 and len(np.unique(outlier_labels_forward)) < num_hits/2:\n",
    "        for i in np.arange(0,num_hits-1,1):\n",
    "            if outlier_labels_forward[i] != most_common_id:\n",
    "                if debug: print(hhh_h.index.values[i])\n",
    "                outlier_indices_forward.append(hhh_h.index.values[i])\n",
    "\n",
    "\n",
    "    outlier_candidates = list(set(outlier_indices_backward).intersection(outlier_indices_forward))\n",
    "    final_outliers = []\n",
    "\n",
    "    if debug: print('before removal:' + str(outlier_candidates))\n",
    "\n",
    "    for i in range(len(outlier_candidates)):\n",
    "        candidate = hhh_h.loc[outlier_candidates[i]]\n",
    "        found = False\n",
    "        for index, row in hhh_h.iterrows():\n",
    "            if np.absolute(candidate.z-row.z) == 0.5 and candidate.volume_id == row.volume_id \\\n",
    "            and candidate.layer_id == row.layer_id and candidate.module_id != row.module_id:\n",
    "                # true hits\n",
    "                if debug: print('true hit' + str(outlier_candidates[i]))\n",
    "                found = True\n",
    "        if found is False:\n",
    "            final_outliers.append(outlier_candidates[i])\n",
    "\n",
    "    if debug: print('new loutliers:' + str(final_outliers))\n",
    "\n",
    "    return final_outliers\n",
    "\n",
    "\n",
    "def remove_outliers_slope(labels, hits):\n",
    "    tracks = np.unique(labels)\n",
    "    count_rem_volume = 0\n",
    "    count_rem_dimension = 0\n",
    "    count_duplicatez = 0\n",
    "    count_singletons = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 4:\n",
    "            outliers = remove_track_outliers_slope(track, labels, hits)\n",
    "            if len(outliers) > 0:\n",
    "                do_something = True\n",
    "                # filter out the outliers\n",
    "            \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope outliers: [17674]\n"
     ]
    }
   ],
   "source": [
    "labels_h3 = np.copy(labels_helix)\n",
    "outliers = remove_track_outliers_slope(14, labels_h3, hits)\n",
    "print('Slope outliers: ' + str(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Removing helix outliers')\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "labels_h3 = remove_outliers_slope(labels_h3, hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 6004,  8657, 11571, 14572, 14632, 17673, 17674, 17726, 25806, 26053]),)\n",
      "       hit_id         particle_id         tx          ty          tz  \\\n",
      "6004     6005  941261186932932608 -10.306700 -166.899002 -962.000000   \n",
      "8657     8658  941261186932932608 -15.997800 -141.957993 -818.000000   \n",
      "11571   11572  941261186932932608 -18.980600 -120.927002 -698.000000   \n",
      "14572   14573  941261186932932608 -19.748899 -103.984001 -602.000000   \n",
      "14632   14633  941261186932932608 -19.760900 -103.278999 -598.000000   \n",
      "25806   25807  112599405269360642 -18.183500  -69.573502 -441.696991   \n",
      "26053   26054  941261186932932608 -18.205200  -69.069000 -403.363007   \n",
      "17726   17727  941261186932932608 -10.917500  -31.369400 -182.873001   \n",
      "17674   17675  153154479326232576 -10.601500  -30.340200 -178.082001   \n",
      "17673   17674  941261186932932608 -10.627500  -30.327600 -176.707993   \n",
      "\n",
      "            tpx       tpy       tpz    weight  \n",
      "6004   0.043563 -0.146399 -0.870692  0.000003  \n",
      "8657   0.027812 -0.151889 -0.871724  0.000003  \n",
      "11571  0.012568 -0.153871 -0.872332  0.000005  \n",
      "14572  0.001914 -0.154090 -0.872416  0.000006  \n",
      "14632  0.003316 -0.153626 -0.873063  0.000007  \n",
      "25806 -0.107853 -0.248958 -0.113892  0.000008  \n",
      "26053 -0.017483 -0.152549 -0.879572  0.000009  \n",
      "17726 -0.040665 -0.148164 -0.880157  0.000010  \n",
      "17674 -0.290885 -0.461139 -3.068830  0.000010  \n",
      "17673 -0.041702 -0.148624 -0.880071  0.000012  \n",
      "       hit_id          x           y           z  volume_id  layer_id  \\\n",
      "6004     6005 -10.294000 -166.899994 -962.000000          7         8   \n",
      "8657     8658 -15.990900 -141.959000 -818.000000          7        10   \n",
      "11571   11572 -18.963499 -120.926003 -698.000000          7        12   \n",
      "14572   14573 -19.756901 -103.983002 -602.000000          7        14   \n",
      "14632   14633 -19.774401 -103.276001 -598.000000          7        14   \n",
      "25806   25807 -18.180901  -69.573898 -441.695007          8         4   \n",
      "26053   26054 -18.216299  -69.067299 -403.368988          8         4   \n",
      "17726   17727 -10.917900  -31.369400 -182.869995          8         2   \n",
      "17674   17675 -10.601700  -30.340099 -178.070007          8         2   \n",
      "17673   17674 -10.626400  -30.328199 -176.697998          8         2   \n",
      "\n",
      "       module_id           r        a0  \n",
      "6004          27  167.217148 -1.632396  \n",
      "8657          26  142.856796 -1.682968  \n",
      "11571         26  122.403885 -1.726349  \n",
      "14572         24  105.843277 -1.758559  \n",
      "14632         26  105.152077 -1.759978  \n",
      "25806          7   71.910172 -1.826398  \n",
      "26053         39   71.429161 -1.828671  \n",
      "17726         68   33.215054 -1.905727  \n",
      "17674         67   32.139034 -1.906962  \n",
      "17673         67   32.135963 -1.907810  \n"
     ]
    }
   ],
   "source": [
    "# Play around with the track number to find a track that contains multiple particles\n",
    "hhh_ix = np.where(labels_helix == 14)\n",
    "print(hhh_ix)\n",
    "hhh_t = truth.loc[hhh_ix]\n",
    "hhh_t = hhh_t.sort_values('tz')\n",
    "print(hhh_t)\n",
    "\n",
    "# Now add r/a0 as columns to the hits, and see if there's a way to detect\n",
    "# which hits belong to the track, and which should be considered outliers.\n",
    "hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "hits['a0'] = np.arctan2(hits.y,hits.x)\n",
    "hhh_h = hits.loc[hhh_ix]\n",
    "\n",
    "\n",
    "#hhh_h = hhh_h[(hhh_h.hit_id != 15459) & (hhh_h.hit_id != 16190) & (hhh_h.hit_id != 16155) ]\n",
    "\n",
    "hhh_h = hhh_h.sort_values('z')\n",
    "print(hhh_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Removing cone outliers')\n",
    "labels_c3 = np.copy(labels_cone)\n",
    "labels_c3 = remove_outliers_slope(labels_c3, hits)\n",
    "\n",
    "print('Removing helix outliers')\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "labels_h3 = remove_outliers_slope(labels_h3, hits)\n",
    "\n",
    "print('Merging tracks')\n",
    "tmp_labels = merge.heuristic_merge_tracks(labels_h3, labels_c3)\n",
    "one_submission = create_one_event_submission(1000, hits, tmp_labels)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merged score for event %d: %.8f\" % (1000, score))\n",
    "\n",
    "#Merged score for event 1000: 0.58701361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_c2 = np.copy(labels_cone)\n",
    "labels_c2 = remove_outliers(labels_c2, hits)\n",
    "\n",
    "labels_simple_merge = merge.merge_tracks(labels_helix, labels_c2)\n",
    "one_submission = create_one_event_submission(1000, hits, labels_simple_merge)\n",
    "#one_submission = create_one_event_submission(1000, hits, labels_c2)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Outlier removal score for cone event %d: %.8f\" % (1000, score))\n",
    "# Orig Score no removals: 0.26152679\n",
    "# With outlier removal:\n",
    "# volume: 1231\n",
    "# bad dim: 6733\n",
    "# score: 0.18735180\n",
    "# With volume removal: 3520, score: 0.23268937\n",
    "# With volume removal, treat -ve+8--> +ve: 2841, score: 0.23394451\n",
    "# With ignore of 8, otherwise full checks: 1056, score: 0.25499520\n",
    "# With ignore of 8, light checks: 1, score: 0.26152679\n",
    "# Bad volume removal: 1, Bad dims: 222, score: 0.25587569, merged: 0.48291748\n",
    "# Bad volume removal: 1, Bad dims: 220, duplicatez: 1675, score: 0.25657711, merged: 0.48742414\n",
    "# HELIX: Bad vol: 15, score: 0.51204521\n",
    "# HELIX: Bad vol: 15, duplicatez: 399, score: 0.51065321\n",
    "# HELIX: Bad vol: 15, duplicatez: 399, bad dim: 286, score: 0.49635631\n",
    "# HELIX: Bad vol: 15, duplicatez: 399, bad dim: 1518, score: 0.50364853\n",
    "# CONE: Bad vol: 0, duplicatez: 1675, bad dim: 1239, score: 0.25871803, merged: 0.48569633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_labels = merge.heuristic_merge_tracks(labels_helix, labels_c2)\n",
    "one_submission = create_one_event_submission(1000, hits, tmp_labels)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"cone score for event %d: %.8f\" % (1000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_h2 = np.copy(labels_helix)\n",
    "labels_h2 = remove_outliers(labels_h2, hits)#, aggressive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_to_remove = 157\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "xxx_ix = np.where(labels_h3 == track_to_remove)[0]\n",
    "print('xxx_ix: ' + str(xxx_ix))\n",
    "(labels_h3x, _, _, _) = remove_track_outliers(track_to_remove, labels_h3, hits, False)#, True)\n",
    "xxx_ixx = np.where(labels_h3x == track_to_remove)[0]\n",
    "print('xxx_ixx: ' + str(xxx_ixx))\n",
    "#(labels_h3, cx, cy, cz) = remove_track_outliers(track_to_remove, labels_c3, hits)\n",
    "#labels_c3x = np.copy(labels_c3)\n",
    "#one_submission = create_one_event_submission(1000, hits, labels_c3)\n",
    "#score = score_event(truth, one_submission)\n",
    "#print(\"Outlier removal score for cone event %d: %.8f\" % (1000, score))\n",
    "# rem 0,0, score 0.26152679\n",
    "#xxx_ix = np.where(labels_c3 == track_to_remove)[0]\n",
    "#print('xxx_ix: ' + str(xxx_ix))\n",
    "xxx_df = truth.loc[xxx_ix]\n",
    "xxx_df = xxx_df.sort_values('tz')\n",
    "xxx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_to_remove = 157\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "xxx_ix = np.where(labels_h3 == track_to_remove)[0]\n",
    "print('xxx_ix: ' + str(xxx_ix))\n",
    "(labels_h3x, _, _, _) = remove_track_outliers(track_to_remove, labels_h3, hits, False)#, True)\n",
    "xxx_ixx = np.where(labels_h3x == track_to_remove)[0]\n",
    "print('xxx_ixx: ' + str(xxx_ixx))\n",
    "#(labels_h3, cx, cy, cz) = remove_track_outliers(track_to_remove, labels_c3, hits)\n",
    "#labels_c3x = np.copy(labels_c3)\n",
    "#one_submission = create_one_event_submission(1000, hits, labels_c3)\n",
    "#score = score_event(truth, one_submission)\n",
    "#print(\"Outlier removal score for cone event %d: %.8f\" % (1000, score))\n",
    "# rem 0,0, score 0.26152679\n",
    "#xxx_ix = np.where(labels_c3 == track_to_remove)[0]\n",
    "#print('xxx_ix: ' + str(xxx_ix))\n",
    "xxx_df = hits.loc[xxx_ix]\n",
    "xxx_df = xxx_df.sort_values('z')\n",
    "xxx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr2_df = truth.loc[truth['particle_id'] == 306250134780379136]\n",
    "tr2_df = tr2_df.sort_values('tz')\n",
    "tr2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr2_df = truth.loc[truth['particle_id'] == 49542619558051840]\n",
    "tr2_df = tr2_df.sort_values('tz')\n",
    "tr2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helidx = np.where(labels_helix==7095)[0]\n",
    "print(helidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_submission = create_one_event_submission(1000, hits, labels_cone)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Outlier removal score for cone event %d: %.8f\" % (1000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_c2 = np.copy(labels_cone)\n",
    "#trks2 = np.unique(labels_c2)\n",
    "#count_rem_volume = 0\n",
    "#count_rem_dimension = 0\n",
    "#for trk2 in trks2:\n",
    "#    if trk2 == 0:\n",
    "#        continue\n",
    "#    trk2_hits = np.where(labels_c2 == trk2)[0]\n",
    "#    if len(trk2_hits) > 3:\n",
    "#        (labels_c2, c1, c2) = remove_track_outliers(trk2, labels_c2, hits)\n",
    "#        count_rem_volume = count_rem_volume + c1\n",
    "#        count_rem_dimension = count_rem_dimension + c2\n",
    "\n",
    "#print('Total removed due to bad volumes: ' + str(count_rem_volume))\n",
    "#print('Total removed due to bad dimensions: ' + str(count_rem_dimension))\n",
    "\n",
    "track_to_remove = 63542\n",
    "labels_c3 = np.copy(labels_cone)\n",
    "xxx_ix = np.where(labels_c3 == track_to_remove)[0]\n",
    "print('xxx_ix: ' + str(xxx_ix))\n",
    "(labels_c3, cx, cy, cz) = remove_track_outliers(track_to_remove, labels_c3, hits)\n",
    "labels_c3x = np.copy(labels_c3)\n",
    "one_submission = create_one_event_submission(1000, hits, labels_c3)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Outlier removal score for cone event %d: %.8f\" % (1000, score))\n",
    "# rem 0,0, score 0.26152679\n",
    "xxx_ix = np.where(labels_c3 == track_to_remove)[0]\n",
    "print('xxx_ix: ' + str(xxx_ix))\n",
    "xxx_df = hits.loc[xxx_ix]\n",
    "xxx_df = xxx_df.sort_values('z')\n",
    "# Bad indexes: 59855, 61697\n",
    "print(xxx_df)\n",
    "xxx_ix = np.where(labels_cone == 63542)\n",
    "xxx_df = hits.loc[xxx_ix]\n",
    "xxx_df = xxx_df.sort_values('z')\n",
    "# indexes in question: ???\n",
    "xxx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_helix = merge.renumber_labels(labels_helix)\n",
    "max_track = np.amax(labels_helix)\n",
    "labels_cone[labels_cone != 0] = labels_cone[labels_cone != 0] + max_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsx = np.copy(labels_cone)\n",
    "trackx = 63949\n",
    "outx = find_dimension_outlier(trackx, labelsx, hits, 'y')\n",
    "print('outx: ' + str(outx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx_ix = np.where(labels_cone == 63542)\n",
    "xxx_df = hits.loc[xxx_ix]\n",
    "xxx_df = xxx_df.sort_values('z')\n",
    "# indexes in question: ???\n",
    "xxx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = truth.loc[xxx_ix]\n",
    "tr_df = tr_df.sort_values('tz')\n",
    "# BAD: 59855, 61697\n",
    "# sorted z, see steps for 'y', ensure they are consistent (good for 59855, ok for 61697)\n",
    "# --> if most in same direction except one/two outliers, and outlier magnitude is large....\n",
    "all_y1 = tr_df.ty.values\n",
    "all_y = np.diff(all_y1)\n",
    "print('mean: ' + str(all_y.mean()))\n",
    "print('max: ' + str(all_y.max()))\n",
    "print('min: ' + str(all_y.min()))\n",
    "#[49193, 59886, 61710]\n",
    "tr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr2_df = truth.loc[truth['particle_id'] == 166642325903114240]\n",
    "tr2_df = tr2_df.sort_values('tz')\n",
    "tr2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track 150 outlier dimension y: [100706]\n",
    "trk large -: 160, large1: -1.592, large2: -61.533\n",
    "track 160 outlier dimension y: [41261]\n",
    "track 169 outlier dimension y: [17867]\n",
    "track 181 outlier dimension y: [103556]\n",
    "track 188 outlier dimension y: [661]\n",
    "track 194 outlier dimension y: [40343]\n",
    "trk large -: 208, large1: -1.2258, large2: -30.6159\n",
    "track 208 outlier dimension y: [23693]\n",
    "trk large -: 223, large1: -1.3697, large2: -36.1324\n",
    "track 223 outlier dimension y: [29081]\n",
    "trk large +: 263, large1: 38.0123, large2: 1.7837\n",
    "track 263 outlier dimension y: [19765]\n",
    "trk large +: 281, large1: 102.727, large2: 3.78752\n",
    "track 281 outlier dimension y: [100657]\n",
    "track 286 outlier dimension y: [62478]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhh_ix = np.where(labels_helix == 5549)\n",
    "hhh_df = truth.loc[hhh_ix]\n",
    "hhh_df = hhh_df.sort_values('tz')\n",
    "# indexes in question: ???\n",
    "hhh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = truth.loc[hhh_ix]\n",
    "tr_df = tr_df.sort_values('tz')\n",
    "# BAD: 59855, 61697\n",
    "# sorted z, see steps for 'y', ensure they are consistent (good for 59855, ok for 61697)\n",
    "# --> if most in same direction except one/two outliers, and outlier magnitude is large....\n",
    "all_y1 = tr_df.ty.values\n",
    "all_y = np.diff(all_y1)\n",
    "print('mean: ' + str(all_y.mean()))\n",
    "print('max: ' + str(all_y.max()))\n",
    "print('min: ' + str(all_y.min()))\n",
    "#[48366, 48517, 51460, 54360, 54471, 57091, 57133]\n",
    "#[6596, 6539, 4232, 4180]\n",
    "tr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_x = np.array([\n",
    "    [\n",
    "        [-12.166300, -156.817993, 962.000],\n",
    "        [20.349701, -242.162003, 1498.500],\n",
    "        [48.221401, -284.333008, 1795.500],\n",
    "        [48.529099, -284.743988, 1798.500],\n",
    "        [122.458000, -404.139008, 2554.500]\n",
    "    ],\n",
    "    [\n",
    "        [-7.17245, -44.973099, -1098.0],\n",
    "        [-6.67366, -39.278999, -962.5],\n",
    "        [-6.65474, -39.091202, -958.0],\n",
    "        [-6.01885, -33.464001, -822.5],\n",
    "        [-5.99555, -33.277901, -818.0]\n",
    "    ],\n",
    "    [\n",
    "        [-6.829120, 30.917900, 40.626202],\n",
    "        [-28.525200, 169.742004, 212.766998],\n",
    "        [-34.125198, 258.694000, 325.647003],\n",
    "        [-31.138201, 361.954987, 453.306000],\n",
    "        [-12.355800, 503.367004, 629.344971]\n",
    "    ]\n",
    "])\n",
    "\n",
    "track_y = np.array([\n",
    "    [1, 1, 1, 1, 0],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [0, 0, 1, 1, 1]\n",
    "])\n",
    "print(track_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = np.unique(labels_h3)\n",
    "track_x = np.zeros((len(tracks), 10, 3))\n",
    "track_y = np.zeros((len(tracks), 10, 1))\n",
    "max_xval = np.amax(hits.x.values)\n",
    "print(max_xval)\n",
    "max_yval = np.amax(hits.y.values)\n",
    "print(max_yval)\n",
    "max_zval = np.amax(hits.z.values)\n",
    "print(max_zval)\n",
    "for idx, track in enumerate(tracks):\n",
    "    if track == 0:\n",
    "        continue\n",
    "    hit_ix = np.where(labels_h3 == track)[0]\n",
    "    df2 = hits.loc[hit_ix]\n",
    "    df2 = df2.sort_values('z_abs')\n",
    "    hit_ix2 = df2.index.values # remember new indexes after sorting\n",
    "    #print(df2)\n",
    "    xs = df2.x.values\n",
    "    ys = df2.y.values\n",
    "    zs = df2.z.values\n",
    "\n",
    "    # From this track, figure out the most common particle ID from the truth.\n",
    "    # Any hits from our track that belong to that particle will be set to '1'\n",
    "    # (correctly predicted hit), otherwise '0' (outlier).\n",
    "    tf2 = truth.loc[hit_ix]\n",
    "    counters = coll.Counter\n",
    "    track_counts = coll.Counter(tf2.particle_id.values).most_common(len(hit_ix2))\n",
    "    track_particle_id = track_counts[0][0]\n",
    "\n",
    "    for i in range(len(hit_ix2)):\n",
    "        if i < 10:\n",
    "            track_x[idx][i][0] = xs[i] / max_xval\n",
    "            track_x[idx][i][1] = ys[i] / max_yval\n",
    "            track_x[idx][i][2] = zs[i] / max_zval\n",
    "            if (truth.loc[hit_ix2[i]].particle_id == track_particle_id):\n",
    "                track_y[idx][i][0] = 3\n",
    "            else:\n",
    "                track_y[idx][i][0] = 30\n",
    "            #track_y[idx][i][0] = (truth.loc[hit_ix2[i]].particle_id == track_particle_id)\n",
    "            \n",
    "    #if idx < 10:\n",
    "    #    ignore_it = True\n",
    "    #elif idx < 20:\n",
    "    #    print(tf2)\n",
    "    #    print(df2)\n",
    "    #    print(track_x[idx])\n",
    "    #    print(track_y[idx])\n",
    "    #else:\n",
    "    #    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as L\n",
    "import keras.models as M\n",
    "\n",
    "# The inputs to the model.\n",
    "# We will create two data points, just for the example.\n",
    "data_x = np.array([\n",
    "    # Datapoint 1\n",
    "#    [\n",
    "#        # Input features at timestep 1\n",
    "#        [1, 2, 3],\n",
    "#        # Input features at timestep 2\n",
    "#        [4, 5, 6]\n",
    "#    ],\n",
    "    # Datapoint 2\n",
    "    [\n",
    "        # Features at timestep 1\n",
    "        [7, 8, 9],\n",
    "        # Features at timestep 2\n",
    "        [10, 11, 12]\n",
    "    ]\n",
    "])\n",
    "\n",
    "# The desired model outputs.\n",
    "# We will create two data points, just for the example.\n",
    "data_y = np.array([\n",
    "    # Datapoint 1\n",
    "    # Target features at timestep 2\n",
    "#    [105, 106, 107, 108, 109],\n",
    "    # Datapoint 2\n",
    "    # Target features at timestep 2\n",
    "    [205, 206, 207, 208, 209]\n",
    "])\n",
    "\n",
    "# Each input data point has 2 timesteps, each with 3 features.\n",
    "# So the input shape (excluding batch_size) is (2, 3), which\n",
    "# matches the shape of each data point in data_x above.\n",
    "model_input = L.Input(shape=(10, 3))\n",
    "\n",
    "# This RNN will return timesteps with 4 features each.\n",
    "# Because return_sequences=True, it will output 2 timesteps, each\n",
    "# with 4 features. So the output shape (excluding batch size) is\n",
    "# (2, 4), which matches the shape of each data point in data_y above.\n",
    "#model_output = L.LSTM(4, return_sequences=False)(model_input)\n",
    "#model_output = L.LSTM(100, return_sequences=True)(model_input)\n",
    "#model_output = L.Dense(100, activation='linear')(model_output)\n",
    "#model_output = L.Dense(4, activation='linear')(model_output)\n",
    "#model_output = L.LSTM(4, return_sequences=False)(model_output)\n",
    "model_output = L.Bidirectional(L.LSTM(10, return_sequences=True, stateful=False))(model_input)\n",
    "#model_output = L.LSTM(30, return_sequences=True, stateful=False)(model_output)\n",
    "model_output = L.Bidirectional(L.LSTM(10, return_sequences=True, stateful=False))(model_output)\n",
    "model_output = L.TimeDistributed(L.Dense(1, activation='linear'))(model_output)\n",
    "#model_output = L.Dense(100, activation='linear')(model_output)\n",
    "#model_output = L.Dense(30, activation='linear')(model_output)\n",
    "# Create the model.\n",
    "model = M.Model(inputs=model_input, outputs=model_output)\n",
    "\n",
    "# You need to pick appropriate loss/optimizers for your problem.\n",
    "# I'm just using these to make the example compile.\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train\n",
    "#model.fit(data_x, data_y)\n",
    "#model.fit(track_x, track_y)\n",
    "\n",
    "# batch_size=3\n",
    "# num_steps=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for e in range(100):\n",
    "#    #for i in range(track_x.shape[0]):\n",
    "#    #    tx = np.expand_dims(track_x[i], axis=0)\n",
    "#    #    ty = np.expand_dims(track_y[i], axis=0)\n",
    "model.fit(track_x, track_y, batch_size=1, epochs=5, verbose=1)\n",
    "#for i in range(track_x.shape[0]):\n",
    "#        tx = np.expand_dims(track_x[i], axis=0)\n",
    "\n",
    "for i in range(track_x.shape[0]):\n",
    "    tx = np.expand_dims(track_x[i], axis=0)\n",
    "    yhat = model.predict(tx)\n",
    "    ty = track_y[i]\n",
    "    print(ty)\n",
    "    print(yhat)\n",
    "    if i > 10:\n",
    "        break\n",
    "# old - (3 LSTM Layers): Loss: 0.0810\n",
    "# old - (2 LSTM, 2 Dense (100, 30)): Loss ~ 0.08\n",
    "# New TimeDistributed Loss: 0.2072\n",
    "# New normalized TimeDistributed loss: 0.3196\n",
    "# New normalized Bidi-LSTM TimeDistributed loss: 0.3100, 0.1920, 0.1362, 0.1273, 0.1193\n",
    "#  -> same, but with 10-hit track input: 0.5413, 0.4646, 0.4242, 0.3943, 0.3550\n",
    "#  -> 3 for right hit, 10 for outlier, 0 for ignore: 5.1141, 5.0738, 5.0602, 5.0038, 4.6757,\n",
    "#                                                    4.1964, 3.9969, 3.8963, 3.8313, 3.7737\n",
    "#  -> 3 for right hit, 30 for outlier, 0 for ignore: 62.5914"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(track_x.shape[0]):\n",
    "    if i < 1000:\n",
    "        continue\n",
    "    tx = np.expand_dims(track_x[i], axis=0)\n",
    "    yhat = model.predict(tx)\n",
    "    ty = track_y[i]\n",
    "    print('Prediction: ' + str(i))\n",
    "    print(ty)\n",
    "    print(yhat)\n",
    "    if i > 1020:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hits.head())\n",
    "print(len(hits))\n",
    "\n",
    "print(particles.head())\n",
    "print(len(particles))\n",
    "\n",
    "print(cells.head())\n",
    "print(len(cells))\n",
    "\n",
    "print(truth.head())\n",
    "print(len(truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the XY plane\n",
    "g = sns.jointplot(hits.x, hits.y, size=12)\n",
    "\n",
    "#Clear the axes containing the scatter plot\n",
    "g.ax_joint.cla()\n",
    "# Set the current axis to the parent of ax\n",
    "plt.sca(g.ax_joint)\n",
    "\n",
    "volumes = hits.volume_id.unique()\n",
    "for volume in volumes:\n",
    "    v = hits[hits.volume_id == volume]\n",
    "    # scattering the hit coordinates with the particle size = 1\n",
    "    plt.scatter(v.x, v.y, s=1, label='volume {}'.format(volume))\n",
    "\n",
    "plt.xlabel('X (mm)')\n",
    "plt.ylabel('Y (mm)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the YZ plane\n",
    "g = sns.jointplot(hits.z, hits.y, s=1, size=12)\n",
    "g.ax_joint.cla()\n",
    "plt.sca(g.ax_joint)\n",
    "\n",
    "volumes = hits.volume_id.unique()\n",
    "for volume in volumes:\n",
    "    v = hits[hits.volume_id == volume]\n",
    "    plt.scatter(v.z, v.y, s=1, label='volume {}'.format(volume))\n",
    "\n",
    "plt.xlabel('Z (mm)')\n",
    "plt.ylabel('Y (mm)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From XYZ 3D perspective\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for volume in volumes:\n",
    "    v = hits[hits.volume_id == volume]\n",
    "    ax.scatter(v.z, v.x, v.y, s=1, label='volume {}'.format(volume), alpha=0.5)\n",
    "ax.set_title('SHit Locations')\n",
    "ax.set_xlabel('Z (millimeters)')\n",
    "ax.set_ylabel('X (millimeters)')\n",
    "ax.set_zlabel('Y (millimeters)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(particles.nhits.values, axlabel='Hits/Particle', bins=50)\n",
    "plt.title('Distribution of number of hits per particle for event 1000.')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(particles.groupby('q')['vx'].count(),\n",
    "        labels=['negative', 'positive'],\n",
    "        autopct='%.0f%%',\n",
    "        shadow=True,\n",
    "        radius=1)\n",
    "plt.title('Distribution of particle charges.')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the original particles of tracks, most particle collisions are generated from the origin\n",
    "\n",
    "g = sns.jointplot(particles.vz, particles.vy,  s=3, size=12)\n",
    "g.ax_joint.cla()\n",
    "plt.sca(g.ax_joint)\n",
    "\n",
    "n_hits = particles.nhits.unique()\n",
    "for n_hit in n_hits:\n",
    "    p = particles[particles.nhits == n_hit]\n",
    "    plt.scatter(p.vz, p.vy, s=1, label='Hits {}'.format(n_hit))\n",
    "\n",
    "plt.xlabel('Z (mm)')\n",
    "plt.ylabel('Y (mm)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "\n",
    "for charge in [-1, 1]:\n",
    "    q = particles[particles.q == charge]\n",
    "    ax.scatter(q.vz, q.vx, q.vy, s=1, label='Charge {}'.format(charge), alpha=0.5)\n",
    "ax.set_title('Sample of 1000 Particle initial location')\n",
    "ax.set_xlabel('Z (millimeters)')\n",
    "ax.set_ylabel('X (millimeters)')\n",
    "ax.set_zlabel('Y (millimeters)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIT_COUNT = 12\n",
    "particle1 = particles.loc[particles.nhits == HIT_COUNT].iloc[0]\n",
    "particle2 = particles.loc[particles.nhits == HIT_COUNT].iloc[1]\n",
    "particle3 = particles.loc[particles.nhits == HIT_COUNT].iloc[2]\n",
    "\n",
    "\n",
    "p_traj_surface1 = truth[truth.particle_id == particle1.particle_id][['tx', 'ty', 'tz']]\n",
    "p_traj_surface2 = truth[truth.particle_id == particle2.particle_id][['tx', 'ty', 'tz']]\n",
    "p_traj_surface3 = truth[truth.particle_id == particle3.particle_id][['tx', 'ty', 'tz']]\n",
    "\n",
    "\n",
    "\n",
    "p_traj1 = (p_traj_surface1\n",
    "          .append({'tx': particle1.vx, 'ty': particle1.vy, 'tz': particle1.vz}, ignore_index=True)\n",
    "          .sort_values(by='tz'))\n",
    "\n",
    "p_traj2 = (p_traj_surface2\n",
    "          .append({'tx': particle2.vx, 'ty': particle2.vy, 'tz': particle2.vz}, ignore_index=True)\n",
    "          .sort_values(by='tz'))\n",
    "\n",
    "p_traj3 = (p_traj_surface3\n",
    "          .append({'tx': particle3.vx, 'ty': particle3.vy, 'tz': particle3.vz}, ignore_index=True)\n",
    "          .sort_values(by='tz'))\n",
    "\n",
    "\n",
    "# Visualize XY projection to the Z-axis\n",
    "\n",
    "plt.plot(p_traj1.tz, p_traj1.ty, '-o', label='hits')\n",
    "plt.plot(p_traj2.tz, p_traj2.ty, '-o', label='hits')\n",
    "plt.plot(p_traj3.tz, p_traj3.ty, '-o', label='hits')\n",
    "plt.xlabel('Z (mm)')\n",
    "plt.ylabel('Y (mm)')\n",
    "plt.title('ZY projection to the X-axis')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot(\n",
    "    xs=p_traj1.tx,\n",
    "    ys=p_traj1.ty,\n",
    "    zs=p_traj1.tz, marker='o')\n",
    "ax.plot(\n",
    "    xs=p_traj2.tx,\n",
    "    ys=p_traj2.ty,\n",
    "    zs=p_traj2.tz, marker='o')\n",
    "ax.plot(\n",
    "    xs=p_traj3.tx,\n",
    "    ys=p_traj3.ty,\n",
    "    zs=p_traj3.tz, marker='o')\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xlabel('X (mm)')\n",
    "ax.set_ylabel('Y (mm)')\n",
    "ax.set_zlabel('Z  (mm) -- Detection layers')\n",
    "plt.title('Trajectories of two particles as they cross the detection surface ($Z$ axis).')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
