{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "from trackml.dataset import load_event, load_dataset\n",
    "from trackml.randomize import shuffle_hits\n",
    "from trackml.score import score_event\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import collections as coll\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "import merge as merge\n",
    "import extension as ext\n",
    "import zroutlier as zro\n",
    "import free_hits as free\n",
    "import track_score as score2\n",
    "import straight_tracks as strt\n",
    "import eda_utils as eda\n",
    "import r0outlier as r0o\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = '../../../input/test'\n",
    "event_id = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "event_prefix = 'event00000' + str(event_id)\n",
    "hits, cells, particles = load_event(os.path.join(TRAIN_PATH, event_prefix))\n",
    "\n",
    "mem_bytes = (hits.memory_usage(index=True).sum() \n",
    "             + cells.memory_usage(index=True).sum() \n",
    "             + particles.memory_usage(index=True).sum() \n",
    "             + truth.memory_usage(index=True).sum())\n",
    "print('{} memory usage {:.2f} MB'.format(event_prefix, mem_bytes / 2**20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_one = pd.read_csv('../../merge_subs/submission_first.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_two = pd.read_csv('../../merge_subs/submission_second.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging for test event: 0\n",
      "Merging for test event: 1\n",
      "Merging for test event: 2\n",
      "Merging for test event: 3\n",
      "Merging for test event: 4\n",
      "Merging for test event: 5\n",
      "Merging for test event: 6\n",
      "Merging for test event: 7\n",
      "Merging for test event: 8\n",
      "Merging for test event: 9\n",
      "Merging for test event: 10\n",
      "Merging for test event: 11\n",
      "Merging for test event: 12\n",
      "Merging for test event: 13\n",
      "Merging for test event: 14\n",
      "Merging for test event: 15\n",
      "Merging for test event: 16\n",
      "Merging for test event: 17\n",
      "Merging for test event: 18\n",
      "Merging for test event: 19\n",
      "Merging for test event: 20\n",
      "Merging for test event: 21\n",
      "Merging for test event: 22\n",
      "Merging for test event: 23\n",
      "Merging for test event: 24\n",
      "Merging for test event: 25\n",
      "Merging for test event: 26\n",
      "Merging for test event: 27\n",
      "Merging for test event: 28\n",
      "Merging for test event: 29\n",
      "Merging for test event: 30\n",
      "Merging for test event: 31\n",
      "Merging for test event: 32\n",
      "Merging for test event: 33\n",
      "Merging for test event: 34\n",
      "Merging for test event: 35\n",
      "Merging for test event: 36\n",
      "Merging for test event: 37\n",
      "Merging for test event: 38\n",
      "Merging for test event: 39\n",
      "Merging for test event: 40\n",
      "Merging for test event: 41\n",
      "Merging for test event: 42\n",
      "Merging for test event: 43\n",
      "Merging for test event: 44\n",
      "Merging for test event: 45\n",
      "Merging for test event: 46\n",
      "Merging for test event: 47\n",
      "Merging for test event: 48\n",
      "Merging for test event: 49\n",
      "Merging for test event: 50\n",
      "Merging for test event: 51\n",
      "Merging for test event: 52\n",
      "Merging for test event: 53\n",
      "Merging for test event: 54\n",
      "Merging for test event: 55\n",
      "Merging for test event: 56\n",
      "Merging for test event: 57\n",
      "Merging for test event: 58\n",
      "Merging for test event: 59\n",
      "Merging for test event: 60\n",
      "Merging for test event: 61\n",
      "Merging for test event: 62\n",
      "Merging for test event: 63\n",
      "Merging for test event: 64\n",
      "Merging for test event: 65\n",
      "Merging for test event: 66\n",
      "Merging for test event: 67\n",
      "Merging for test event: 68\n",
      "Merging for test event: 69\n",
      "Merging for test event: 70\n",
      "Merging for test event: 71\n",
      "Merging for test event: 72\n",
      "Merging for test event: 73\n",
      "Merging for test event: 74\n",
      "Merging for test event: 75\n",
      "Merging for test event: 76\n",
      "Merging for test event: 77\n",
      "Merging for test event: 78\n",
      "Merging for test event: 79\n",
      "Merging for test event: 80\n",
      "Merging for test event: 81\n",
      "Merging for test event: 82\n",
      "Merging for test event: 83\n",
      "Merging for test event: 84\n",
      "Merging for test event: 85\n",
      "Merging for test event: 86\n",
      "Merging for test event: 87\n",
      "Merging for test event: 88\n",
      "Merging for test event: 89\n",
      "Merging for test event: 90\n",
      "Merging for test event: 91\n",
      "Merging for test event: 92\n",
      "Merging for test event: 93\n",
      "Merging for test event: 94\n",
      "Merging for test event: 95\n",
      "Merging for test event: 96\n",
      "Merging for test event: 97\n",
      "Merging for test event: 98\n",
      "Merging for test event: 99\n",
      "Merging for test event: 100\n",
      "Merging for test event: 101\n",
      "Merging for test event: 102\n",
      "Merging for test event: 103\n",
      "Merging for test event: 104\n",
      "Merging for test event: 105\n",
      "Merging for test event: 106\n",
      "Merging for test event: 107\n",
      "Merging for test event: 108\n",
      "Merging for test event: 109\n",
      "Merging for test event: 110\n",
      "Merging for test event: 111\n",
      "Merging for test event: 112\n",
      "Merging for test event: 113\n",
      "Merging for test event: 114\n",
      "Merging for test event: 115\n",
      "Merging for test event: 116\n",
      "Merging for test event: 117\n",
      "Merging for test event: 118\n",
      "Merging for test event: 119\n",
      "Merging for test event: 120\n",
      "Merging for test event: 121\n",
      "Merging for test event: 122\n",
      "Merging for test event: 123\n",
      "Merging for test event: 124\n",
      "Saving submission file as: ../../merge_subs/submission_new.csv\n",
      "Submission file saved.\n"
     ]
    }
   ],
   "source": [
    "#print(submission_one.head)\n",
    "path_to_test = '../../../input/test'\n",
    "test_skip = 0\n",
    "test_events = 125\n",
    "for event_id, hits, cells in load_dataset(path_to_test, skip=test_skip, nevents=test_events, parts=['hits', 'cells']):\n",
    "    print('Merging for test event: ' + str(event_id))\n",
    "    hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "    hits['zr'] = hits['z'] / hits['r']\n",
    "    df1 = submission_one.loc[(submission_one['event_id'] == event_id)]\n",
    "    df2 = submission_two.loc[(submission_two['event_id'] == event_id)]\n",
    "    labels1 = df1.track_id.values\n",
    "    labels1 = merge.remove_outliers(labels1, hits, cells, smallest_track_size=2, aggressive=False, print_counts=False)\n",
    "    labels2 = df2.track_id.values\n",
    "    labels2 = merge.remove_outliers(labels2, hits, cells, smallest_track_size=2, aggressive=False, print_counts=False)\n",
    "    labels_merged = merge.heuristic_merge_tracks(labels1, labels2, hits, overwrite_limit=4, weak_tracks=True)\n",
    "    labels_merged = merge.renumber_labels(labels_merged)\n",
    "    submission_one.loc[submission_one.event_id == event_id, 'track_id'] = labels_merged\n",
    "\n",
    "\n",
    "submission_file = os.path.join('../../merge_subs/submission_new.csv')\n",
    "print('Saving submission file as: ' + str(submission_file))\n",
    "submission_one.to_csv(submission_file, index=False, header=True)\n",
    "print('Submission file saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_helix1 = pd.read_csv('../../1000_r0_exp4/event_' + str(event_id) + '_labels_train_helix1.csv').label.values\n",
    "#labels_helix6 = pd.read_csv('../../1000_r0_exp4/event_1000_labels_train_helix6_phase1_dbscan1.csv').label.values\n",
    "labels_helix1 = pd.read_csv('../../1000_r0_exp3/event_' + str(event_id) + '_labels_train_helix1.csv').label.values\n",
    "labels_helix2 = pd.read_csv('../../1000_r0_exp3/event_' + str(event_id) + '_labels_train_helix2.csv').label.values\n",
    "labels_helix3 = pd.read_csv('../../1000_r0_exp3/event_' + str(event_id) + '_labels_train_helix3.csv').label.values\n",
    "labels_helix4 = pd.read_csv('../../1000_r0_exp3/event_' + str(event_id) + '_labels_train_helix4.csv').label.values\n",
    "labels_helix5 = pd.read_csv('../../1000_r0_exp3/event_' + str(event_id) + '_labels_train_helix5.csv').label.values\n",
    "labels_helix6 = pd.read_csv('../../1000_r0_exp3/event_' + str(event_id) + '_labels_train_helix6.csv').label.values\n",
    "labels_helix7 = pd.read_csv('../../1000_r0_exp3/event_' + str(event_id) + '_labels_train_helix7.csv').label.values\n",
    "labels_helix8 = pd.read_csv('../../1000_r0_exp3/event_' + str(event_id) + '_labels_train_helix8.csv').label.values\n",
    "labels_helix9 = pd.read_csv('../../1000_r0_exp3/event_' + str(event_id) + '_labels_train_helix9.csv').label.values\n",
    "labels_helix10 = pd.read_csv('../../1000_r0_exp3/event_' + str(event_id) + '_labels_train_helix10.csv').label.values\n",
    "labels_helix11 = pd.read_csv('../../1000_r0_exp3/event_' + str(event_id) + '_labels_train_helix11.csv').label.values\n",
    "labels_helix12 = pd.read_csv('../../1000_r0_exp3/event_' + str(event_id) + '_labels_train_helix12.csv').label.values\n",
    "labels_helix13 = pd.read_csv('../../1000_r0_exp3/event_' + str(event_id) + '_labels_train_helix13.csv').label.values\n",
    "labels_helix14 = pd.read_csv('../../1000_r0_exp3/event_' + str(event_id) + '_labels_train_helix14.csv').label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_event_submission(event_id, hits, labels):\n",
    "    sub_data = np.column_stack(([event_id]*len(hits), hits.hit_id.values, labels))\n",
    "    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n",
    "    return submission\n",
    "\n",
    "def score_one_submission(event_id, hits, labels, truth):\n",
    "    submission = create_one_event_submission(event_id, hits, labels)\n",
    "    score = score_event(truth, submission)\n",
    "    print(\"Score for event %d: %.8f\" % (event_id, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits['z_abs'] = hits.z.abs()\n",
    "hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "hits['zr'] = hits['z'] / hits['r']\n",
    "hits['azr'] = np.arctan2(hits['z'], hits['r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_merged = merge.heuristic_merge_tracks(labels_helix1_filter, labels_helix2_filter, overwrite_limit=3)\n",
    "#labels_merged = merge.heuristic_merge_tracks(labels_merged, labels_helix3_filter)\n",
    "score_one_submission(event_id, hits, labels_merged, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_helix1_filter2 = remove_outliers_zr(labels_helix1, hits)\n",
    "labels_helix1_filter3 = remove_outliers_zr(labels_helix1_filter2, hits)\n",
    "#labels_helix1_filter4 = safe_outlier_removal(labels_helix1_filter3, hits, truth, debug=False)\n",
    "labels_helix1_filter4 = remove_outliers_zr(labels_helix1_filter3, hits)\n",
    "labels_helix1_filter5 = merge.remove_outliers(labels_helix1_filter4, hits, print_counts=True)\n",
    "labels_helix2_filter2 = remove_outliers_zr(labels_helix2, hits)\n",
    "labels_helix2_filter3 = remove_outliers_zr(labels_helix2_filter2, hits)\n",
    "labels_helix2_filter4 = remove_outliers_zr(labels_helix2_filter3, hits)\n",
    "#labels_helix2_filter4 = safe_outlier_removal(labels_helix2_filter3, hits, truth, debug=False)\n",
    "labels_helix2_filter5 = merge.remove_outliers(labels_helix2_filter4, hits, print_counts=True)\n",
    "labels_merged = merge.heuristic_merge_tracks(labels_helix1_filter5, labels_helix2_filter5, overwrite_limit=3)\n",
    "score_one_submission(event_id, hits, labels_merged, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_helix1_filter2 = safe_outlier_removal(labels_helix1, hits, truth, find_all=True, debug=False)\n",
    "#labels_helix1_filter2 = safe_outlier_removal(labels_helix1_filter2, hits, truth, debug=False)\n",
    "#labels_helix2_filter2 = safe_outlier_removal(labels_helix2, hits, truth, debug=False)\n",
    "#labels_helix2_filter2 = safe_outlier_removal(labels_helix2_filter2, hits, truth, debug=False)\n",
    "#labels_helix2_filter2 = safe_outlier_removal(labels_helix2_filter2, hits, truth, debug=False)\n",
    "#labels_helix1_filter3 = safe_outlier_removal(labels_helix1_filter2, hits, truth)\n",
    "# 321 v 167, 394 v 199 for find_all\n",
    "# 2143 v 407, 2634 v 512 for find_all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_circle_d0(p0, phi0, kappa):\n",
    "    x0_sin = p0.x * std::sin(phi0)\n",
    "    y0_cos = p0.y * std::cos(phi0)\n",
    "    return (y0_cos - x0_sin) + 0.5 * kappa * (p0.x**2 + p0.y**2)\n",
    "        + 0.5 * kappa * (x0_sin**2 + y0_cos**2 - 2 * x0_sin * y0_cos);\n",
    "\n",
    "# Estimate the z position at vanishing transverse radius.\n",
    "def estimate_z_helix_z0(p0, theta):\n",
    "    return p0.z - std::hypot(p0.x, p0.y * std::tan(M_PI_2 - theta);\n",
    "\n",
    "namespace Acts {\n",
    "namespace Seeding {\n",
    "\n",
    "  struct HelixSeedConfig\n",
    "  {\n",
    "    double rangePhi1     = 0.2;  // search range in phi at layer 1\n",
    "    double rangePhi2     = 0.2;  // search range in phi at layer 2\n",
    "    double maxDeltaTheta = 0.1;  // cut on difference in theta between doublets\n",
    "  };\n",
    "}  // namespace Seeding\n",
    "}  // namespace Acts\n",
    "\n",
    "template <typename Identifier>\n",
    "inline void\n",
    "Acts::Seeding::findHelixSeeds(const HelixSeedConfig&               cfg,\n",
    "                              const BarrelSpacePoints<Identifier>& barrel0,\n",
    "                              const BarrelSpacePoints<Identifier>& barrel1,\n",
    "                              const BarrelSpacePoints<Identifier>& barrel2,\n",
    "                              TrackSeeds3<Identifier>&             seeds)\n",
    "{\n",
    "  for (const auto& p0 : barrel0.points) {\n",
    "    for (const auto& p1 : barrel1.rangeDeltaPhi(p0.phi(), cfg.rangePhi1)) {\n",
    "      Vector3D d01     = p1.position() - p0.position();\n",
    "      double   theta01 = d01.theta();\n",
    "      // Acts::Vector3D at2\n",
    "      //     = detail::calcLineCircleIntersection(p0, d01, barrel2.radius);\n",
    "      for (const auto& p2 : barrel2.rangeDeltaPhi(p1.phi(), cfg.rangePhi2)) {\n",
    "        Vector3D d12     = p2.position() - p1.position();\n",
    "        double   theta12 = d12.theta();\n",
    "\n",
    "        if (cfg.maxDeltaTheta < std::abs(theta12 - theta01)) continue;\n",
    "\n",
    "        double kappa = detail::calcCircleCurvature(d01, d12);\n",
    "        // initial direction correction due to curvature, use\n",
    "        //   chord = 2 * radius * sin(propagation angle / 2)\n",
    "        // and assume sin(x) = x\n",
    "        double phi01 = d01.phi() - d01.head<2>().norm() * kappa / 2;\n",
    "        // track parameters defined at the first space point\n",
    "        seeds.emplace_back(phi01, theta01, kappa, p0, p1, p2);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_helix1_filter = merge.remove_outliers(labels_helix1, hits, print_counts=True)\n",
    "labels_helix2_filter = merge.remove_outliers(labels_helix2, hits, print_counts=True)\n",
    "labels_merged = merge.heuristic_merge_tracks(labels_helix1_filter, labels_helix2_filter, hits, overwrite_limit=6)\n",
    "score_one_submission(event_id, hits, labels_merged, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_horrible_track(track, labels, hits):\n",
    "    hit_ix = np.where(labels==track)[0]\n",
    "    df = hits.iloc[hit_ix]\n",
    "    df = df.sort_values('z')\n",
    "    vols = df.volume_id.values\n",
    "    lays = df.layer_id.values\n",
    "    zs = df.z.values\n",
    "    dupz_count = 0\n",
    "    seen_vols = [0]\n",
    "    seen_lays = [0]\n",
    "    horrible = 0\n",
    "    last_lay_count = 0\n",
    "    for ix, vol in enumerate(vols):\n",
    "        if vol != seen_vols[-1]:\n",
    "            seen_lays = [lays[ix]]\n",
    "            # Check if vol in seen_vols (i.e. go back and forth between volumes)\n",
    "            # 8/14/0 (so-so, not too great though, too many false positives!)\n",
    "            seen_vols.append(vol)\n",
    "            last_lay_count = 1\n",
    "        elif lays[ix] != seen_lays[-1]:\n",
    "            if vol != 7 and vol != 9 and (lays[ix] != (seen_lays[-1] + 2)) and (lays[ix] != (seen_lays[-1] - 2)):\n",
    "                horrible = 3\n",
    "                break\n",
    "            seen_lays.append(lays[ix])\n",
    "            last_lay_count = 1\n",
    "        else:\n",
    "            last_lay_count = last_lay_count + 1\n",
    "            # count==7: 3/0/0, count==6: 15/2/0, count==5: 28/8/0, count==4: 83/87/2\n",
    "            if last_lay_count == 4:\n",
    "                horrible = 1\n",
    "                break\n",
    "        if ix > 0 and zs[ix] == zs[ix-1] and vol == vols[ix-1] and lays[ix] == lays[ix-1]:\n",
    "            dupz_count = dupz_count + 1\n",
    "            # count==1: 30/17/0, count==2: 21/4/0, count==3: 12/2/0, count==4: 11/0/0\n",
    "            if dupz_count == 1:\n",
    "                horrible = 2\n",
    "                break\n",
    "\n",
    "    return horrible\n",
    "    \n",
    "def find_horrible_tracks(labels, hits):\n",
    "    tracks = np.unique(helix6)\n",
    "    horrible_tracks = []\n",
    "    for track in tracks:\n",
    "        if track == 0: continue\n",
    "        if is_horrible_track(track, labels, hits):\n",
    "            horrible_tracks.append(track)\n",
    "    return horrible_tracks\n",
    "\n",
    "def find_badr0_tracks(labels, hits):\n",
    "    bad_r0s = []\n",
    "    tracks = np.unique(labels)\n",
    "    for track in tracks:\n",
    "        if track == 0: continue\n",
    "        tix = np.where(labels==track)[0]\n",
    "        if len(tix) < 4:\n",
    "            continue\n",
    "        t = hits.iloc[tix].as_matrix(columns=['x','y','z'])\n",
    "        t = t[np.argsort(np.fabs(t[:,2]))]\n",
    "        x0, y0, r0  = helix_estimate_param_from_track(t)\n",
    "        #print('ii: ' + str(ii) + ', r0: '+ str(r0))\n",
    "        if int(r0) >= 325:\n",
    "            bad_r0s.append(track)\n",
    "    return bad_r0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hor_trk2 = find_horrible_tracks(helix6, hits)\n",
    "hor_trk = find_badr0_tracks(helix6, hits)\n",
    "print('hor_trk length: ' + str(len(hor_trk)))\n",
    "print(hor_trk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = []\n",
    "maybe = []\n",
    "wrong = []\n",
    "for trk in hor_trk:\n",
    "    if trk in horrible_tracks:\n",
    "        right.append(trk)\n",
    "    elif trk in imperfect_tracks:\n",
    "        maybe.append(trk)\n",
    "    else:\n",
    "        wrong.append(trk)\n",
    "        \n",
    "print('Right: ' + str(len(right)))\n",
    "print('Maybe: ' + str(len(maybe)))\n",
    "print('Wrong: ' + str(len(wrong)))\n",
    "print(maybe)\n",
    "print(wrong)\n",
    "# was right 48, maybe 13\n",
    "# was right 36, maybe 13\n",
    "# 56, 20\n",
    "# 39, 14\n",
    "# ign_w: 52/17, 17: [412, 508, 612, 1108, 1368, 1376, 1436, 1465, 1840, 1853, 2030, 2592, 2825, 3357, 4824, 6117, 6245]\n",
    "# 2w: 56/20, 20: [412, 508, 612, 1108, 1368, 1376, 1436, 1465, 1840, 1853, 2030, 2215, 2592, 2825, 3357, 4824, 4835, 5167, 6117, 6245]\n",
    "# 1w: too many!\n",
    "# removed: [2215 4835 5167]\n",
    "# now: 47 v 24\n",
    "# bad_r0s: >300: 359, 172/186/1\n",
    "# bad_r0s: >350: 73, 56/17/0\n",
    "# bad_r0s: >325: 169, 107/62/0\n",
    "# bad_r0s + horrible tracks: 234, 148/86/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hort in hor_trk2:\n",
    "    if not hort in hor_trk:\n",
    "        hor_trk.append(hort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helix6 = np.copy(labels_helix9)\n",
    "#helix6 = merge.remove_outliers(helix6, hits, smallest_track_size=6, aggressive=False, print_counts=False)\n",
    "(helix6, small_count) = merge.remove_small_tracks(helix6, smallest_track_size=6)\n",
    "#helix6 = r0o.remove_badr0_tracks(helix6, hits)\n",
    "helix6 = merge.renumber_labels(helix6)\n",
    "tracks = np.unique(helix6)\n",
    "short_tracks = []\n",
    "perfect_tracks = []\n",
    "imperfect_tracks = []\n",
    "horrible_tracks = []\n",
    "for track in tracks:\n",
    "    if track == 0: continue\n",
    "    tix = np.where(helix6 == track)[0]\n",
    "    if len(tix) < 6: continue\n",
    "    else:\n",
    "        (is_match, correct,incorrect) = eda.track_distance_from_truth(track, helix6, hits, truth)\n",
    "        if is_match:\n",
    "            perfect_tracks.append(track)\n",
    "        elif incorrect == 0:\n",
    "            short_tracks.append(track)\n",
    "        elif incorrect <= 4 and correct >= incorrect:\n",
    "            imperfect_tracks.append(track)\n",
    "        else:\n",
    "            horrible_tracks.append(track)\n",
    "\n",
    "print('Total tracks:     ' + str(len(tracks)))\n",
    "print('Perfect tracks:   ' + str(len(perfect_tracks)))\n",
    "print('Short tracks:     ' + str(len(short_tracks)))\n",
    "print('Imperfect tracks: ' + str(len(imperfect_tracks)))\n",
    "print('Horrible tracks:  ' + str(len(horrible_tracks)))\n",
    "#print(horrible_tracks)\n",
    "# original: 38138 tracks, 9003 perfect, 29135 imperfect\n",
    "# outlier_rem(smallest 4?): 18711 tracks, 2360 perfect, 16350 imperfect\n",
    "# small_rem: 18711 tracks, 2136 perfect, 16574 imperfect\n",
    "# outlier_rem(smallest 6): 6197 tracks, 1436 perfect, 4760 imperfect\n",
    "# small_rem(6): 6417 tracks, 1214 perfect, 5202 imperfect\n",
    "# Nicole, outlier_rem, smallest 6: 6258 tracks, 1463 perfect, 4794 imperfect\n",
    "# tracks 6258, perfect 1463, imperfect 4420, horrible 374\n",
    "# after fixing: tracks 6258, perfect 1463, imperfect 4219, horrible 575\n",
    "# >500r0: tracks 5580, perfect 1310, imperfect 3755, horrible 514\n",
    "# new labels: tracks 6305, perfect 1448, imperfect 4215, horrible 641\n",
    "# Tracks: 7959, perfect 1304, imperfect 5913, horrible 741\n",
    "# Tracks: 8329, perfect 1307, imperfect 6200, horrible 821\n",
    "# Tracks: 40555, short 34222, perfect 1252, imperfect 4725, horrible 355\n",
    "# tracks: 6333, perfect 1252, short 2297, imperfect 2428, horrible 355\n",
    "# Helix1: 8143, perfect 1972, short 1364, imperfect 3547, horrible 1259\n",
    "# Helix2: 7666, perfect 2020, short 1170, imperfect 3522, horrible 953\n",
    "# Helix3: 6717, perfect 1783, short 1589, imperfect 2696, horrible 648\n",
    "# Helix4: 6688, perfect 1547, short 1649, imperfect 2780, horrible 711\n",
    "# Helix5: 6645, perfect 1600, short 1763, imperfect 2781, horrible 500\n",
    "# Helix6: 7436, perfect 1755, short 1716, imperfect 3361, horrible 603\n",
    "# Helix7: 7451, perfect 1795, short 1795, imperfect 3299, horrible 589\n",
    "# Helix8: 7421, perfect 1734, short 1638, imperfect 3302, horrible 746\n",
    "# Helix9: 7414, perfect 1807, short 1699, imperfect 3276, horrible 631"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracks: 7959, perfect 1304, imperfect 5913, horrible 741\n",
    "r0s_perfect1 = []\n",
    "r0s_perfect2 = []\n",
    "#r0s_imperfect1 = []\n",
    "#r0s_imperfect2 = []\n",
    "#r0s_horrible1 = []\n",
    "#r0s_horrible2 = []\n",
    "for ii in range(5913):\n",
    "    #if ii == 0: continue\n",
    "    #track_id = horrible_tracks[ii]\n",
    "    track_id = perfect_tracks[ii]\n",
    "    tix = np.where(helix6==track_id)[0]\n",
    "    if len(tix) < 4:\n",
    "        print('short')\n",
    "        continue\n",
    "    #t = hits.iloc[tix].as_matrix(columns=['x','y','z'])\n",
    "    #t = t[np.argsort(np.fabs(t[:,2]))]\n",
    "    x0, y0, r0  = helix_estimate_param_from_track(tix, hits, first_half=True)\n",
    "    x1, y1, r1  = helix_estimate_param_from_track(tix, hits, first_half=False)\n",
    "    #print('ii: ' + str(ii) + ', r0: '+ str(r0))\n",
    "    r0s_imperfect1.append(int(r0))\n",
    "    r0s_imperfect2.append(int(r1))\n",
    "    #r0s_horrible1.append(int(r0))\n",
    "    #r0s_horrible2.append(int(r1))\n",
    "\n",
    "\n",
    "    \n",
    "#print(np.unique(r0s_perfect1))\n",
    "#print(r0s_perfect1)\n",
    "#print(np.unique(r0s_perfect2))\n",
    "#print(r0s_perfect2)\n",
    "distance = np.asarray(r0s_imperfect2) - np.asarray(r0s_imperfect1)\n",
    "#distance = np.asarray(r0s_horrible2) - np.asarray(r0s_horrible1)\n",
    "#print(list(distance))\n",
    "print(np.unique(distance))\n",
    "#print(np.unique(r0s))\n",
    "#6000, 6015, 6021, 6036, 6043, 6055, 6057, 6065, 6071, 6079, 6080, 6095, 6096, 6126, 6127, 6131, 6172, 6176, 6182,\n",
    "#6183, 6196, 6212, 6220, 6228, 6234, 6237, 6268, 6296, 6300, 6309, 6313, 6314, 6317, 6333, 6336, 6347, 6365, 6366,\n",
    "#6370, 6373, 6387, 6400, 6412, 6450, 6458, 6466, 6480, 6481, 6499, 6503, 6516, 6532, 6543, 6549, \n",
    "#track_id = 6036\n",
    "#eda.compare_track_to_truth(track_id, helix6, hits, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance = np.asarray(r0s_perfect2) - np.asarray(r0s_perfect1)\n",
    "#print(np.unique(distance))\n",
    "difference = []\n",
    "r0s1 = r0s_horrible1\n",
    "r0s2 = r0s_horrible2\n",
    "for i in range(len(r0s1)):\n",
    "    if r0s2[i] > r0s1[i]:\n",
    "        difference.append(r0s2[i] - r0s1[i])\n",
    "    else:\n",
    "        difference.append(r0s1[i] - r0s2[i])\n",
    "print(np.unique(difference))\n",
    "ix = np.where(np.asarray(difference) > 205)[0]\n",
    "print(len(ix))\n",
    "# Perfect: 0..203 --> 0/203\n",
    "# Imperfect: 0..314 --> 108/5913\n",
    "# Horrible: 0..283 --> 29/741"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.copy(helix6)\n",
    "score_one_submission(event_id, hits, labels, truth)\n",
    "tracks = np.unique(labels)\n",
    "total_hits_removed = 0\n",
    "total_tracks_removed = 0\n",
    "woops = 0\n",
    "waaps = 0\n",
    "wuups = 0\n",
    "wiips = 0\n",
    "for track in tracks:\n",
    "    if track == 0: continue\n",
    "    tix = np.where(labels==track)[0]\n",
    "    if len(tix) < 4:\n",
    "        #print('short track: ' + str(track))\n",
    "        continue\n",
    "    #t = hits.iloc[tix].as_matrix(columns=['x','y','z'])\n",
    "    #t = t[np.argsort(np.fabs(t[:,2]))]\n",
    "    r0, r1, r2  = helix_estimate_param_from_track(tix, hits)\n",
    "    if r2 > r1:\n",
    "        distance = r2 - r1\n",
    "    else:\n",
    "        distance = r1 - r2\n",
    "    if distance > 210:\n",
    "        if track in perfect_tracks:\n",
    "            woops = woops + 1\n",
    "        elif track in short_tracks:\n",
    "            waaps = waaps + 1\n",
    "        elif track in imperfect_tracks:\n",
    "            wuups = wuups + 1\n",
    "        elif track in horrible_tracks:\n",
    "            wiips = wiips + 1\n",
    "        else:\n",
    "            print('Unknown track classification for: ' + str(track))\n",
    "        total_tracks_removed = total_tracks_removed + 1\n",
    "        total_hits_removed = total_hits_removed + len(tix)\n",
    "        labels[tix] = 0\n",
    "    elif False:\n",
    "        horrible = is_horrible_track(track, labels, hits)\n",
    "        if horrible == 1:\n",
    "            waaps = waaps + 1\n",
    "            #labels[tix] = 0\n",
    "        elif horrible == 2:\n",
    "            wuups = wuups + 1\n",
    "            labels[tix] = 0\n",
    "\n",
    "print('Hits removed: ' + str(total_hits_removed))\n",
    "print('Tracks removed: ' + str(total_tracks_removed))\n",
    "print('Woops tracks: ' + str(woops))\n",
    "print('Waaps tracks: ' + str(waaps))\n",
    "print('Wuups tracks: ' + str(wuups))\n",
    "print('Wiips tracks: ' + str(wiips))\n",
    "score_one_submission(event_id, hits, labels, truth)\n",
    "# helix6 Base: 0.54537665\n",
    "# rem > 200: 1319 hits removed, 171 tracks, score: 0.53772466\n",
    "# rem > 205: 1108 hits removed, 144 tracks, score: 0.53941216\n",
    "### rem > 210: 937 hits removed, 127 tracks, score: 0.54056964\n",
    "# rem > 215: 753 hits removed, 105 tracks, score: 0.54201582\n",
    "# rem > 220: 607 hits removed, 84 tracks, score: 0.54288226\n",
    "# rem > 225: 464 hits removed, 69 tracks, score: 0.54362237\n",
    "# rem > 250: 155 hits removed, 23 tracks, score: 0.54491888\n",
    "# rem > 275: 54 hits removed, 10 tracks, score: 0.54531920\n",
    "# helix7 Base: 0.53555362\n",
    "# rem > 200: 1114 hits removed, 143 tracks, score: 0.52910335\n",
    "# rem > 210: 761 hits removed, 101 tracks, score: 0.53156531\n",
    "# rem > 275: 40 hits removed, 8 tracks, score: 0.53555362\n",
    "# helix8 base: 0.47650426\n",
    "# rem > 210: 797 hits removed, 129 tracks, score: 0.47395447\n",
    "# rem > 275: 107 hits removed, 20 tracks, score: 0.47644394\n",
    "# helix9 base: 0.52300172\n",
    "# rem > 210: 791 hits removed, 111 tracks, score: 0.51926913\n",
    "# rem > 275: 71 hits removed, 15 tracks, score: 0.52289633\n",
    "# helix1 base: 0.67825680\n",
    "# rem > 210: 1121 hits removed, 116 tracks, score: 0.67238344\n",
    "# rem > 220: 709 hits removed, 79 tracks, score: 0.67473819\n",
    "# rem > 275: 101 hits removed, 14 tracks, score: 0.67793125\n",
    "# helix2 base: 0.68286378\n",
    "# rem > 210: 1364, 122, 0.67502624\n",
    "# rem > 220: 955, 86, 0.67801350\n",
    "# rem > 275: 108, 13: 0.68261130\n",
    "# helix5 base: 0.56273412\n",
    "# rem > 210: 628, 51, 0.55822773\n",
    "# rem > 220: 412, 34, 0.56000621\n",
    "# rem > 275: 26, 3, 0.56267727\n",
    "# wuups == 2-dupz: 504, score 0.49717993\n",
    "# wuups == 3-dupz: 166, score 0.52564486\n",
    "# wuups == 4-dupz: 99, score: 0.53100398\n",
    "# wuups == 5-dupz: 51, score: 0.53691025 (from 0.54056964)\n",
    "# waaps == 5-layer: 134, score: 0.52633199\n",
    "# waaps == 6-layer: 81, score: 0.52948584 (from 0.53691025)\n",
    "# waaps == 7-layer: 8, score: 0.53423387"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r0s_perfect)\n",
    "print(r0s_imperfect)\n",
    "print(r0s_horrible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Perfect tracks:   ' + str(perfect_tracks))\n",
    "print('Imperfect tracks: ' + str(imperfect_tracks))\n",
    "print('Horrible tracks:  ' + str(horrible_tracks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, bins, ignored = plt.hist(perfect_tracks, 500)#, normed=True)\n",
    "plt.show()\n",
    "count, bins, ignored = plt.hist(imperfect_tracks, 500)#, normed=True)\n",
    "plt.show()\n",
    "count, bins, ignored = plt.hist(horrible_tracks, 500)#, normed=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import least_squares\n",
    "\n",
    "def helix_estimate_param_from_track(track_ix, hits):\n",
    "    \n",
    "    def residuals_xy(param, x, y):\n",
    "        x0, y0 = param\n",
    "        r = np.sqrt((x-x0)**2 + (y-y0)**2)\n",
    "        d = r - r.mean()\n",
    "        return d\n",
    "    \n",
    "    def residuals_z(param, r, z):\n",
    "        m2,m1,m0 = param\n",
    "        zz = m2*r**2 + m1*r + m0\n",
    "        d = z - zz\n",
    "        return d\n",
    "\n",
    "    df = hits.iloc[track_ix]\n",
    "    #t = df.as_matrix(columns=['x','y','z'])\n",
    "    t = df[['x', 'y', 'z']].values\n",
    "    #print(t)\n",
    "    t = t[np.argsort(np.fabs(t[:,2]))]\n",
    "    #print(t)\n",
    "\n",
    "    x = t[:,0]\n",
    "    y = t[:,1]\n",
    "    #z = xyz[:,2]\n",
    "    param0 = (x.mean(), y.mean())\n",
    "    res_lsq0 = least_squares(residuals_xy, param0, loss='soft_l1', f_scale=1.0, args=(x,y))\n",
    "    x0,y0 = res_lsq0.x\n",
    "    r0 = np.sqrt((x-x0)**2 + (y-y0)**2).mean()\n",
    "\n",
    "    x = t[0:int(len(t)/2),0]\n",
    "    y = t[0:int(len(t)/2),1]\n",
    "    #z = xyz[:,2]\n",
    "    param1 = (x.mean(), y.mean())\n",
    "    res_lsq1 = least_squares(residuals_xy, param1, loss='soft_l1', f_scale=1.0, args=(x,y))\n",
    "    x1,y1 = res_lsq1.x\n",
    "    r1 = np.sqrt((x-x1)**2 + (y-y1)**2).mean()\n",
    "\n",
    "    x = t[int(len(t)/2):,0]\n",
    "    y = t[int(len(t)/2):,1]\n",
    "    #z = xyz[:,2]\n",
    "    \n",
    "    param2 = (x.mean(), y.mean())\n",
    "    res_lsq2 = least_squares(residuals_xy, param2, loss='soft_l1', f_scale=1.0, args=(x,y))\n",
    "    x2,y2 = res_lsq2.x\n",
    "    r2 = np.sqrt((x-x2)**2 + (y-y2)**2).mean()\n",
    "    #r = np.sqrt(x**2 + y**2)\n",
    "    \n",
    "    #if 1:\n",
    "    #    param0 = (0,0,0)\n",
    "    #    res_lsq = least_squares(residuals_z, param0, args=(r, z))\n",
    "    #    m2, m1, m0 = res_lsq.x\n",
    "        \n",
    "    #if 0:\n",
    "    #    # polynomial fit of the degree 2, x**2 - quadradic \n",
    "    #    m2,m1,m0 = np.polyfit(r,z,2)\n",
    "        \n",
    "    #param = (x0,y0,r0,m1,m2,m0)\n",
    "    param = (r0,r1,r2)\n",
    "    return param\n",
    "\n",
    "# 3 points - x0, x1, x2, convert to 2 vectors\n",
    "# 1: x1-x0, y1-y0      2: x2-x1, y2-y1\n",
    "def find_circle_curvature(d01x, d01y, d12x, d12y):\n",
    "    x01 = d01x\n",
    "    y01 = d01y\n",
    "    x12 = d12x\n",
    "    y12 = d12y\n",
    "    x02 = x01 + x12\n",
    "    y02 = y01 + y12\n",
    "    # length of the triangle sides\n",
    "    a = (x12**2 + y12**2)**0.5 #np.sqrt(x12**2, y12**2)\n",
    "    b = (x02**2 + y02**2)**0.5 #np.sqrt(x02**2, y02**2)\n",
    "    c = (x01**2 + y01**2)**0.5 #np.sqrt(x01**2, y01**2)\n",
    "    # 2 * (signed) area of the triangle\n",
    "    k = (x02 * y01 - x01 * y02)\n",
    "    # radius = product of side lengths / 4 times triangle area\n",
    "    if a == 0 or b == 0 or c == 0:\n",
    "        return 10\n",
    "    else:\n",
    "        return (2 * k) / (a * b * c)\n",
    "\n",
    "def find_track_curvature(track, labels, hits):\n",
    "    trk_ix = np.where(labels == track)[0]\n",
    "    if len(trk_ix) < 5:\n",
    "        #print('Track too short: ' + str(track))\n",
    "        return (1, 1, 1)\n",
    "    df = hits.loc[trk_ix]\n",
    "    df = df.sort_values('z_abs')\n",
    "    x = df.x.values\n",
    "    y = df.y.values\n",
    "    mid = int(len(x)/2)\n",
    "    # Use (0,0) as starting point for cleaner results\n",
    "    d01xa = x[0]\n",
    "    d01ya = y[0]\n",
    "    d12xa = x[mid] - d01xa\n",
    "    d12ya = y[mid] - d01ya\n",
    "    \n",
    "    d01xb = x[mid]\n",
    "    d01yb = y[mid]\n",
    "    d12xb = x[-1] - d01xb\n",
    "    d12yb = y[-1] - d01yb\n",
    "\n",
    "    d01xc = x[1]\n",
    "    d01yc = y[1]\n",
    "    d12xc = x[-1] - d01xc\n",
    "    d12yc = y[-1] - d01yc\n",
    "\n",
    "    curv02a = find_circle_curvature(d01xa, d01ya, d12xa, d12ya)\n",
    "    curv02b = find_circle_curvature(d01xb, d01yb, d12xb, d12yb)\n",
    "    curv02c = find_circle_curvature(d01xc, d01yc, d12xc, d12yc)\n",
    "    #print(curv02a)\n",
    "    #print(curv02b)\n",
    "    #print(curv02c)\n",
    "    return (curv02a, curv02b, curv02c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bad_curvature_tracks(labels, hits):\n",
    "    rejects = []\n",
    "    tracks = np.unique(labels)\n",
    "    for track in tracks:\n",
    "        if track == 0: continue\n",
    "        (curv1, curv2, curv3) = find_track_curvature(track, labels, hits)\n",
    "        if np.sign(curv1) != np.sign(curv2) or np.sign(curv1) != np.sign(curv3):\n",
    "            rejects.append(track)\n",
    "        else:\n",
    "            c1 = min(abs(curv1), abs(curv2))\n",
    "            c2 = max(abs(curv1), abs(curv2))\n",
    "            c3 = abs(curv3)\n",
    "            ratio = 1.0 - c1/c2\n",
    "            if ratio > 0.50:\n",
    "                rejects.append(track)\n",
    "    return rejects\n",
    "\n",
    "def remove_bad_curvature_tracks(labels, hits):\n",
    "    labels = np.copy(labels)\n",
    "    rejects = find_bad_curvature_tracks(labels, hits)\n",
    "    print('Removing ' + str(len(rejects)) + ' tracks with bad curvature.')\n",
    "    hit_count = 0\n",
    "    for reject in rejects:\n",
    "        tix = np.where(labels==reject)[0]\n",
    "        hit_count = hit_count + len(tix)\n",
    "        labels[labels == reject] = 0\n",
    "    print('Removed ' + str(hit_count) + ' hits from those tracks')\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.copy(helix6)\n",
    "score_one_submission(event_id, hits, labels, truth)\n",
    "labels = remove_bad_curvature_tracks(labels, hits)\n",
    "score_one_submission(event_id, hits, labels, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tracks_based_on_quality(labels, hits):\n",
    "    \"\"\"Split input tracks into 3 categories - strong, medium, weak.\n",
    "    Splitting is determined mainly by how consistent the track helix curvature is.\"\"\"\n",
    "    strong_tracks = []\n",
    "    medium_tracks = []\n",
    "    weak_tracks = []\n",
    "    hits['z_abs'] = hits.z.abs()\n",
    "    tracks, counts = np.unique(labels, return_counts=True)\n",
    "    strong_labels = np.zeros_like(labels)\n",
    "    medium_labels = np.zeros_like(labels)\n",
    "    weak_labels = np.zeros_like(labels)\n",
    "    for ix, track in enumerate(tracks):\n",
    "        if track == 0: continue\n",
    "        if counts[ix] < 5:\n",
    "            if counts[ix] > 3:\n",
    "                medium_tracks.append(track)\n",
    "            # else, discard, too short.\n",
    "            continue\n",
    "        (curv1, curv2, curv3) = r0o.find_track_curvature(track, labels, hits)\n",
    "        if np.sign(curv1) != np.sign(curv2) or np.sign(curv1) != np.sign(curv3):\n",
    "            weak_tracks.append(track)\n",
    "            continue\n",
    "\n",
    "        c1 = min(abs(curv1), abs(curv2))\n",
    "        c2 = max(abs(curv1), abs(curv2))\n",
    "        c3 = abs(curv3)\n",
    "        ratio = 1.0 - c1/c2\n",
    "        if ratio > 0.50:\n",
    "            weak_tracks.append(track)\n",
    "        elif ratio < 0.2:\n",
    "            if counts[ix] > 20 or r0o.is_horrible_track2(track, labels, hits):\n",
    "                medium_tracks.append(track)\n",
    "            else:\n",
    "                strong_tracks.append(track)\n",
    "        else:\n",
    "            medium_tracks.append(track)\n",
    "\n",
    "    for track in strong_tracks:\n",
    "        strong_labels[labels==track] = track\n",
    "    for track in medium_tracks:\n",
    "        medium_labels[labels==track] = track\n",
    "    for track in weak_tracks:\n",
    "        weak_labels[labels==track] = track\n",
    "\n",
    "    return (strong_labels, medium_labels, weak_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_track_quality(labels, hits, truth):\n",
    "    tracks, counts = np.unique(labels, return_counts=True)\n",
    "    short_tracks = 0\n",
    "    perfect_tracks = 0\n",
    "    imperfect_tracks = 0\n",
    "    horrible_tracks = 0\n",
    "    for ix, track in enumerate(tracks):\n",
    "        if track == 0: continue\n",
    "        if counts[ix] < 6: continue\n",
    "\n",
    "        (is_match, correct,incorrect) = eda.track_distance_from_truth(track, labels, hits, truth)\n",
    "        if is_match:\n",
    "            perfect_tracks = perfect_tracks + 1\n",
    "        elif incorrect == 0:\n",
    "            short_tracks = short_tracks + 1\n",
    "        elif incorrect <= 4 and correct >= incorrect:\n",
    "            imperfect_tracks = imperfect_tracks + 1\n",
    "        else:\n",
    "            horrible_tracks = horrible_tracks + 1\n",
    "\n",
    "    print('Total tracks:     ' + str(len(tracks)))\n",
    "    print('Perfect tracks:   ' + str(perfect_tracks))\n",
    "    print('Short tracks:     ' + str(short_tracks))\n",
    "    print('Imperfect tracks: ' + str(imperfect_tracks))\n",
    "    print('Horrible tracks:  ' + str(horrible_tracks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.copy(labels_helix6)\n",
    "score_one_submission(event_id, hits, labels, truth) # 0.63580502\n",
    "(strong, medium, weak) = split_tracks_based_on_quality(labels, hits)\n",
    "score_one_submission(event_id, hits, strong, truth)\n",
    "score_one_submission(event_id, hits, medium, truth)\n",
    "score_one_submission(event_id, hits, weak, truth)\n",
    "# (0.5 weak, 0.1 strong): 0.28793004 (1776/3737), 0.31400817 (1582/5075), 0.03201160 (28/646)\n",
    "# (0.5 weak, 0.15 strong): 0.35599925 (2247/4484), 0.24593896 (1111/4328), 0.03201160 (28/646)\n",
    "# (0.5 weak, 0.2 strong): 0.40582813 (2604/5035), 0.19611008 (754/3777), 0.03201160 (28/646)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_track_quality(strong, hits, truth)\n",
    "display_track_quality(medium, hits, truth)\n",
    "display_track_quality(weak, hits, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_helix1f = merge.remove_outliers(labels_helix1, hits, cells, print_counts=False)\n",
    "labels_helix2f = merge.remove_outliers(labels_helix2, hits, cells, print_counts=False)\n",
    "labels_helix3f = merge.remove_outliers(labels_helix3, hits, cells, print_counts=False)\n",
    "labels_helix4f = merge.remove_outliers(labels_helix4, hits, cells, print_counts=False)\n",
    "labels_helix5f = merge.remove_outliers(labels_helix5, hits, cells, print_counts=False)\n",
    "# LIAM: Add more outlier removal here. Explore different cutoffs (> 0.2?)\n",
    "labels_helix1f = r0o.remove_badr0_tracks(labels_helix1f, hits)\n",
    "labels_helix2f = r0o.remove_badr0_tracks(labels_helix2f, hits)\n",
    "labels_helix3f = r0o.remove_badr0_tracks(labels_helix3f, hits)\n",
    "labels_helix4f = r0o.remove_badr0_tracks(labels_helix4f, hits)\n",
    "labels_helix5f = r0o.remove_badr0_tracks(labels_helix5f, hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = []\n",
    "all_labels.append(labels_helix1f)\n",
    "all_labels.append(labels_helix2f)\n",
    "all_labels.append(labels_helix6)\n",
    "all_labels.append(labels_helix7)\n",
    "all_labels.append(labels_helix8)\n",
    "all_labels.append(labels_helix9)\n",
    "all_labels.append(labels_helix10)\n",
    "all_labels.append(labels_helix11)\n",
    "all_labels.append(labels_helix12)\n",
    "all_labels.append(labels_helix13)\n",
    "all_labels.append(labels_helix14)\n",
    "all_labels.append(labels_helix5f)\n",
    "all_labels.append(labels_helix3f)\n",
    "all_labels.append(labels_helix4f)\n",
    "strong_labels = []\n",
    "medium_labels = []\n",
    "weak_labels = []\n",
    "for label in all_labels:\n",
    "    (strong, medium, weak) = split_tracks_based_on_quality(label, hits)\n",
    "    strong_labels.append(strong)\n",
    "    medium_labels.append(medium)\n",
    "    weak_labels.append(weak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_labels(all_labels, hits, truth):\n",
    "    merge_count = 0\n",
    "    labels_merged = np.copy(all_labels[0])\n",
    "    for i in range(len(all_labels)):\n",
    "        if i == 0: continue\n",
    "        labels_merged = merge.heuristic_merge_tracks(labels_merged, all_labels[i], hits, overwrite_limit=6, print_summary=False)\n",
    "        merge_count = merge_count + 1\n",
    "        #message = 'Merged loop 1-' + str(i+1) + ' score for event '\n",
    "        #display_score(event_id, hits, labels_merged, truth, message)\n",
    "        score_one_submission(event_id, hits, labels_merged, truth)\n",
    "    return labels_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_merged = merge_all_labels(all_labels, hits, truth)\n",
    "# No outlier removal, order 1-9: 0.70948133\n",
    "# outlier removal, 1-2,6-9,5,3,4: 0.71563907\n",
    "# + r0 outlier rem: 0.71652579\n",
    "# + more models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_merged = merge_all_labels(strong_labels, hits, truth)\n",
    "# 0.1 cutoff: 0.47028638\n",
    "# 0.15 cutoff: 0.54866460\n",
    "# 0.2 cutoff: 0.59883933\n",
    "# outlier rem, 0.2 cutoff: 0.60684391\n",
    "# +r0 out. rem: 0.60669428"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_merged = merge_all_labels(medium_labels, hits, truth)\n",
    "# 0.1 cutoff: 0.50709876\n",
    "# 0.15 cutoff: 0.43838218\n",
    "# 0.2 cutoff: 0.38796599\n",
    "# outlier rem, 0.2 cutoff: 0.36414178\n",
    "# +r0 out. rem: 0.36216201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_merged = merge_all_labels(weak_labels, hits, truth)\n",
    "# 0.5 cutoff: 0.09220529\n",
    "# 0.5 cutoff + outlier rem: 0.08541034\n",
    "# + r0 out. rem: 0.08497520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_merged = merge.heuristic_merge_tracks(strong_merged, medium_merged, hits, overwrite_limit=3, print_summary=False)\n",
    "score_one_submission(event_id, hits, labels_merged, truth)\n",
    "# strong=0.1 cutoff: 0.7186 (overwrite=3)\n",
    "# strong=0.15 cutoff: 0.7192 (overwrite=3)\n",
    "# strong=0.2 cutoff: 0.7196\n",
    "# outlier rem., 0.2 cutoff: 0.7246\n",
    "# + r0 out. rem: 0.7260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_merged2 = merge.heuristic_merge_tracks(labels_merged, weak_merged, hits, overwrite_limit=1, print_summary=False)\n",
    "score_one_submission(event_id, hits, labels_merged2, truth)\n",
    "# strong=0.1 cutoff: 0.7228 (overwrite=1)\n",
    "# strong=0.15 cutoff: 0.7237 (overwrite=1)\n",
    "# strong=0.2 cutoff: 0.7241 (overwrite=1)\n",
    "# outlier rem. strong=0.2: 0.7293\n",
    "# +r0 out. rem: 0.7311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_helix6f = remove_outliers2(labels_helix6, hits, cells, print_counts=True)\n",
    "score_one_submission(event_id, hits, labels_helix6, truth)\n",
    "score_one_submission(event_id, hits, labels_helix6f, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_track_outliers2(track, labels, hits, cells, aggressive):\n",
    "    labels = np.copy(labels)\n",
    "    found_bad_volume = 0\n",
    "    found_bad_cell = 0\n",
    "    found_bad_dimension = 0\n",
    "    found_bad_slope = 0\n",
    "    found_bad_z = 0\n",
    "    found_bad_zr = 0\n",
    "\n",
    "    if True:\n",
    "        outlier_zr = zro.find_track_outliers_zr(track, labels, hits)\n",
    "        if len(outlier_zr) > 0:\n",
    "            #print('track ' + str(track) + ' zr outliers: ' + str(outlier_zr))\n",
    "            found_bad_zr = found_bad_zr + len(outlier_zr)\n",
    "            for oix in outlier_zr:\n",
    "                labels[oix] = 0\n",
    "\n",
    "    if True:\n",
    "        # Check if the sorted hits (on z-axis) go through the volumes\n",
    "        # and layers in the expected order\n",
    "        duplicatez_ix = merge.find_duplicate_z_using_zr(track, labels, hits)\n",
    "        if len(duplicatez_ix) > 0:\n",
    "            #print('track ' + str(track) + ' duplicate z: ' + str(duplicatez_ix))\n",
    "            found_bad_z = found_bad_z + len(duplicatez_ix)\n",
    "            for bzix in duplicatez_ix:\n",
    "                labels[bzix] = 0\n",
    "\n",
    "    if False:#True:\n",
    "        # Check the helix slope, discard hits that do not match\n",
    "        outlier_slope_ix = merge.remove_track_outliers_slope(track, labels, hits)\n",
    "        if len(outlier_slope_ix) > 0:\n",
    "            #print('track ' + str(track) + ' slope outliers: ' + str(outlier_slope_ix))\n",
    "            found_bad_slope = found_bad_slope + len(outlier_slope_ix)\n",
    "            for oix in outlier_slope_ix:\n",
    "                labels[oix] = 0\n",
    "\n",
    "    return (labels, found_bad_volume, found_bad_dimension, found_bad_z, found_bad_slope, found_bad_zr, found_bad_cell)\n",
    "\n",
    "\n",
    "def remove_outliers2(labels, hits, cells, smallest_track_size=2, aggressive=False, print_counts=True):\n",
    "    tracks = np.unique(labels)\n",
    "    hits['z_abs'] = hits.z.abs()\n",
    "    hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "    hits['a0'] = np.arctan2(hits.y,hits.x)\n",
    "    hits['zr'] = hits['z'] / hits['r']\n",
    "    count_rem_volume = 0\n",
    "    count_rem_dimension = 0\n",
    "    count_duplicatez = 0\n",
    "    count_rem_slope = 0\n",
    "    count_small_tracks = 0\n",
    "    count_zr = 0\n",
    "    count_cell = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 3:\n",
    "            (labels, c1, c2, c3, c4, c5, c6) = remove_track_outliers2(track, labels, hits, cells, aggressive)\n",
    "            count_rem_volume = count_rem_volume + c1\n",
    "            count_rem_dimension = count_rem_dimension + c2\n",
    "            count_duplicatez = count_duplicatez + c3\n",
    "            count_rem_slope = count_rem_slope + c4\n",
    "            count_zr = count_zr + c5\n",
    "            count_cell = count_cell + c6\n",
    "\n",
    "    # Remove small tracks, we do not get any score for those. This is done\n",
    "    # last, in case removing the outliers (above) removed enough hits\n",
    "    # from a track to make them smaller than the threshold.\n",
    "    (labels, count_small_tracks) = merge.remove_small_tracks(labels, smallest_track_size=smallest_track_size)\n",
    "\n",
    "    if print_counts:\n",
    "        print('Total removed due to bad cells: ' + str(count_cell))\n",
    "        print('Total removed due to bad volumes: ' + str(count_rem_volume))\n",
    "        print('Total removed due to bad zr values: ' + str(count_zr))\n",
    "        print('Total removed due to bad dimensions: ' + str(count_rem_dimension))\n",
    "        print('Total removed due to duplicate zs: ' + str(count_duplicatez))\n",
    "        print('Total removed due to bad slopes: ' + str(count_rem_slope))\n",
    "        print('Total removed small tracks (<' + str(smallest_track_size) + ') hits: ' + str(count_small_tracks))\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracks: 6333, perfect 1252, imperfect 4725, horrible 355\n",
    "labels = np.copy(helix6)\n",
    "reject = []\n",
    "reject_c1 = []\n",
    "reject_c2 = []\n",
    "reject_c3 = []\n",
    "accept = []\n",
    "accept_ratio = []\n",
    "accept_c1 = []\n",
    "accept_c2 = []\n",
    "great = []\n",
    "great_c1 = []\n",
    "great_c2 = []\n",
    "great_c3 = []\n",
    "great_reject = []\n",
    "track_list = perfect_tracks\n",
    "for ii in range(len(track_list)):\n",
    "    #if ii < 10: continue\n",
    "    track_id = track_list[ii]\n",
    "    #track_id = imperfect_tracks[ii]\n",
    "    (curv1, curv2, curv3) = find_track_curvature(track_id, labels, hits)\n",
    "    #eda.compare_track_to_truth(track_id, labels, hits, truth)\n",
    "    if np.sign(curv1) != np.sign(curv2) or np.sign(curv1) != np.sign(curv3):\n",
    "        reject.append(track_id)\n",
    "        #reject_c1.append(curv1)\n",
    "    else:\n",
    "        c1 = min(abs(curv1), abs(curv2))\n",
    "        c2 = max(abs(curv1), abs(curv2))\n",
    "        c3 = abs(curv3)\n",
    "        ratio = 1.0 - c1/c2\n",
    "        if ratio > 0.50:# and c1 > 0.003:\n",
    "            reject.append(track_id)\n",
    "            reject_c1.append(c1)\n",
    "            reject_c2.append(c2)\n",
    "            reject_c3.append(abs(curv3))\n",
    "        else:\n",
    "            if ratio < 0.1:\n",
    "                hit_ix = np.where(labels == track_id)[0]\n",
    "                #df = hits.loc[hit_ix]\n",
    "                #z_vals = df.z.values\n",
    "                #if len(z_vals) > len(np.unique(z_vals)):\n",
    "                #    great_reject.append(track_id)\n",
    "                if is_horrible_track(track_id, labels, hits):\n",
    "                    great_reject.append(track_id)\n",
    "                elif len(hit_ix) > 20:\n",
    "                    great_reject.append(track_id)\n",
    "                else:\n",
    "                    great.append(track_id)\n",
    "                    great_c1.append(c1)\n",
    "                    great_c2.append(c2)\n",
    "                    great_c3.append(abs(curv3))\n",
    "            accept_ratio.append(ratio)\n",
    "            accept.append(track_id)\n",
    "            accept_c1.append(c1)\n",
    "            accept_c2.append(c2)\n",
    "\n",
    "print(len(reject))\n",
    "print(len(great))\n",
    "print(len(great_reject))\n",
    "#print(great_c1[-10:-1])\n",
    "#print(great_c2[-10:-1])\n",
    "#print(great_c3[-10:-1])\n",
    "#print(np.unique(reject_c1))\n",
    "#print(np.unique(reject_c2))\n",
    "#print(np.unique(reject_c3))\n",
    "#print(np.mean(np.asarray(accept_c1)))\n",
    "#print(np.mean(np.asarray(accept_c2)))\n",
    "#print(np.mean(np.asarray(reject_c1)))\n",
    "#print(np.unique(reject_c1))\n",
    "#print(np.mean(np.asarray(accept_ratio)))\n",
    "#print(accept_ratio)\n",
    "# 0.25 ratio: horrible: 130/355 rejected, imperfect: 289/4725 rejected, perfect: 166/1252\n",
    "# 0.3 ratio: h 123/355, i 217/4725, p 108/1252\n",
    "# 0.4 ratio: h 106/355, i 139/4725, p 45/1252\n",
    "# DISCARD ABOVE 0.4: h 106/155, s 15/2297, i 125/2428, p 45/1252\n",
    "# DISCARD ABOVE 0.6: h 86/155, s 4/2297, 89/2428, 12/1252\n",
    "##### DISCARD ABOVE 0.5: h 93/355, s 7/2297, i 101/2428, p 17/1252\n",
    "##### GREAT 0.1: h: 51(rej. 115), s 1681(rej. 215), i 956(rej. 839), p 610(rej. 14)\n",
    "##### GREAT 0.05: h: 42(rej. 70), s 1279(rej. 202), i 795(rej. 587), p 340(rej. 10)\n",
    "# 0.6 ratio: h 86/355, i 92/4725, p 12/1252   (-7, -15, -5)\n",
    "# great 0.1: h 166/355, i 3692/4725, 624/1252\n",
    "# --> h: 62(166) reject dupz, i: 658(3692), p: 11(624)\n",
    "# --> h: 92(166) reject dupz+lays, i: 782(3692), p: 14(624)\n",
    "# --> h: 115(166) dupz+lays+skiplays, i: 1185(3692), p: 37(624)\n",
    "### --> h: 115(166) dupz+lays+bskiplays, i: 1063(3692), p: 14(624)\n",
    "# --> h: 105(166) 2dupz+lays+bskiplays, i: 632(3692), p: 4(624)\n",
    "# great 0.05: h 112/355, i 2863/4725, p 350/1252\n",
    "# great 0.01: h 32/355, i 1007/4725, p 85/1252\n",
    "## mean_c1_reject: horrible: 0.0003 (0.006-0.000002)  imperfect: 0.0005 (0.0087-0.000011)   perfect: 0.000559 (0.0028-0.000027)\n",
    "# horrible: c1 mean:  0.0009475, c2 mean: 0.001010\n",
    "# imperfect: c1 mean: 0.001234,  c2 mean: 0.001295\n",
    "# perfect: c1 mean:   0.0012925, c2 mean: 0.0014275\n",
    "# tracks: 6333, perfect 1252, short 2297, imperfect 2428, horrible 355\n",
    "# Helix1: 8143, perfect 1972, short 1364, imperfect 3547, horrible 1259\n",
    "# Helix2: 7666, perfect 2020, short 1170, imperfect 3522, horrible 953\n",
    "# Helix3: 6717, perfect 1783, short 1589, imperfect 2696, horrible 648\n",
    "# Helix4: 6688, perfect 1547, short 1649, imperfect 2780, horrible 711\n",
    "# Helix5: 6645, perfect 1600, short 1763, imperfect 2781, horrible 500\n",
    "# Helix6: 7436, perfect 1755, short 1716, imperfect 3361, horrible 603\n",
    "# Helix7: 7451, perfect 1795, short 1795, imperfect 3299, horrible 589\n",
    "# Helix8: 7421, perfect 1734, short 1638, imperfect 3302, horrible 746\n",
    "# Helix9: 7414, perfect 1807, short 1699, imperfect 3276, horrible 631\n",
    "# HELIX1 DISCARD ABOVE 0.5: h 605/1259, s 14/1364, i 472/3547, 17/1972\n",
    "# HELIX1 GREAT < 0.1: h 119 (rej. 140), s 814 (rej. 65), i 1457 (rej. 489), 869 (rej. 19)\n",
    "# HELIX2 DISCARD ABOVE 0.5: h 266/953, s 10/1170, i 330/3522, p 19/2020\n",
    "# HELIX2 GREAT < 0.1: h 143 (rej. 145), s 699 (rej. 70), i 1422 (rej. 494), p 911 (rej. 18)\n",
    "# HELIX3 DISCARD ABOVE 0.5: h 97/648, s 5/1589, i 158/2696, p 16/1783\n",
    "# HELIX3 GREAT < 0.1: h 103 (rej. 104), s 978 (rej. 77), i 1222 (rej. 337), p 818 (rej. 9)\n",
    "# HELIX4 DISCARD ABOVE 0.5: h 116/711, s 16/1649, i 179/2780, p 14/1547\n",
    "# HELIX4 GREAT < 0.1: h 109 (rej. 116), s 982 (rej. 84), i 1222 (rej. 340), p 707 (rej. 8)\n",
    "# HELIX5 DISCARD ABOVE 0.5: h 136/500, s 23/1763, i 207/2781, p 14/1600\n",
    "# HELIX5 GREAT < 0.1: h 81 (rej. 87), s 1092 (rej. 93), i 1394 (rej. 305), p 720 (rej. 9)\n",
    "# HELIX6 DISCARD ABOVE 0.5: h 196/603, s 9/1716, i 402/3361, p 21/1755\n",
    "# HELIX6 GREAT < 0.1: h 107 (rej. 120), s 1098 (rej. 140), i 1481 (rej. 532), 778 (rej. 10)\n",
    "# HELIX7 DISCARD ABOVE 0.5: h 174/589, s 9/1795, i 397/3299, p 22/1795\n",
    "# HELIX7 GREAT < 0.1: h 90 (rej. 123), s 1111 (rej. 139), i 1420 (rej. 556), p 797 (rej. 13)\n",
    "# HELIX8 DISCARD ABOVE 0.5: h 263/746, s 6/1638, i 480/3302, p 25/1734\n",
    "# HELIX8 GREAT < 0.1: h 96 (rej. 142), s 1008 (rej. 150), 1408 (rej. 490), p 748 (rej. 7)\n",
    "# HELIX9 DISCARD ABOVE 0.5: h 188/631, s 10/1699, i 414/3276, p 22/1807\n",
    "# HELIX9 GREAT < 0.1: h 101 (rej. 132), s 1062 (rej. 141), i 1405 (rej. 528), p 816 (rej. 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(great_reject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.compare_track_to_truth(6101, helix6, hits, truth)\n",
    "# perfect: 7.2->7.6, 7.4->7.8, 7.6->7.10, 7.10->7.14, 9.6->9.10, 9.10->9.14, 9.8->9.12, 9.8->9.14\n",
    "#Horrible:\n",
    "# 513: ???\n",
    "# 643: ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helix1 = np.copy(labels_helix1)\n",
    "tracks = np.unique(helix1)\n",
    "perfect_tracks = []\n",
    "imperfect_tracks = []\n",
    "for track in tracks:\n",
    "    if track == 0: continue\n",
    "    if is_track_equal_to_truth(track, helix1, hits, truth):\n",
    "        perfect_tracks.append(track)\n",
    "    else:\n",
    "        imperfect_tracks.append(track)\n",
    "\n",
    "print('Total tracks:     ' + str(len(tracks)))\n",
    "print('Perfect tracks:   ' + str(len(perfect_tracks)))\n",
    "print('Imperfect tracks: ' + str(len(imperfect_tracks)))\n",
    "print(imperfect_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "labels_merged = heuristic_merge_tracks_qqq(labels_helix1_filter, labels_helix2_filter, hits, overwrite_limit=6)\n",
    "time2 = time.time()\n",
    "elapsed = time2 - time1\n",
    "print('Elapsed time: ' + str(elapsed))\n",
    "score_one_submission(event_id, hits, labels_merged, truth)\n",
    "#Elapsed time: 0.7021181583404541\n",
    "#Score for event 1000: 0.67459106\n",
    "#Elapsed time: 0.6612889766693115\n",
    "#Score for event 1000: 0.67459106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_helix1_filter = merge.remove_outliers(labels_helix1, hits, print_counts=True)\n",
    "#labels_helix2_filter = merge.remove_outliers(labels_helix2, hits, print_counts=True)\n",
    "#labels_helix1_filter2 = assign_free_hits(labels_helix1_filter, hits)\n",
    "#labels_helix2_filter2 = assign_free_hits(labels_helix2_filter, hits)\n",
    "labels_merged = merge.heuristic_merge_tracks(labels_helix1_filter, labels_helix2_filter, overwrite_limit=3)\n",
    "score_one_submission(event_id, hits, labels_merged, truth)\n",
    "labels_merged2 = free.assign_free_hits(labels_merged, hits)\n",
    "score_one_submission(event_id, hits, labels_merged2, truth)\n",
    "#labels_merged2 = np.copy(labels_merged)\n",
    "#labels_merged2 = assign_free_hits(labels_merged2, hits)\n",
    "#score_one_submission(event_id, hits, labels_merged2, truth)\n",
    "# zr:     0.68239816, 0.68255797, 0.68265559\n",
    "# crs:    0.68239816, 0.68264529, 0.68269281\n",
    "# crs+zr: 0.68239816, 0.68454839, 0.68476117  (rem 2,3,4)\n",
    "# crs+zr: 0.68239816, 0.68396086, 0.68419647  (rem 1,2,3)\n",
    "# crs+zr: 0.68239816, 0.68471627, 0.68479848  (rem 2,3, no short track favoring)\n",
    "# crs+zr: 0.68239816, 0.68432189, 0.68465735, 0.68476141  (rem 1,2,3, no short track favoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_helix1_filter\n",
    "labels = np.copy(labels_merged)\n",
    "tracks = np.unique(labels)\n",
    "straight_tracks = []\n",
    "straight_ids = []\n",
    "for track in tracks:\n",
    "    if track == 0: continue\n",
    "    (is_straight, track_dims) = is_straight_track(track, labels, hits)\n",
    "    if is_straight:\n",
    "        straight_tracks.append(track_dims)\n",
    "        straight_ids.append(track)\n",
    "\n",
    "print('Found: ' + str(len(straight_tracks)))\n",
    "#if len(straight_tracks) > 10:\n",
    "#    draw_prediction_xyz(straight_tracks, straight_tracks)\n",
    "#print('straight tracks: ' + str(straight_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('straight tracks: ' + str(straight_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "straight_ids_equal = []\n",
    "straight_ids_unequal = []\n",
    "for straight_track in straight_ids:\n",
    "    straight_ids_unequal.append(straight_track)\n",
    "    #if matches_straight_truth_track(straight_track, labels, hits, truth):\n",
    "    #    straight_ids_equal.append(straight_track)\n",
    "    #else:\n",
    "    #    straight_ids_unequal.append(straight_track)\n",
    "\n",
    "print('Num equal to ground truth: ' + str(len(straight_ids_equal)))\n",
    "print('Num differences: ' + str(len(straight_ids_unequal)))\n",
    "#print('Differences: ' + str(straight_ids_unequal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_id = 2988\n",
    "lengthen_straight_track(track_id, labels, hits, truth)\n",
    "#labels_straightx = straight_track_extension(track_id, labels, hits)\n",
    "# 21.988119\n",
    "# 21.968767\n",
    "# ??? --> 21.949 --> wrong one :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#TODO! Track: 14066 cannot take hits, more work needed, ixes: [55819, 55821], old tracks: [5119 5174], weak_tracks: [False, False]\n",
    "#Track: 14066, zr_exp: 6.48827600479, assigning dup-z hits: [55756]\n",
    "#TODO! Track: 14066 cannot take hits, more work needed, ixes: [52956, 52958], old tracks: [7720 6432], weak_tracks: [False, False]\n",
    "#Track: 14066, zr_exp: 7.60134005547, assigning dup-z hits: [52927]\n",
    "#Track: 14066, zr_exp: 8.22216129303, assigning hits: [46923 46840] from tracks: [1802 1802]\n",
    "\n",
    "track_id = 22851\n",
    "#graph_my_track(track_id, labels, hits)\n",
    "lengthen_straight_track(track_id, labels, hits, truth)\n",
    "labels_straightx = straight_track_extension(track_id, labels, hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#23004 - one hit, 7951, should be weak\n",
    "#7015 - many hits, strong\n",
    "#8088 - one hit, 10765, should be weak\n",
    "#track_id = 7366\n",
    "track_id = labels[103818]\n",
    "print(track_id)\n",
    "if track_id != 0:\n",
    "    lengthen_straight_track(track_id, labels, hits, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lengthen_straight_track(18, labels, hits, truth)\n",
    "possible_matches = hits.loc[(hits['zr'] > 25.25) & (hits['zr'] < 25.35) & (hits['volume_id'] == 9) & (hits['layer_id'] == 6)]\n",
    "px1 = possible_matches[['x','y', 'z', 'zr', 'volume_id', 'layer_id']]\n",
    "print(px1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_id = 3105\n",
    "ix1 = np.where(labels == track_id)[0]\n",
    "print(ix1)\n",
    "lengthen_straight_track(track_id, labels, hits, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_id = 123\n",
    "ix1 = np.where(labels == track_id)[0]\n",
    "print(ix1)\n",
    "lengthen_straight_track(track_id, labels, hits, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_id = 325\n",
    "ix1 = np.where(labels == track_id)[0]\n",
    "print(ix1)\n",
    "lengthen_straight_track(track_id, labels, hits, truth)\n",
    "#ix1 = np.where(labels == track_id)[0]\n",
    "#print(ix1)\n",
    "#straight_labels = straight_track_extension(track_id, labels, hits)\n",
    "#ix1 = np.where(straight_labels == track_id)[0]\n",
    "#print(ix1)\n",
    "#Duplicatez found on track 2988, removed: [44825, 44759, 42056, 41974]\n",
    "#track: 123 steal_ixes: [], dup-zs: [51149, 51190, 51084, 51099]\n",
    "#--> old tracks: [ 325 3105  325 3105]\n",
    "#               x          y      z         zr  volume_id  layer_id  module_id\n",
    "##51149 -45.120602  47.249100  957.5  14.655781          9         8         96\n",
    "#51190 -47.384998  44.974800  957.5  14.656254          9         8         96\n",
    "##51084 -45.347500  47.436501  962.0  14.659087          9         8         93\n",
    "#51099 -47.596699  45.219700  962.0  14.652879          9         8         93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_id = 7461\n",
    "ix1 = np.where(labels == track_id)[0]\n",
    "print(ix1)\n",
    "lengthen_straight_track(track_id, labels, hits, truth)\n",
    "#Merge tracks [7456, 7436] allowed, matching volume [9] and non-overlapping adjacent layers: [ 8 10], [6]\n",
    "#Track: 7456, assigning hits: [47110 47164 47244]\n",
    "#Track: 7456, assigning hits: [53774] from tracks: [0]\n",
    "#Merge tracks [7461, 7440] allowed, matching volume [9] and non-overlapping adjacent layers: [ 8 10 12 14], [6]\n",
    "#Track: 7461, assigning hits: [47716 47792 47823]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_id = 2988\n",
    "outliers = find_duplicate_z_using_zr(track_id, labels, hits)\n",
    "print('Duplicate-z outliers: ' + str(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_id = 16\n",
    "\n",
    "straight_labels = straight_track_extension(track_id, labels, hits)\n",
    "\n",
    "score_one_submission(event_id, hits, labels, truth)\n",
    "score_one_submission(event_id, hits, straight_labels, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(straight_ids_unequal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_straight = np.copy(labels)\n",
    "straight_id = 123\n",
    "labels_straight = straight_track_extension(straight_id, labels_straight, hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_straight = np.copy(labels_merged)\n",
    "for straight_id in straight_ids_unequal:\n",
    "    labels_straight = straight_track_extension(straight_id, labels_straight, hits)\n",
    "score_one_submission(event_id, hits, labels, truth)\n",
    "score_one_submission(event_id, hits, labels_straight, truth)\n",
    "#Score for event 1029: 0.69792186236579401015234225269523\n",
    "#Score for event 1029: 0.69855311091658223165978824908962\n",
    "# multi-round score:   0.69863787156127243882508537353715\n",
    "# final score:         0.69869633909142048189266915869666\n",
    "# final score 7+9:     0.69918788392731723302375712592038\n",
    "# 7+9 with 1 outlier:  0.69929738679703468129389420937514\n",
    "# 7+9, middle opt:     0.69932251250215871074544793373207\n",
    "# 7+9, 12.4-7.2:       0.69940171659638750156773312482983\n",
    "# 7+9, ++9.14-14.12    0.69959762753955190284216314466903\n",
    "# 7+9, ++9.14-14.12+4% 0.69961411693382824505960115857306\n",
    "# 0.69963199337734927052423472559894\n",
    "#Score for event 1000: 0.68841518499173393053069958114065\n",
    "#Score for event 1000: 0.68934396036442713295855355681852\n",
    "# 1 outlier allowed:   0.68950940891426271139863501957734\n",
    "# 1 outl. all. >5hit:  0.68948735589332810214102664758684\n",
    "# 2 outl. all. >5hit:  0.68953104674629384085449146368774\n",
    "# 3/4 outl. all. >min: 0.68952229106574391970241322269430\n",
    "# Score for event 1000: 0.68953104674629384085449146368774\n",
    "# Allow >= 3 hits:     0.68959137299867523385188405882218\n",
    "# --> no pure 7,9 res. 0.68974421273703834245338839536998\n",
    "## Score for event 1000: 0.68981841935186816172631552035455\n",
    "## Score for event 1000: 0.68985939458400502566348677646602\n",
    "## steal from the weak:  0.68988046467484864798791477369377\n",
    "## +more weak:           0.68988884486548651686632638302399\n",
    "## +more weak:           0.68991445718732746783530274115037"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.copy(labels_straight)\n",
    "tracks = np.unique(labels)\n",
    "for track in tracks:\n",
    "    if track == 0: continue\n",
    "    labels = straight_track_extension(track, labels, hits)\n",
    "score_one_submission(event_id, hits, labels_merged, truth)\n",
    "score_one_submission(event_id, hits, labels, truth)\n",
    "#Score for event 1000: 0.68841518499173393053069958114065\n",
    "#Score for event 1000: 0.68986529899372017737846363161225\n",
    "#Better merge check: Score for event 1000: 0.68998311613039131628966060816310\n",
    "#Aggressive straight tracks, normal for others: 0.69012032082762320150948198715923\n",
    "#++Merge inner layer: 0.69017637968013256788424314436270\n",
    "#++steal from weak:   0.69026576295437469532600971433567\n",
    "#++more weak:         0.69028111102535416598868778237374\n",
    "#++more weak:         0.69034641708780575264370327204233\n",
    "#++steal 2 weak:      0.69038830236949055851880530099152\n",
    "#++more weak:         0.69050782135411337137043119582813\n",
    "#++second round:      0.69058547077857790341681720747147"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = np.copy(labels_merged)\n",
    "tracks = np.unique(labels)\n",
    "for track in tracks:\n",
    "    if track == 0: continue\n",
    "    (is_straight, track_dims) = is_straight_track(track, labels, hits)\n",
    "    # Tracks that appear to be straight are more aggressive when finding\n",
    "    # a hit in the next layer.\n",
    "    if is_straight:\n",
    "        labels = straight_track_extension(track, labels, hits, is_straight)\n",
    "score_one_submission(event_id, hits, labels_merged, truth)\n",
    "score_one_submission(event_id, hits, labels, truth)\n",
    "#Score for event 1000: 0.68841518499173393053069958114065\n",
    "#Score for event 1000: 0.69043357293162865850888465502067\n",
    "#Score for event 1000: 0.69068164998317349567713563374127\n",
    "###\n",
    "#Score for event 1000: 0.68841518499173393053069958114065\n",
    "#Score for event 1000: 0.69042538896130123404759615368675\n",
    "#Score for event 1000: 0.69075573729752981488871910187299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_range(ix, xs, ys, zrs, use_largest_zrdiff=False):\n",
    "    def get_min_max(old_val, diff):\n",
    "        new_val = old_val - diff\n",
    "        if new_val > old_val:\n",
    "            min_val = old_val\n",
    "            max_val = new_val - diff\n",
    "        else:\n",
    "            min_val = new_val - diff\n",
    "            max_val = old_val\n",
    "        return (min_val, new_val, max_val)\n",
    "\n",
    "    # FIXME: Simple for now, expect linear changes\n",
    "    # Should be enhanced to look at trends as well.\n",
    "    xdiffs = np.diff(xs)\n",
    "    ydiffs = np.diff(ys)\n",
    "    zrdiffs = np.diff(zrs)\n",
    "    # The difference in zr values is typically much smaller than the\n",
    "    # difference in x and y values. Use the largest found zr diff,\n",
    "    # instead of the adjacent zr diff (x and y use adjacent values).\n",
    "    best_zrdiff = 0\n",
    "    for iix, zr in enumerate(zrs):\n",
    "        if iix < (len(zrs)-1) and zr != 0 and zrs[iix+1] != 0:\n",
    "            if (zrdiffs[iix] < 0 and zrdiffs[iix] < best_zrdiff) or (zrdiffs[iix] > 0 and zrdiffs[iix] > best_zrdiff):\n",
    "                best_zrdiff = zrdiffs[iix]\n",
    "\n",
    "    if ix < (len(xs) - 1) and ix > 0 and xs[ix-1] != 0 and xs[ix+1] != 0:\n",
    "        # Look at next 2 hits in track to find the expected value.\n",
    "        x_min = min(xs[ix-1], xs[ix+1])\n",
    "        x_max = max(xs[ix-1], xs[ix+1])\n",
    "        x_exp = (x_min + x_max) / 2\n",
    "        y_min = min(ys[ix-1], ys[ix+1])\n",
    "        y_max = max(ys[ix-1], ys[ix+1])\n",
    "        y_exp = (y_min + y_max) / 2\n",
    "        zr_min = min(zrs[ix-1], zrs[ix+1])\n",
    "        zr_max = max(zrs[ix-1], zrs[ix+1])\n",
    "        zr_exp = (zr_min + zr_max) / 2\n",
    "    elif ix < (len(xs) - 2) and xs[ix+1] != 0 and xs[ix+2] != 0:\n",
    "        # Look at next 2 hits in track to find the expected value.\n",
    "        (x_min, x_exp, x_max) = get_min_max(xs[ix+1], xdiffs[ix+1])\n",
    "        (y_min, y_exp, y_max) = get_min_max(ys[ix+1], ydiffs[ix+1])\n",
    "        if use_largest_zrdiff:\n",
    "            # Use best_zrdiff to determine the min/max range, but actual zrdiff\n",
    "            # to select most likely value\n",
    "            (zr_min, zr_exp, zr_max) = get_min_max(zrs[ix+1], best_zrdiff)\n",
    "            (_, zr_exp, _) = get_min_max(zrs[ix+1], zrdiffs[ix+1])\n",
    "        else:\n",
    "            (zr_min, zr_exp, zr_max) = get_min_max(zrs[ix+1], zrdiffs[ix+1])\n",
    "    else:\n",
    "        # Look at previous 2 hits in track to find the expected value.\n",
    "        (x_min, x_exp, x_max) = get_min_max(xs[ix-1], -xdiffs[ix-2])\n",
    "        (y_min, y_exp, y_max) = get_min_max(ys[ix-1], -ydiffs[ix-2])\n",
    "        if use_largest_zrdiff:\n",
    "            (zr_min, zr_exp, zr_max) = get_min_max(zrs[ix-1], -best_zrdiff)\n",
    "            (_, zr_exp, _) = get_min_max(zrs[ix-1], -zrdiffs[ix-2])\n",
    "        else:\n",
    "            (zr_min, zr_exp, zr_max) = get_min_max(zrs[ix-1], -zrdiffs[ix-2])\n",
    "    return (x_min, x_exp, x_max, y_min, y_exp, y_max, zr_min, zr_exp, zr_max)\n",
    "\n",
    "def get_volume_switch_expected_zr_range(zr, factor=0.01):\n",
    "    new_val_1 = zr * (1.00-factor)\n",
    "    new_val_2 = zr * (1.00+factor)\n",
    "    if new_val_1 > zr:\n",
    "        min_val = new_val_2\n",
    "        max_val = new_val_1\n",
    "    else:\n",
    "        min_val = new_val_1\n",
    "        max_val = new_val_2\n",
    "    return (min_val, max_val)\n",
    "\n",
    "def is_weak_track(track, labels, volume, hits):\n",
    "    \"\"\" Consider tracks with few hits (and no consecutive-layer hits) in our target volume\n",
    "    as weak tracks we can steal hits from.\n",
    "    \"\"\"\n",
    "    is_weak = False\n",
    "    tix = np.where(labels == track)[0]\n",
    "    df = hits.loc[tix]\n",
    "    df.sort_values('z')\n",
    "    volumes = df.volume_id.values\n",
    "    layers = df.layer_id.values\n",
    "    has_consecutive_layers = False\n",
    "    volume_layers = []\n",
    "    for ix, vol in enumerate(volumes):\n",
    "        if vol == volume:\n",
    "            volume_layers.append(layers[ix])\n",
    "    if len(volume_layers) <= 4:\n",
    "        uniq_layers = np.unique(volume_layers)\n",
    "        if len(uniq_layers) == 1 or (len(uniq_layers) == 2 and (uniq_layers[1] - uniq_layers[0] > 2)):\n",
    "            is_weak = True\n",
    "\n",
    "    return is_weak\n",
    "\n",
    "def can_merge_tracks(track1, track2, labels, hits):\n",
    "    # FIXME: Can be much smarter, for now, only consider merging both\n",
    "    # tracks if they are both in the same volume, and do not have\n",
    "    # any overlapping layers. Caller should verify that the tracks\n",
    "    # are likely related, i.e. possibly by comparing zr values in\n",
    "    # neighbouring layers to see if they are similar.\n",
    "    merge_valid = False\n",
    "    t1_ix = np.where(labels == track1)[0]\n",
    "    t2_ix = np.where(labels == track2)[0]\n",
    "    df1 = hits.loc[t1_ix]\n",
    "    df2 = hits.loc[t2_ix]\n",
    "    volume1 = np.unique(df1.volume_id.values)\n",
    "    volume2 = np.unique(df2.volume_id.values)\n",
    "    if len(volume1) == 1 and len(volume2) > 1 and volume1[0] in volume2:\n",
    "        df2 = df2.loc[(df2['volume_id'] == volume1[0])]\n",
    "        volume2 = np.unique(df2.volume_id.values)\n",
    "    elif len(volume2) == 1 and len(volume1) > 1 and volume2[0] in volume1:\n",
    "        df1 = df1.loc[(df1['volume_id'] == volume2[0])]\n",
    "        volume1 = np.unique(df1.volume_id.values)\n",
    "    if len(volume1) == 1 and len(volume2) == 1:\n",
    "        if volume1[0] == volume2[0]:\n",
    "            layers1 = np.unique(df1.layer_id.values)\n",
    "            layers2 = np.unique(df2.layer_id.values)\n",
    "            if (layers1[-1] + 2 == layers2[0]) or (layers2[-1] + 2 == layers1[0]):\n",
    "                print('Merge tracks [' + str(track1) + ', ' + str(track2) + '] allowed, matching volume ' + str(volume1) + ' and non-overlapping adjacent layers: ' + str(layers1) + ', ' + str(layers2))\n",
    "                merge_valid = True\n",
    "            elif (len(layers1) == 1 and not layers1[0] in layers2) or (len(layers2) == 1 and not layers2[0] in layers1):\n",
    "                print('Merge tracks [' + str(track1) + ', ' + str(track2) + '] allowed, matching volume ' + str(volume1) + ' and single-layer merge: ' + str(layers1) + ', ' + str(layers2))\n",
    "                merge_valid = True\n",
    "    return (merge_valid, t1_ix, t2_ix)\n",
    "\n",
    "def find_nearest_zrs(dup_ixes, ixes, zs, zrs, ideal_zr, max_zrs=3):\n",
    "    test_zrs = []\n",
    "    test_zs = []\n",
    "    test_ixes = []\n",
    "    for aix, ix in enumerate(ixes):\n",
    "        if ix in dup_ixes:\n",
    "            test_zrs.append(zrs[aix])\n",
    "            test_zs.append(zs[aix])\n",
    "            test_ixes.append(ix)\n",
    "    array = np.asarray(test_zrs)\n",
    "    nearest_zrs_ix = []\n",
    "    while len(test_zrs) > 0:\n",
    "        array = np.asarray(test_zrs)\n",
    "        idx = (np.abs(array - ideal_zr)).argmin()\n",
    "        rem_z_value = test_zs[idx]\n",
    "        nearest_zrs_ix.append(test_ixes[idx])\n",
    "        test_zrs.pop(idx)\n",
    "        test_zs.pop(idx)\n",
    "        test_ixes.pop(idx)\n",
    "        indexes = [i for i,z in enumerate(test_zs) if z == rem_z_value]\n",
    "        if len(nearest_zrs_ix) >= max_zrs:\n",
    "            break\n",
    "        #print(len(test_zrs))\n",
    "        #print(len(test_zs))\n",
    "        #print(len(test_ixes))\n",
    "        #print(len(indexes))\n",
    "        #print(indexes)\n",
    "        for ii in sorted(indexes, reverse=True):\n",
    "            test_zrs.pop(ii)\n",
    "            test_zs.pop(ii)\n",
    "            test_ixes.pop(ii)\n",
    "    return nearest_zrs_ix[0:max_zrs]\n",
    "\n",
    "def select_best_zr_matches(track, labels, ix, xs, ys, zrs, px1, hits, aggressive_zr_estimation):\n",
    "    # FIXME: Can be much smarter...\n",
    "    px1_ixes = px1.index.values\n",
    "    z_values = px1.z.values\n",
    "    zr_values = px1.zr.values\n",
    "    steal_ixes = []\n",
    "    # First, assign any hits that do not contain duplicates\n",
    "    duplicate_zs = []\n",
    "    next_ix_is_dup = False\n",
    "    for iix, z in enumerate(z_values):\n",
    "        if next_ix_is_dup:\n",
    "            duplicate_zs.append(px1_ixes[iix])\n",
    "            next_ix_is_dup = False\n",
    "        elif iix == (len(z_values) - 1) or z != z_values[iix+1]:\n",
    "            steal_ixes.append(px1_ixes[iix])\n",
    "        else:\n",
    "            duplicate_zs.append(px1_ixes[iix])\n",
    "            next_ix_is_dup = True\n",
    "    if len(duplicate_zs) > 0:\n",
    "        old_tracks = labels[duplicate_zs]\n",
    "        #print('track: ' + str(track) + ' steal_ixes: ' + str(steal_ixes) + ', dup-zs: ' + str(duplicate_zs) + ', old tracks: ' + str(old_tracks))\n",
    "        unique_tracks = np.unique(old_tracks)\n",
    "        if len(unique_tracks) == 1:\n",
    "            steal_z_from_other_track = True\n",
    "            (x_min, x_exp, x_max, y_min, y_exp, y_max, zr_min, zr_exp, zr_max) = get_expected_range(ix, xs, ys, zrs, aggressive_zr_estimation)\n",
    "            steal_ixes = find_nearest_zrs(duplicate_zs, px1_ixes, z_values, zr_values, zr_exp)\n",
    "            print('Track: ' + str(track) + ', stole ixes: ' + str(steal_ixes) + ' from all dup zrs: ' + str(duplicate_zs) + ', from tracks: ' + str(old_tracks))\n",
    "        elif len(unique_tracks) == 2:\n",
    "            # There are 2 separate tracks that the z values are distributed to.\n",
    "            # First, check if we can merge.\n",
    "            can_merge1 = False\n",
    "            can_merge2 = False\n",
    "            if unique_tracks[0] != 0:\n",
    "                (can_merge1, m1_t1_ix, m1_t2_ix) = can_merge_tracks(track, unique_tracks[0], labels, hits)\n",
    "            (can_merge2, m2_t1_ix, m2_t2_ix) = can_merge_tracks(track, unique_tracks[1], labels, hits)\n",
    "            if can_merge1 and not can_merge2:\n",
    "                #print('Track: ' + str(track) + ' can be merged with track1: ' + str(unique_tracks[0]))\n",
    "                steal_ixes = m1_t2_ix\n",
    "            elif can_merge2 and not can_merge1:\n",
    "                #print('Track: ' + str(track) + ' can be merged with track2: ' + str(unique_tracks[1]))\n",
    "                steal_ixes = m2_t2_ix\n",
    "            else:\n",
    "                if unique_tracks[0] == 0:\n",
    "                    # just take the un-assigned hits, less risk of taking the wrong ones.\n",
    "                    print('Track: ' + str(track) + ' taking un-assigned hits, ixes: ' + str(duplicate_zs) + ', old tracks: ' + str(old_tracks))\n",
    "                    for dupz in duplicate_zs:\n",
    "                        if labels[dupz] == 0:\n",
    "                            steal_ixes.append(dupz)\n",
    "                else:\n",
    "                    # See if there are up to 2 weak/invalid tracks we can steal from.\n",
    "                    track_is_weak = []\n",
    "                    for trk in unique_tracks:\n",
    "                        track_is_weak.append(is_weak_track(trk, labels, px1.volume_id.values[0], hits))\n",
    "                    weak_ixes = np.where(np.asarray(track_is_weak) == True)[0]\n",
    "                    if len(weak_ixes) == 1 or len(weak_ixes) == 2:\n",
    "                        weak_tracks = unique_tracks[weak_ixes]\n",
    "                        for dupz in duplicate_zs:\n",
    "                            if labels[dupz] in weak_tracks:\n",
    "                                steal_ixes.append(dupz)\n",
    "                        print('Track: ' + str(track) + ' taking hits from weak tracks: ' + str(weak_tracks) + ', hits: ' + str(steal_ixes))\n",
    "                    else:\n",
    "                        print('TODO! Track: ' + str(track) + ' cannot take hits, more work needed, ixes: ' + str(duplicate_zs) + ', old tracks: ' + str(old_tracks) + ', weak_tracks: ' + str(track_is_weak))\n",
    "        elif unique_tracks[0] == 0:\n",
    "            # just take the un-assigned hits, less risk of taking the wrong ones.\n",
    "            print('Track: ' + str(track) + ' many unique tracks, only taking un-assigned hits, ixes: ' + str(duplicate_zs) + ', old tracks: ' + str(old_tracks))\n",
    "            test_steal_ixes = steal_ixes\n",
    "            steal_ixes = []\n",
    "            for tsix in test_steal_ixes:\n",
    "                if labels[tsix] == 0:\n",
    "                    steal_ixes.append(tsix)\n",
    "            for dupz in duplicate_zs:\n",
    "                if labels[dupz] == 0:\n",
    "                    steal_ixes.append(dupz)\n",
    "        else:\n",
    "            # See if there are up to 2 weak/invalid tracks we can steal from.\n",
    "            track_is_weak = []\n",
    "            for trk in unique_tracks:\n",
    "                track_is_weak.append(is_weak_track(trk, labels, px1.volume_id.values[0], hits))\n",
    "            weak_ixes = np.where(np.asarray(track_is_weak) == True)[0]\n",
    "            if len(weak_ixes) == 1 or len(weak_ixes) == 2:\n",
    "                weak_tracks = unique_tracks[weak_ixes]\n",
    "                for dupz in duplicate_zs:\n",
    "                    if labels[dupz] in weak_tracks:\n",
    "                        steal_ixes.append(dupz)\n",
    "                print('Track: ' + str(track) + ' taking hits from weak tracks: ' + str(weak_tracks) + ', hits: ' + str(steal_ixes))\n",
    "            else:\n",
    "                print('TODO! Track: ' + str(track) + ', too many tracks to steal duplicate zs from, ignoring dups: ' + str(duplicate_zs) + ', old tracks: ' + str(old_tracks) + ', weak_tracks: ' + str(track_is_weak))\n",
    "\n",
    "    #print(px1)\n",
    "    return steal_ixes\n",
    "\n",
    "def generate_zr_layer_data(x, y, zr, layer, lmap):\n",
    "    xs = [0,0,0,0,0,0,0]\n",
    "    ys = [0,0,0,0,0,0,0]\n",
    "    zrs = [0,0,0,0,0,0,0]\n",
    "    counts = [0,0,0,0,0,0,0]\n",
    "    for ix, l in enumerate(layer):\n",
    "        aix = lmap[l]\n",
    "        counts[aix] = counts[aix] + 1\n",
    "        xs[aix] = xs[aix] + x[ix]\n",
    "        ys[aix] = ys[aix] + y[ix]\n",
    "        zrs[aix] = zrs[aix] + zr[ix]\n",
    "    for ix, count in enumerate(counts):\n",
    "        if count != 0:\n",
    "            xs[ix] = xs[ix] / count\n",
    "            ys[ix] = ys[ix] / count\n",
    "            zrs[ix] = zrs[ix] / count\n",
    "    return (xs, ys, zrs, counts)\n",
    "\n",
    "def one_round_straight_track_extension(track, labels, hits, aggressive_zr_estimation):\n",
    "    more_rounds_possible = False\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "    if len(hit_ix) == 0:\n",
    "        return (more_rounds_possible, labels)\n",
    "    df = hits.loc[hit_ix]\n",
    "    df = df.loc[(df['volume_id'] == 7) | (df['volume_id'] == 7) | (df['volume_id'] == 9)]\n",
    "    msg = 'Track: ' + str(track) + ', '\n",
    "    if not np.all(df.volume_id.values == 7) and not np.all(df.volume_id.values == 9) and not np.all(df.volume_id.values == 13):\n",
    "        # FIXME: Future improvement, handle other volumes, and handle\n",
    "        # tracks that span volumes.\n",
    "        #print(msg + 'Can only lengthen straight tracks in volume 9, found: ' + str(df.volume_id.values))\n",
    "        return (more_rounds_possible, labels)\n",
    "    df = df.sort_values('z')\n",
    "    x,y,zr = df[['x', 'y', 'zr']].values.astype(np.float32).T\n",
    "    volume,layer = df[['volume_id', 'layer_id' ]].values.T\n",
    "    #  indexes:  [2->0,4->1,6->2,8->3,10->4,12->5,14->6]\n",
    "    lmap = [0,0,0,0,1,0,2,0,3,0,4,0,5,0,6]\n",
    "    all_layers = [2,4,6,8,10,12,14]\n",
    "    uniq_layers = np.unique(layer)\n",
    "    uniq_volumes = np.unique(volume)\n",
    "    if len(uniq_volumes) > 1:\n",
    "        # FIXME: Future improvement, handle tracks across volumes\n",
    "        #print(msg + 'All hits must be in same volume, volumes found: ' + str(uniq_volumes))\n",
    "        return (more_rounds_possible, labels)\n",
    "    elif np.array_equal(all_layers, uniq_layers):\n",
    "        # FIXME: Future improvement, we can have multiple hits per layer,\n",
    "        # check if we are missing some hits. Hits within the same layer\n",
    "        # should have very small deltas\n",
    "        #print(msg + 'All layers already have at least one hit')\n",
    "        return (more_rounds_possible, labels)\n",
    "    elif len(uniq_layers) == 1:\n",
    "        #print(msg + 'Only one layer defined, unable to determine trends for extension')\n",
    "        return (more_rounds_possible, labels)\n",
    "\n",
    "    (xs, ys, zrs, counts) = generate_zr_layer_data(x, y, zr, layer, lmap)\n",
    "    #print(msg + 'xs: ' + str(xs))\n",
    "    #print(msg + 'ys: ' + str(ys))\n",
    "    #print(msg + 'zrs: ' + str(zrs))\n",
    "    for ix, l in enumerate(all_layers):\n",
    "        if xs[ix] == 0:\n",
    "            if (ix >= 2 and xs[ix-1] != 0 and xs[ix-2] != 0) or (ix < (len(xs) - 2) and xs[ix+1] != 0 and xs[ix+2] != 0):\n",
    "                (x_min, x_exp, x_max, y_min, y_exp, y_max, zr_min, zr_exp, zr_max) = get_expected_range(ix, xs, ys, zrs, aggressive_zr_estimation)\n",
    "                # DO IT!\n",
    "                #print('x: ' + str(x_min) + ', ' + str(x_max) + ', y: ' + str(y_min) + ', ' + str(y_max) + ', zr: ' + str(zr_min) + ', ' + str(zr_max))\n",
    "                possible_matches = hits.loc[(hits['y'] > y_min) & (hits['y'] < y_max) & (hits['x'] > x_min) & (hits['x'] < x_max) & (hits['zr'] > zr_min) & (hits['zr'] < zr_max) & (hits['volume_id'] == uniq_volumes[0]) & (hits['layer_id'] == l)]\n",
    "                possible_matches = possible_matches.sort_values('z')\n",
    "                px1 = possible_matches[['x','y', 'z', 'zr', 'volume_id', 'layer_id', 'module_id']]\n",
    "                msg2 = msg + 'zr_exp: ' + str(zr_exp) + ', '\n",
    "                if len(px1) >= 2 and len(np.unique(px1.z.values)) < len(px1) and len(np.unique(px1.z.values)) <= 3:\n",
    "                    steal_ixs = select_best_zr_matches(track, labels, ix, xs, ys, zrs, px1, hits, aggressive_zr_estimation)\n",
    "                    if len(steal_ixs) > 0:\n",
    "                        # Assign hits!\n",
    "                        #print(steal_ixs)\n",
    "                        #print(labels[steal_ixs])\n",
    "                        print(msg2 + 'assigning dup-z hits: ' + str(steal_ixs))\n",
    "                        labels[steal_ixs] = track\n",
    "                        more_rounds_possible = True\n",
    "                        #print(labels[steal_ixs])\n",
    "                        #print(px1)\n",
    "                elif len(px1) >= 1 and len(px1) <= 3:\n",
    "                    #(can_merge, t1_ix, t2_ix) = can_merge_tracks(track, unique_tracks[0], labels, hits)\n",
    "                    # Assign hits!\n",
    "                    steal_ixs = px1.index.values\n",
    "                    steal_from_tracks = np.unique(labels[steal_ixs])\n",
    "                    can_merge = False\n",
    "                    if len(steal_from_tracks) == 1:\n",
    "                        (can_merge, t1_ix, t2_ix) = can_merge_tracks(track, steal_from_tracks[0], labels, hits)\n",
    "                    if can_merge:\n",
    "                        print(msg2 + 'assigning hits: ' + str(t2_ix))\n",
    "                        labels[t2_ix] = track\n",
    "                    else:\n",
    "                        #print(steal_ixs)\n",
    "                        #print(labels[steal_ixs])\n",
    "                        print(msg2 + 'assigning hits: ' + str(steal_ixs) + ' from tracks: ' + str(labels[steal_ixs]))\n",
    "                        labels[steal_ixs] = track\n",
    "                        #print(labels[steal_ixs])\n",
    "                    more_rounds_possible = True\n",
    "                    #print(px1)\n",
    "                elif len(px1) > 0:\n",
    "                    match_ixs = px1.index.values\n",
    "                    # See if there are up to 2 weak/invalid tracks we can steal from.\n",
    "                    unique_tracks = np.unique(labels[match_ixs])\n",
    "                    track_is_weak = []\n",
    "                    for trk in unique_tracks:\n",
    "                        track_is_weak.append(is_weak_track(trk, labels, px1.volume_id.values[0], hits))\n",
    "                    weak_ixes = np.where(np.asarray(track_is_weak) == True)[0]\n",
    "                    if len(weak_ixes) == 1 or len(weak_ixes) == 2:\n",
    "                        weak_tracks = unique_tracks[weak_ixes]\n",
    "                        steal_ixs = []\n",
    "                        for mix in match_ixs:\n",
    "                            if labels[mix] in weak_tracks:\n",
    "                                steal_ixs.append(mix)\n",
    "                        labels[steal_ixs] = track\n",
    "                        more_rounds_possible = True\n",
    "                        print(msg2 + 'assigning hits: ' + str(steal_ixs) + ' stolen from weak tracks: ' + str(weak_tracks))\n",
    "                    else:\n",
    "                        print(msg2 + str(len(px1)) + ' possible matches')\n",
    "                        print(msg2 + str(match_ixs))\n",
    "                        print(msg2 + str(labels[match_ixs]))\n",
    "\n",
    "    return (more_rounds_possible, labels)\n",
    "\n",
    "def cleanse_straight_track(track, labels, hits):\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "    if len(hit_ix) == 0:\n",
    "        return labels\n",
    "    df = hits.loc[hit_ix]\n",
    "    msg = 'Track: ' + str(track) + ', '\n",
    "    if not np.all(df.volume_id.values == 12) and not np.all(df.volume_id.values == 8):\n",
    "        # FIXME: Future improvement, handle other volumes, and handle\n",
    "        # tracks that span volumes.\n",
    "        #print(msg + 'Can only lengthen straight tracks in volume 9, found: ' + str(df.volume_id.values))\n",
    "        return (labels)\n",
    "    df = df.sort_values('z')\n",
    "    hit_ix2 = df.index.values\n",
    "    x,y,zr = df[['x', 'y', 'zr']].values.astype(np.float32).T\n",
    "    volume,layer = df[['volume_id', 'layer_id' ]].values.T\n",
    "    #  indexes:  [2->0,4->1,6->2,8->3,10->4,12->5,14->6]\n",
    "    lmap = [0,0,0,0,1,0,2,0,3,0,4,0,5,0,6]\n",
    "    all_layers = [2,4,6,8,10,12,14]\n",
    "    uniq_layers = np.unique(layer)\n",
    "    uniq_volumes = np.unique(volume)\n",
    "    xs = [0,0,0,0,0,0,0]\n",
    "    ys = [0,0,0,0,0,0,0]\n",
    "    zrs = [0,0,0,0,0,0,0]\n",
    "    counts = [0,0,0,0,0,0,0]\n",
    "    # FIXME: LIAM: Within a single layer, only allow 0.5% variation?\n",
    "    for ix, l in enumerate(layer):\n",
    "        aix = lmap[l]\n",
    "        counts[aix] = counts[aix] + 1\n",
    "        xs[aix] = xs[aix] + x[ix]\n",
    "        ys[aix] = ys[aix] + y[ix]\n",
    "        zrs[aix] = zrs[aix] + zr[ix]\n",
    "    for ix, count in enumerate(counts):\n",
    "        if count != 0:\n",
    "            xs[ix] = xs[ix] / count\n",
    "            ys[ix] = ys[ix] / count\n",
    "            zrs[ix] = zrs[ix] / count\n",
    "\n",
    "    # DOES NOT WORK! \n",
    "    #if len(uniq_layers) == 3:\n",
    "    #    # Favour keeping beginning of track, cut out high end if it looks wrong\n",
    "    #    # sample zrs: 6.16, 6.22, 6.05\n",
    "    #    if zrs[1] > zrs[0] and zrs[2] < (0.995*zrs[1]):\n",
    "    #        print(msg + ' is a positive possible cleansing target')\n",
    "    #    elif zrs[1] < zrs[0] and zrs[2] > (0.995*zrs[1]):\n",
    "    #        print(msg + ' is a negative possible cleansing target')\n",
    "\n",
    "    return labels\n",
    "\n",
    "def merge_with_other_volumes(track, labels, hits):\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "    if len(hit_ix) == 0:\n",
    "        return labels\n",
    "    df = hits.loc[hit_ix]\n",
    "    msg = 'Track: ' + str(track) + ', '\n",
    "    if not np.all(df.volume_id.values == 7) and not np.all(df.volume_id.values == 9):\n",
    "        # FIXME: Future improvement, handle other volumes, and handle\n",
    "        # tracks that span volumes.\n",
    "        #print(msg + 'Can only lengthen straight tracks in volume 9, found: ' + str(df.volume_id.values))\n",
    "        return (labels)\n",
    "    df = df.sort_values('z')\n",
    "    x,y,zr = df[['x', 'y', 'zr']].values.astype(np.float32).T\n",
    "    volume,layer = df[['volume_id', 'layer_id' ]].values.T\n",
    "    #  indexes:  [2->0,4->1,6->2,8->3,10->4,12->5,14->6]\n",
    "    lmap = [0,0,0,0,1,0,2,0,3,0,4,0,5,0,6]\n",
    "    all_layers = [2,4,6,8,10,12,14]\n",
    "    uniq_layers = np.unique(layer)\n",
    "    uniq_volumes = np.unique(volume)\n",
    "    if len(uniq_volumes) > 1:\n",
    "        # FIXME: Future improvement, handle tracks across volumes\n",
    "        #print(msg + 'All hits must be in same volume, volumes found: ' + str(uniq_volumes))\n",
    "        return (labels)\n",
    "    elif len(uniq_layers) == 1:\n",
    "        #print(msg + 'Only one layer defined, unable to determine trends for extension')\n",
    "        return (labels)\n",
    "\n",
    "    find_zr_ix = 0\n",
    "    attempt_vol12_merge = False\n",
    "    print_debug = False\n",
    "    (xs, ys, zrs, counts) = generate_zr_layer_data(x, y, zr, layer, lmap)\n",
    "    if uniq_volumes[0] == 7 and zrs[0] != 0 and zrs[1] != 0:\n",
    "        #print(msg + 'searching for extension from 7.2 down to 12.4')\n",
    "        xs.insert(0,0)\n",
    "        ys.insert(0,0)\n",
    "        zrs.insert(0,0)\n",
    "        counts.insert(0,0)\n",
    "        attempt_vol12_merge = True\n",
    "        target_layer_id = 4\n",
    "    elif uniq_volumes[0] == 7 and zrs[1] != 0 and zrs[2] != 0:\n",
    "        #print(msg + 'searching for extension from 7.4 down to 12.6)\n",
    "        attempt_vol12_merge = True\n",
    "        target_layer_id = 6\n",
    "        #print_debug = True\n",
    "\n",
    "    if attempt_vol12_merge:\n",
    "        (x_min, x_exp, x_max, y_min, y_exp, y_max, zr_min, zr_exp, zr_max) = get_expected_range(find_zr_ix, xs, ys, zrs, use_largest_zrdiff=False)\n",
    "        #print('x: ' + str(x_min) + ', ' + str(x_max) + ', y: ' + str(y_min) + ', ' + str(y_max) + ', zr: ' + str(zr_min) + ', ' + str(zr_max))\n",
    "        #possible_matches = hits.loc[(hits['y'] > y_min) & (hits['y'] < y_max) & (hits['x'] > x_min) & (hits['x'] < x_max) & (hits['zr'] > zr_min) & (hits['zr'] < zr_max) & (hits['volume_id'] == 12) & (hits['layer_id'] == 4)]\n",
    "        (zr_min, zr_max) = get_volume_switch_expected_zr_range(zrs[1], factor=0.02)\n",
    "        if xs[1] > xs[2] and ys[1] > ys[2]:\n",
    "            possible_matches = hits.loc[(hits['x'] > xs[1]) & (hits['y'] > ys[1]) & (hits['zr'] > zr_min) & (hits['zr'] < zr_max) & (hits['volume_id'] == 12) & (hits['layer_id'] == target_layer_id)]\n",
    "        elif xs[1] > xs[2] and ys[1] < ys[2]:\n",
    "            possible_matches = hits.loc[(hits['x'] > xs[1]) & (hits['y'] < ys[1]) & (hits['zr'] > zr_min) & (hits['zr'] < zr_max) & (hits['volume_id'] == 12) & (hits['layer_id'] == target_layer_id)]\n",
    "        elif xs[1] < xs[2] and ys[1] > ys[2]:\n",
    "            possible_matches = hits.loc[(hits['x'] < xs[1]) & (hits['y'] > ys[1]) & (hits['zr'] > zr_min) & (hits['zr'] < zr_max) & (hits['volume_id'] == 12) & (hits['layer_id'] == target_layer_id)]\n",
    "        else:\n",
    "            possible_matches = hits.loc[(hits['x'] < xs[1]) & (hits['y'] < ys[1]) & (hits['zr'] > zr_min) & (hits['zr'] < zr_max) & (hits['volume_id'] == 12) & (hits['layer_id'] == target_layer_id)]\n",
    "\n",
    "        possible_matches = possible_matches.sort_values('z')\n",
    "        px1 = possible_matches[['x','y', 'z', 'zr', 'volume_id', 'layer_id', 'module_id']]\n",
    "        msg2 = 'LIAM: ' + msg + 'matches: ' + str(len(px1)) + ', zr_min: ' + str(zr_min) + ', zr_exp: ' + str(zr_exp) + ', zr_max: ' + str(zr_max) + ', '\n",
    "        if print_debug: print(msg2 + 'x_min: ' + str(x_min) + ', y_min: ' + str(y_min))\n",
    "        #print(msg + 'searching for extension from 7.2 down to 12.4, targets: ' + str(unique_tracks))\n",
    "        if len(px1) > 0 and len(px1) <= 15:\n",
    "            old_tracks = labels[px1.index.values]\n",
    "            #print('track: ' + str(track) + ' steal_ixes: ' + str(steal_ixes) + ', dup-zs: ' + str(duplicate_zs) + ', old tracks: ' + str(old_tracks))\n",
    "            unique_tracks = np.unique(old_tracks)\n",
    "            if print_debug: print(msg2 + 'searching for extension from 7.2 down to 12.4, targets: ' + str(unique_tracks))\n",
    "            steal_ixes_track0 = []\n",
    "            steal_ixes_merge = []\n",
    "            for utrk in unique_tracks:\n",
    "                if utrk == 0:\n",
    "                    utrk_hits_ix = []\n",
    "                    for iix in px1.index.values:\n",
    "                        if labels[iix] == 0:\n",
    "                            utrk_hits_ix.append(iix)\n",
    "                else:\n",
    "                    utrk_hits_ix = np.where(labels == utrk)[0]\n",
    "                utrk_hits = hits.loc[utrk_hits_ix]\n",
    "                if utrk == 0:\n",
    "                    px2 = utrk_hits[['x','y', 'z', 'zr', 'volume_id', 'layer_id']]\n",
    "                    if print_debug: print(px2)\n",
    "                    px2 = px2.sort_values('z')\n",
    "                    dup_ixes = px2.index.values\n",
    "                    ixes = dup_ixes\n",
    "                    target_zs = px2.z.values\n",
    "                    target_zrs = px2.zr.values\n",
    "                    ideal_zr = zr_exp\n",
    "                    steal_ixes_track0 = find_nearest_zrs(dup_ixes, ixes, target_zs, target_zrs, ideal_zr, max_zrs=1)\n",
    "                    if print_debug: print(msg2 + 'can steal hit: ' + str(steal_ixes_track0))\n",
    "                else:\n",
    "                    all_target_volumes = utrk_hits.volume_id.values\n",
    "                    uniq_target_volumes = np.unique(all_target_volumes)\n",
    "                    all_target_layers = utrk_hits.layer_id.values\n",
    "                    all_target_indexes = utrk_hits.index.values\n",
    "                    if len(uniq_target_volumes) == 1 and np.unique(all_target_layers)[-1] <= 6:\n",
    "                        if print_debug: print(msg2 + 'can steal track ' + str(utrk))\n",
    "                        #print(utrk_hits)\n",
    "                        steal_ixes_merge = all_target_indexes\n",
    "                        break\n",
    "                    else:\n",
    "                        if print_debug: print(msg2 + 'no perfect match for track ' + str(utrk) + ', running more checks....')\n",
    "                        count_vol7 = 0\n",
    "                        max_vol7_layer = 0\n",
    "                        for iix, v in enumerate(all_target_volumes):\n",
    "                            if v == 12:\n",
    "                                steal_ixes_merge.append(all_target_indexes[iix])\n",
    "                            elif v == 7:\n",
    "                                count_vol7 = count_vol7 + 1\n",
    "                                max_vol7_layer = max(max_vol7_layer, all_target_layers[iix])\n",
    "                        if count_vol7 <= 6 and max_vol7_layer <= 4:\n",
    "                            if print_debug: print(msg2 + 'short vol7 track ' + str(utrk) + ', can steal vol 12 hits.')\n",
    "                            break\n",
    "                        else:\n",
    "                            if print_debug: print(msg2 + 'too many vol7 hits in track ' + str(utrk) + ', ' + str(count_vol7) + ', ' + str(max_vol7_layer))\n",
    "                            steal_ixes_merge = []\n",
    "                            \n",
    "            # FIXME: Can potentially make this smarter if we find multiple\n",
    "            # tracks we can merge with, to only merge with the one closest\n",
    "            # to our ideal zr value. For now, just merge with first track\n",
    "            # we find that seems like a candidate\n",
    "            if len(steal_ixes_merge) > 0:\n",
    "                labels[steal_ixes_merge] = track\n",
    "            elif len(steal_ixes_track0) > 0:\n",
    "                labels[steal_ixes_track0] = track\n",
    "\n",
    "    find_zr_ix = len(zrs)\n",
    "    attempt_volnext_merge = False\n",
    "    print_debug = False\n",
    "    factor = 0.04\n",
    "    if ((uniq_volumes[0] == 7) or (uniq_volumes[0] == 9)) and zrs[-1] != 0 and zrs[-2] != 0:\n",
    "        #print(msg + 'searching for extension from 7.14 up to 8.2')\n",
    "        xs.append(0)\n",
    "        ys.append(0)\n",
    "        zrs.append(0)\n",
    "        counts.append(0)\n",
    "        attempt_volnext_merge = True\n",
    "        #print_debug = True\n",
    "        if uniq_volumes[0] == 7:\n",
    "            target_volume_id = 8\n",
    "            target_layer_id = 2\n",
    "        else:\n",
    "            target_volume_id = 14\n",
    "            target_layer_id = 12\n",
    "\n",
    "    if attempt_volnext_merge:\n",
    "        (x_min, x_exp, x_max, y_min, y_exp, y_max, zr_min, zr_exp, zr_max) = get_expected_range(find_zr_ix, xs, ys, zrs, use_largest_zrdiff=False)\n",
    "        #print('x: ' + str(x_min) + ', ' + str(x_max) + ', y: ' + str(y_min) + ', ' + str(y_max) + ', zr: ' + str(zr_min) + ', ' + str(zr_max))\n",
    "        #possible_matches = hits.loc[(hits['y'] > y_min) & (hits['y'] < y_max) & (hits['x'] > x_min) & (hits['x'] < x_max) & (hits['zr'] > zr_min) & (hits['zr'] < zr_max) & (hits['volume_id'] == 12) & (hits['layer_id'] == 4)]\n",
    "        (zr_min, zr_max) = get_volume_switch_expected_zr_range(zrs[-2], factor=factor)\n",
    "        if xs[-2] > xs[-3] and ys[-2] > ys[-3]:\n",
    "            possible_matches = hits.loc[(hits['x'] > xs[-2]) & (hits['y'] > ys[-2]) & (hits['zr'] > zr_min) & (hits['zr'] < zr_max) & (hits['volume_id'] == target_volume_id) & (hits['layer_id'] == target_layer_id)]\n",
    "        elif xs[-2] > xs[-3] and ys[-2] < ys[-3]:\n",
    "            possible_matches = hits.loc[(hits['x'] > xs[-2]) & (hits['y'] < ys[-2]) & (hits['zr'] > zr_min) & (hits['zr'] < zr_max) & (hits['volume_id'] == target_volume_id) & (hits['layer_id'] == target_layer_id)]\n",
    "        elif xs[-2] < xs[-3] and ys[-2] > ys[-3]:\n",
    "            possible_matches = hits.loc[(hits['x'] < xs[-2]) & (hits['y'] > ys[-2]) & (hits['zr'] > zr_min) & (hits['zr'] < zr_max) & (hits['volume_id'] == target_volume_id) & (hits['layer_id'] == target_layer_id)]\n",
    "        else:\n",
    "            possible_matches = hits.loc[(hits['x'] < xs[-2]) & (hits['y'] < ys[-2]) & (hits['zr'] > zr_min) & (hits['zr'] < zr_max) & (hits['volume_id'] == target_volume_id) & (hits['layer_id'] == target_layer_id)]\n",
    "\n",
    "        possible_matches = possible_matches.sort_values('z')\n",
    "        px1 = possible_matches[['x','y', 'z', 'zr', 'volume_id', 'layer_id', 'module_id']]\n",
    "        msg2 = 'LIAM5: ' + msg + 'matches: ' + str(len(px1)) + ', zr_min: ' + str(zr_min) + ', zr_exp: ' + str(zr_exp) + ', zr_max: ' + str(zr_max) + ', '\n",
    "        if print_debug: print(msg2 + 'x_min: ' + str(x_min) + ', y_min: ' + str(y_min))\n",
    "        #print(msg + 'searching for extension from 7.2 down to 12.4, targets: ' + str(unique_tracks))\n",
    "        if len(px1) > 0 and len(px1) <= 15:\n",
    "            old_tracks = labels[px1.index.values]\n",
    "            #print('track: ' + str(track) + ' steal_ixes: ' + str(steal_ixes) + ', dup-zs: ' + str(duplicate_zs) + ', old tracks: ' + str(old_tracks))\n",
    "            unique_tracks = np.unique(old_tracks)\n",
    "            if print_debug: print(msg2 + 'searching for extension from ' + str(uniq_volumes[0]) + '.14 up to ' + str(target_volume_id) + '.' + str(target_layer_id) + ', targets: ' + str(unique_tracks))\n",
    "            if print_debug: print(msg2 + 'searching for extension from ' + str(uniq_volumes[0]) + '.14 up to ' + str(target_volume_id) + '.' + str(target_layer_id) + ', targets: ' + str(px1.index.values))\n",
    "            steal_ixes_track0 = []\n",
    "            steal_ixes_merge = []\n",
    "            for utrk in unique_tracks:\n",
    "                if utrk == 0:\n",
    "                    utrk_hits_ix = []\n",
    "                    for iix in px1.index.values:\n",
    "                        if labels[iix] == 0:\n",
    "                            utrk_hits_ix.append(iix)\n",
    "                else:\n",
    "                    utrk_hits_ix = np.where(labels == utrk)[0]\n",
    "                utrk_hits = hits.loc[utrk_hits_ix]\n",
    "                if utrk == 0:\n",
    "                    px2 = utrk_hits[['x','y', 'z', 'zr', 'volume_id', 'layer_id']]\n",
    "                    if print_debug: print(px2)\n",
    "                    px2 = px2.sort_values('z')\n",
    "                    dup_ixes = px2.index.values\n",
    "                    ixes = dup_ixes\n",
    "                    target_zs = px2.z.values\n",
    "                    target_zrs = px2.zr.values\n",
    "                    ideal_zr = zr_exp\n",
    "                    steal_ixes_track0 = find_nearest_zrs(dup_ixes, ixes, target_zs, target_zrs, ideal_zr, max_zrs=2)\n",
    "                    if print_debug: print(msg2 + 'can steal hit: ' + str(steal_ixes_track0))\n",
    "                else:\n",
    "                    all_target_volumes = utrk_hits.volume_id.values\n",
    "                    uniq_target_volumes = np.unique(all_target_volumes)\n",
    "                    all_target_layers = utrk_hits.layer_id.values\n",
    "                    all_target_indexes = utrk_hits.index.values\n",
    "                    if len(uniq_target_volumes) == 1 and len(np.unique(all_target_layers)) <= 2:\n",
    "                        if print_debug: print(msg2 + 'can steal track ' + str(utrk))\n",
    "                        #print(utrk_hits)\n",
    "                        steal_ixes_merge = all_target_indexes\n",
    "                        break\n",
    "                    else:\n",
    "                        if print_debug: print(msg2 + 'no perfect match for track ' + str(utrk) + ', running more checks....')\n",
    "                        count_my_volume = 0\n",
    "                        max_my_volume_layer = 0\n",
    "                        for iix, v in enumerate(all_target_volumes):\n",
    "                            if v == target_volume_id:\n",
    "                                steal_ixes_merge.append(all_target_indexes[iix])\n",
    "                            elif v == uniq_volumes[0]:\n",
    "                                count_my_volume = count_my_volume + 1\n",
    "                                max_my_volume_layer = max(max_my_volume_layer, all_target_layers[iix])\n",
    "                        if count_my_volume <= 4: # ???  and max_vol7_layer <= 4:\n",
    "                            if print_debug: print(msg2 + 'short vol7 track ' + str(utrk) + ', can steal vol 12 hits.')\n",
    "                            break\n",
    "                        else:\n",
    "                            if print_debug: print(msg2 + 'too many vol7 hits in track ' + str(utrk) + ', ' + str(count_my_volume) + ', ' + str(max_my_volume_layer))\n",
    "                            steal_ixes_merge = []\n",
    "                            \n",
    "            # FIXME: Can potentially make this smarter if we find multiple\n",
    "            # tracks we can merge with, to only merge with the one closest\n",
    "            # to our ideal zr value. For now, just merge with first track\n",
    "            # we find that seems like a candidate\n",
    "            if len(steal_ixes_merge) > 0:\n",
    "                labels[steal_ixes_merge] = track\n",
    "            elif len(steal_ixes_track0) > 0:\n",
    "                labels[steal_ixes_track0] = track\n",
    "\n",
    "\n",
    "\n",
    "    return labels\n",
    "\n",
    "def straight_track_extension(track, labels, hits, aggressive_zr_estimation):\n",
    "    labels = np.copy(labels)\n",
    "    #labels = cleanse_straight_track(track, labels, hits)\n",
    "    more_rounds = True\n",
    "    while more_rounds:\n",
    "        (more_rounds, labels) = one_round_straight_track_extension(track, labels, hits, aggressive_zr_estimation)\n",
    "    labels = merge_with_other_volumes(track, labels, hits)\n",
    "    return labels\n",
    "\n",
    "def matches_straight_truth_track(track, labels, hits, truth):\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "    \n",
    "    tdf = truth.loc[hit_ix]\n",
    "    hdf = hits.loc[hit_ix]\n",
    "    truth_count = coll.Counter(tdf.particle_id.values).most_common(2)\n",
    "    truth_particle_id = truth_count[0][0]\n",
    "    if truth_particle_id == 0:\n",
    "        if len(truth_count) > 1:\n",
    "            truth_particle_id = truth_count[1][0]\n",
    "        else:\n",
    "            return False\n",
    "    tdf2 = truth.loc[(truth.particle_id == truth_particle_id)]\n",
    "    if np.all(hdf.volume_id == 7) or np.all(hdf.volume_id.values == 9):\n",
    "        arr_s1 = np.copy(hit_ix)\n",
    "        arr_s1.sort()\n",
    "        arr_s2 = np.copy(tdf2.index.values)\n",
    "        arr_s2.sort()\n",
    "        return np.array_equal(arr_s1, arr_s2)\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def lengthen_straight_track(track, labels, hits, truth):\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "    df = hits.loc[hit_ix]\n",
    "    \n",
    "    # Sort on abs(z)? Do we need to sort at all? We may want to find hits in the middle too....\n",
    "    df = df.sort_values('z')\n",
    "    hit_ix2 = df.index.values\n",
    "    x,y,z = df[['x', 'y', 'z' ]].values.astype(np.float32).T\n",
    "    track_dims = np.column_stack((x, y, z))\n",
    "    (m,b) = solve_linear_equation(track_dims)\n",
    "    #hits['y_hat'] = (hits['x'] * m) + b\n",
    "    #hits['y_hat_diff'] = hits['y'] - hits['y_hat']\n",
    "    #print('Straight track ix: ' + str(hit_ix))\n",
    "    df = hits.loc[hit_ix]\n",
    "    df = df.sort_values('z')\n",
    "    #print('m: ' + str(m) + ', b: ' + str(b))\n",
    "    \n",
    "    tdf = truth.loc[hit_ix]\n",
    "    truth_count = coll.Counter(tdf.particle_id.values).most_common(2)\n",
    "    truth_particle_id = truth_count[0][0]\n",
    "    if truth_particle_id == 0 and len(truth_count) > 1:\n",
    "        truth_particle_id = truth_count[1][0]\n",
    "    tdf2 = truth.loc[(truth.particle_id == truth_particle_id)]\n",
    "    tdf2 = tdf2.sort_values('tz')\n",
    "    tdfx2 = tdf2[['tx', 'ty', 'tz']]\n",
    "    #print(tdfx2)\n",
    "\n",
    "    arr_s1 = np.copy(hit_ix)\n",
    "    arr_s1.sort()\n",
    "    arr_s2 = np.copy(tdf2.index.values)\n",
    "    arr_s2.sort()\n",
    "    dfx1 = df[['x','y', 'z', 'zr', 'volume_id', 'layer_id']]\n",
    "    if np.array_equal(arr_s1, arr_s2):\n",
    "        print('Equal!')\n",
    "        print(arr_s1)\n",
    "        print(arr_s2)\n",
    "        print(dfx1)\n",
    "    else:\n",
    "        print('Detected track: ' + str(arr_s1))\n",
    "        print('Truth track:    ' + str(arr_s2))\n",
    "        print(dfx1)\n",
    "\n",
    "        df3 = hits.loc[tdf2.index.values]\n",
    "        dfx3 = df3[['x','y', 'z', 'zr', 'volume_id', 'layer_id']]\n",
    "        print(dfx3)\n",
    "        \n",
    "    #df2 = hits.loc[(hits['y_hat_diff'] < 0.1) & (hits['y_hat_diff'] > -0.1)]\n",
    "    #dfx2 = df2[['x','y', 'z', 'y_hat', 'y_hat_diff']]\n",
    "    #print(dfx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve for y = mx + b\n",
    "def solve_single_linear_equation(val1, val2):\n",
    "    x1 = val1[0]\n",
    "    x2 = val2[0]\n",
    "    y1 = val1[1]\n",
    "    y2 = val2[1]\n",
    "    is_valid = (x1 != x2 and y1 != y2)\n",
    "    m = 0\n",
    "    b = 0\n",
    "    if is_valid:\n",
    "        a = np.array([[x1, 1], [x2, 1]])\n",
    "        b = np.array([y1, y2])\n",
    "        sol = np.linalg.solve(a, b)\n",
    "        m = sol[0]\n",
    "        b = sol[1]\n",
    "    return is_valid, m, b\n",
    "\n",
    "\n",
    "def solve_linear_equation(vals):\n",
    "    \"\"\"Take average of y=mx+b slope across all pair-wise elements, as well as between\n",
    "       first and last elements.\"\"\"\n",
    "    m = 0\n",
    "    b = 0\n",
    "    count = 0\n",
    "    for i in range(len(vals) - 1):\n",
    "        is_valid, m2, b2 = solve_single_linear_equation(vals[i], vals[i+1])\n",
    "        if is_valid:\n",
    "            count = count + 1\n",
    "            m = m + m2\n",
    "            b = b + b2\n",
    "    is_valid, m2, b2 = solve_single_linear_equation(vals[0], vals[-1])\n",
    "    if is_valid:\n",
    "        count = count + 1\n",
    "        m = m + m2\n",
    "        b = b + b2\n",
    "    if count > 1:\n",
    "        m = m / count\n",
    "        b = b / count\n",
    "    return m, b\n",
    "\n",
    "\n",
    "def check_is_track_straight(vals, m, b):\n",
    "    # m/b solved for vals[0], vals[1], test with others\n",
    "    xdiffs = []\n",
    "    ydiffs = []\n",
    "    is_reject = 0\n",
    "    is_straight = 1\n",
    "    if len(vals) < 4:\n",
    "        is_straight = 0\n",
    "    outlier_count = 0\n",
    "    y_sum = abs(np.sum(vals[:,1]))\n",
    "    for ix, val in enumerate(vals):\n",
    "        yhat = m * val[0] + b\n",
    "        ydiff = abs(yhat - val[1])\n",
    "        ydiffs.append(ydiff)\n",
    "        #print('yhat: ' + str(yhat) + ', y: ' + str(val[1]) + ', diff: ' + str(ydiff))\n",
    "        xhat = (val[1] - b) / m\n",
    "        xdiff = abs(xhat - val[0])\n",
    "        xdiffs.append(xdiff)\n",
    "        #print('xhat: ' + str(xhat) + ', x: ' + str(val[0]) + ', diff: ' + str(xdiff))\n",
    "        #print('Diff at ix ' + str(ix) + ' is: ' + str(diff))\n",
    "        if (ydiff > 1):#abs(val[1]*0.01)):#(ydiff > 3 and ydiff > abs(val[1]*0.05)):\n",
    "            outlier_count = outlier_count + 1\n",
    "            is_straight = 0\n",
    "            if (ydiff > abs(val[1]*0.5)):\n",
    "                is_reject = 1\n",
    "    #print(str(is_straight) + ', ' + str(y_sum) + ', ' + str(sum(ydiffs)))\n",
    "    # Tolerate a single outlier if the total diff is minimal\n",
    "    #if outlier_count == 1 and is_reject == 0 and sum(ydiffs) <= y_sum*0.02:\n",
    "    #    is_straight = 1\n",
    "    #    #print('Straightened!')\n",
    "    #    #print('Straightened: ' + str(is_straight) + ', ' + str(y_sum) + ', ' + str(sum(ydiffs)))\n",
    "    #if is_straight:\n",
    "    #    print(str(is_straight) + ', ' + str(y_sum) + ', ' + str(sum(ydiffs)))\n",
    "    return is_straight, xdiffs, ydiffs\n",
    "\n",
    "def check_if_zr_straight(zr_values):\n",
    "    diff_zrs = np.diff(zr_values)\n",
    "    abs_diff_zrs = np.absolute(diff_zrs)\n",
    "    min_zr = zr_values.min()\n",
    "    max_zr = zr_values.max()\n",
    "    mean_diff_zr = diff_zrs.mean()\n",
    "    median_zr = abs(np.median(zr_values))\n",
    "    mean_zr = zr_values.mean()\n",
    "    num_outliers = 0\n",
    "    if mean_zr < 0:\n",
    "        allowed_min = mean_zr * 1.02\n",
    "        allowed_max = mean_zr * 0.98\n",
    "    else:\n",
    "        allowed_min = mean_zr * 0.98\n",
    "        allowed_max = mean_zr * 1.02\n",
    "    if len(zr_values) < 3:\n",
    "        is_straight = 0\n",
    "    else:\n",
    "        is_straight = 1\n",
    "    # FIXME: Ignores outliers for now, tracks with outliers will\n",
    "    # not likely be considered 'straight'.\n",
    "    for zr_value in zr_values:\n",
    "        if zr_value < allowed_min or zr_value > allowed_max:\n",
    "            num_outliers = num_outliers + 1\n",
    "            is_straight = 0\n",
    "    return (is_straight, num_outliers)\n",
    "\n",
    "def is_straight_track(track, labels, hits):\n",
    "    is_straight = 0\n",
    "    # Idea: Solve y=mx+b for each pair of hits, allow for a few outliers\n",
    "    # (one outlier will cause up to two bad values).\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "\n",
    "    df = hits.loc[hit_ix]\n",
    "    # Sort on abs(z)? Do we need to sort at all? We may want to find hits in the middle too....\n",
    "    df = df.sort_values('z')\n",
    "    hit_ix2 = df.index.values\n",
    "    \n",
    "    #x,y,z = df[['x', 'y', 'z' ]].values.astype(np.float32).T\n",
    "    #track_dims = np.column_stack((x, y, z/3))\n",
    "    #(m,b) = solve_linear_equation(track_dims)\n",
    "    #(is_straight, x_diffs, y_diffs) = check_is_track_straight(track_dims, m, b)\n",
    "    x,y,z,zr,v,azr = df[['x', 'y', 'z', 'zr', 'volume_id', 'azr' ]].values.astype(np.float32).T\n",
    "    track_dims = np.column_stack((x, y, z/3))\n",
    "    if len(hit_ix2) > 1:\n",
    "        (is_straight, num_outliers) = check_if_zr_straight(zr)\n",
    "        #if not is_straight and len(np.unique(v)) == 1 and num_outliers <= 2 and len(hit_ix2) > 4:\n",
    "        if not is_straight and num_outliers <= 2 and len(hit_ix2) > 4:\n",
    "            is_straight = 1\n",
    "    #elif not is_straight and len(np.unique(v)) == 1 and num_outliers <= 3 and len(hit_ix2) > 6:\n",
    "    #    is_straight = 1\n",
    "    #elif not is_straight and len(np.unique(v)) == 1 and num_outliers <= 4 and len(hit_ix2) > 8:\n",
    "    #    is_straight = 1\n",
    "    #    #print('Track : ' + str(track))# + ', possible outlier: ' + str(zr) + ', azr: ' + str(azr))\n",
    "    #if is_straight:\n",
    "    #    my_track = []\n",
    "    #    my_track.append(track_dims)\n",
    "    #    draw_prediction_xyz(my_track, my_track)\n",
    "    return is_straight, track_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_my_track(track, labels, hits):\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "    df = hits.loc[hit_ix]\n",
    "    \n",
    "    # Sort on abs(z)? Do we need to sort at all? We may want to find hits in the middle too....\n",
    "    df = df.sort_values('z')\n",
    "    hit_ix2 = df.index.values\n",
    "    x,y,z = df[['x', 'y', 'z' ]].values.astype(np.float32).T\n",
    "    track_dims = np.column_stack((x, y, z))\n",
    "    draw_prediction_xyz([track_dims], [track_dims])\n",
    "\n",
    "def draw_prediction(truth, predict, start=0, end=1):\n",
    "   \n",
    "    fig1 = plt.figure(figsize=(12,12))\n",
    "    ax1  = fig1.add_subplot(111, projection='3d')\n",
    "    fig1.patch.set_facecolor('white')\n",
    "    ax1.set_xlabel('x', fontsize=16)\n",
    "    ax1.set_ylabel('y', fontsize=16)\n",
    "    ax1.set_zlabel('z', fontsize=16)\n",
    "        \n",
    "\n",
    "    fig2 = plt.figure(figsize=(12,12))\n",
    "    ax2  = fig2.add_subplot(111, projection='3d')\n",
    "    fig2.patch.set_facecolor('white')\n",
    "    ax2.set_xlabel('a', fontsize=16)\n",
    "    ax2.set_ylabel('r', fontsize=16)\n",
    "    ax2.set_zlabel('z', fontsize=16)\n",
    "\n",
    "    for n in range(start,end,1):\n",
    "        x, y, z, v, l, m, a, r, zdr = truth[n].T\n",
    "        #x = r*np.cos(a)\n",
    "        #y = r*np.sin(a)\n",
    "        \n",
    "        ex, ey, ez, ev, el, em, ea, er, ezdr = predict[n].T\n",
    "        #ex = er*np.cos(ea)\n",
    "        #ey = er*np.sin(ea)\n",
    "        \n",
    "        color = np.random.uniform(0,1,3)\n",
    "        ax1.plot(ex,ey,ez,'.-',color = [0.75,0.75,0.75], markersize=10)\n",
    "        ax1.plot(x,y,z,'.-',color = color, markersize=5)\n",
    "        \n",
    "        \n",
    "        ax2.plot(ea,er,ez,'.-',color = [0.75,0.75,0.75], markersize=10)\n",
    "        ax2.plot(a,r,z,'.-',color = color, markersize=5)\n",
    "        if n==50: plt.show(1)\n",
    "\n",
    "def draw_prediction_xyz(truth, predict):\n",
    "   \n",
    "    fig1 = plt.figure(figsize=(12,12))\n",
    "    ax1  = fig1.add_subplot(111, projection='3d')\n",
    "    fig1.patch.set_facecolor('white')\n",
    "    ax1.set_xlabel('x', fontsize=16)\n",
    "    ax1.set_ylabel('y', fontsize=16)\n",
    "    ax1.set_zlabel('z', fontsize=16)\n",
    "        \n",
    "\n",
    "    #fig2 = plt.figure(figsize=(12,12))\n",
    "    #ax2  = fig2.add_subplot(111, projection='3d')\n",
    "    #fig2.patch.set_facecolor('white')\n",
    "    #ax2.set_xlabel('a', fontsize=16)\n",
    "    #ax2.set_ylabel('r', fontsize=16)\n",
    "    #ax2.set_zlabel('z', fontsize=16)\n",
    "\n",
    "    predict_size = len(predict)\n",
    "    #predict_size = 10\n",
    "    for n in range(0,predict_size,1):\n",
    "        x, y, z = truth[n].T\n",
    "        ex, ey, ez = predict[n].T\n",
    "        \n",
    "        color = np.random.uniform(0,1,3)\n",
    "        ax1.plot(ex,ey,ez,'.-',color = [0.75,0.75,0.75], markersize=10)\n",
    "        ax1.plot(x,y,z,'.-',color = color, markersize=5)\n",
    "        #ax1.axis('scaled')\n",
    "        #plt.gca().set_aspect('equal', adjustable='box')\n",
    "        plt.axis('equal')\n",
    "        \n",
    "        #if n==50: plt.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_outliers_zr(labels, hits):\n",
    "    labels = np.copy(labels)\n",
    "    tracks = np.unique(labels)\n",
    "    hits['z_abs'] = hits.z.abs()\n",
    "    hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "    #hits['zr'] = hits['z'] / hits['r']\n",
    "    hits['zr'] = np.log1p(np.absolute(hits['z'] / hits['r']))*np.sign(hits['z'])\n",
    "    hits['a0'] = np.arctan2(hits.y,hits.x)\n",
    "    count_rem_zr_slope = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 4:\n",
    "            #outliers = zro.find_track_outliers_zr(track, labels, hits)\n",
    "            #outliers = merge.remove_track_outliers_slope(track, labels, hits)\n",
    "            outliers = merge.find_duplicate_z_using_zr(track, labels, hits)\n",
    "            if len(outliers) > 0:\n",
    "                count_rem_zr_slope = count_rem_zr_slope + len(outliers)\n",
    "                for oix in outliers:\n",
    "                    labels[oix] = 0\n",
    "            \n",
    "    print('zr outliers removed: ' + str(count_rem_zr_slope))\n",
    "\n",
    "    return labels\n",
    "\n",
    "def safe_outlier_removal(labels, hits, truth, find_all=False, debug=False):\n",
    "    labels = np.copy(labels)\n",
    "    tracks = np.unique(labels)\n",
    "    hits['z_abs'] = hits.z.abs()\n",
    "    hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "    #hits['zr'] = hits['z'] / hits['r']\n",
    "    hits['zr'] = np.log1p(np.absolute(hits['z'] / hits['r']))*np.sign(hits['z'])\n",
    "    hits['a0'] = np.arctan2(hits.y,hits.x)\n",
    "    count_removed = 0\n",
    "    count_not_removed = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 3:\n",
    "            #outlier_ix = zro.find_track_outliers_zr(track, labels, hits, find_all=find_all, truth=truth, debug=debug)\n",
    "            #outlier_ix = merge.remove_track_outliers_slope(track, labels, hits)\n",
    "            outlier_ix = merge.find_duplicate_z_using_zr(track, labels, hits)\n",
    "            if len(outlier_ix) > 0:\n",
    "                tdf = truth.loc[track_hits]\n",
    "                truth_count = coll.Counter(tdf.particle_id.values).most_common(1)\n",
    "                truth_particle_id = truth_count[0][0]\n",
    "                for out_ix in outlier_ix:\n",
    "                    if tdf.loc[out_ix].particle_id != truth_particle_id:\n",
    "                        labels[out_ix] = 0\n",
    "                        count_removed = count_removed + 1\n",
    "                    else:\n",
    "                        count_not_removed = count_not_removed + 1\n",
    "\n",
    "    print('safe count_removed: ' + str(count_removed))\n",
    "    print('safe count_not_removed: ' + str(count_not_removed))\n",
    "    return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.copy(labels_helix6)\n",
    "score_one_submission(event_id, hits, labels, truth)\n",
    "labels2 = safe_outlier_removal(labels, hits, truth)\n",
    "#labels2 = remove_outliers_zr(labels, hits)\n",
    "score_one_submission(event_id, hits, labels2, truth)\n",
    "# base: 0.54537665\n",
    "# z/r: 193/54, 78.1% - 0.54466673\n",
    "# log(abs(z/r))*sign: 138/39, 78.0% - 0.54507259\n",
    "# slope: 485/345, 58.4% - 0.54149804\n",
    "# dupz: 1689/574, 74.6% - 0.53938707\n",
    "# dupz.log(): 1689/574 - 0.53940099"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels3 = np.copy(labels2)\n",
    "labels3 = safe_outlier_removal(labels3, hits, truth)\n",
    "score_one_submission(event_id, hits, labels3, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.copy(labels_helix6)\n",
    "labels2 = extend_straight_tracks_new(labels, hits)\n",
    "score_one_submission(event_id, hits, labels2, truth)\n",
    "# z/r: 0.55526982\n",
    "# log(z/r): 0.55494226"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_straight_tracks_new(labels, hits):\n",
    "    labels = np.copy(labels)\n",
    "    hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "    #hits['zr'] = hits['z'] / hits['r']\n",
    "    hits['zr'] = np.log1p(np.absolute(hits['z'] / hits['r']))*np.sign(hits['z'])\n",
    "\n",
    "    # For the first round, try to lengthen any tracks based on expected\n",
    "    # zr values in adjacent layers.\n",
    "    tracks = np.unique(labels)\n",
    "    for track in tracks:\n",
    "        if track == 0: continue\n",
    "        is_straight = strt.is_straight_track(track, labels, hits)\n",
    "        # Tracks that appear to be straight are more aggressive when finding\n",
    "        # a hit in the next layer.\n",
    "        labels = strt.straight_track_extension(track, labels, hits, is_straight)\n",
    "\n",
    "    # For the second round, only do the tracks that appear straight. These\n",
    "    # are lengthened a second time in case non-straight tracks accidentally\n",
    "    # took some hits away from the straight tracks.\n",
    "    tracks = np.unique(labels)\n",
    "    for track in tracks:\n",
    "        if track == 0: continue\n",
    "        if strt.is_straight_track(track, labels, hits):\n",
    "            labels = strt.straight_track_extension(track, labels, hits, True)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS HISTORY:\n",
    "# all +ve or all -ve, remove excessive slope: 13 awesome, 3 crappy --> 81%\n",
    "# most +ve, remove most -ve: 773 awesome, 310 crappy --> 71%\n",
    "# most +ve, remove extreme else remove most -ve: 793 awesome, 290 crappy --> 73%\n",
    "# most +ve, rem 1st opposing jump else rem extreme else rem most -ve: 799 awesome, 289 crappy --> 73%\n",
    "# most +ve, rem 1st opposing jump else rem extreme else rem most -ve > 2*mean: 739 awesome, 184 crappy --> 80%\n",
    "# same as above, remove final extreme outlier test: 719 awesome, 170 crappy --> 80.9%\n",
    "# reverse_opt tweak, 10x mean limit: 720 v 166, 81%\n",
    "# same as above, re-enabled extreme jump test: 743 awesome, 166 crappy --> 81.7%\n",
    "# most -ve, same as above: 302 awesome, 404 crappy --> 43%\n",
    "# most -ve, flip sign diff_zrs, opposing jump only: 113 awesome, 42 crappy --> 73%\n",
    "# most -ve, flip sign diff_zrs, opposing jump only, reverse opt for -ve jump1: 128 awesome, 21 crappy --> 86%\n",
    "# most -ve, same as above, remove extreme jump: 172 awesome, 27 crappy --> 86%\n",
    "# most -ve, all checks as +ve slopes, some -ve-specific checks: 456 awesome, 124 crappy --> 78.6%\n",
    "# Combined: 1199 awesome vs 294 crappy: 80.3%   (reverse opt mean fix in opposing jump: 1199 vs 290, 80.5%)\n",
    "# Combined with abs(mean) fix in opposing jumps: 1206 awesome, 284 crappy: 80.9%\n",
    "# Combined with abs(mean), favour 1st in opposing jumps: 1206 awesome, 276 crappy: 81.4%\n",
    "# Combined as above, limit to tracks >= 5 hits: 1166 awesome, 235 crappy: 83.2%\n",
    "# Combined, fix negative jump check, limit to >4x mean, 1108 v 227: 82.9%\n",
    "# +ve,-ve jump threshold 4x, fix negative finder, 1042 v 203: 83.7%\n",
    "# +ve,-ve jump threshold 2x, fix negative finder, 1155 vs 246: 82.4%\n",
    "# +ve,-ve jump threshold 3x: 1097 v 221: 83.2%\n",
    "# threshold 4x: 1042 v 204: 83.6%\n",
    "# threshold 4x, no smart 0/1 picker: 1048 v 197: 84.1%\n",
    "# threshold 2x, no smart 0/1 picker: 1166 v 235: 83.2%\n",
    "# fix diff_zrs, now 1154 v 238, 82.9%\n",
    "# fix diff_zrs, threshold 4x, 1091 v 215: 83.5%\n",
    "\n",
    "# 4: chop valley tail x2.5, opposing jumps std mean: 152 awesome, 22 crappy: 87%\n",
    "#  favour_1st_removal opposing jumps: 142 v 20, 87.6%\n",
    "#  + remove extreme: 160 v 23, 87.4%\n",
    "#  + remove final slope too large: 165 v 25, 86.8%\n",
    "#  + simple opposites compare prev diff: 235 v 46: 83.6%\n",
    "#  + fix simple opposites bug: 215 v 35: 86%\n",
    "#  + ensure valley decreases at beginning: 225 v 35: 86.5%  (274 v 44, 86.1%, biggest opposing)\n",
    "# Total 1,2,3,4: 1452 awesome, 319 crappy: 82%\n",
    "# AFTER ALL CRAZY CHANGES:\n",
    "# 3: 225 v 110, 67.1%\n",
    "# 3: check for bad initial slope earlier: 226 v 109, 67.5%\n",
    "# 3: -> and extreme opposite jump: 234 v 109, 68.2%\n",
    "# 3: -> and set extreme threshold to 10x (def. 3x?): 199 v 63, 76.0%\n",
    "# 3: -> remove negative extreme jump: 190 v 38, 83.3%\n",
    "# 3: -> positive extreme jump using abs. mean 10x: 142 v 23, 86%\n",
    "# 3: -> positive extreme jump using abs. mean 5x: 167 v 27, 86%\n",
    "# 3: -> positive extreme jump using abs. mean 10x: 138 v 22, 86.3%  (10/20 ext. 144 v 25, 85.2% )\n",
    "# 3:  169 v 34: 83.3%\n",
    "# 3: common 4/8: 159 v 30: 84.1%\n",
    "#----\n",
    "# 4: chop valley tail: 12 v 1, 92%\n",
    "# 4: remove first wrong slope: 37 v 5, 88%\n",
    "# 4: remove first wrong slope > 3x mean: 34 v 2, 94.4%\n",
    "# 4: add biggest opposing jump: 91 v 19, 82.7%\n",
    "# 4: add extreme opposing jumps: 100 v 24, 80.6%\n",
    "# 4: do not do reverse extreme opposing jump opt: 101 v 23, 81.4%\n",
    "# 4: favor first extreme opposing jump opt: 102 v 22, 82.3%\n",
    "# 4: extreme jump: 130 v 36, 78.3%\n",
    "# 4: 5x extreme jumps: 102 v 22, 82.3% -- 105 v 26 common code, 80.2% (107 v 28 common, 79.3%)\n",
    "#    10/20 ext 113 v 31, 78.5%\n",
    "# 4: 126 v 35, 78.3%\n",
    "# 4: common 4/8: 116 v 29, 80%\n",
    "#---\n",
    "# 0: first slope wrong: 11 v 0, 100%\n",
    "# 0: last slope wrong: 19 v 1, 95%\n",
    "# 0: biggest opposing jump: 137 v 23, 85.6%\n",
    "# 0: opposing extreme jumps: 170 v 34, 83.3%\n",
    "# 0: add pos/neg extreme jump removal: 171 v 34, 83.4%\n",
    "# 0: 8x pos/neg extreme jump removal: 177 v 35, 83.5%\n",
    "# 0: Reverse opt true, 165 v 32, 83.8%\n",
    "# 0: 168 v 33, 83.6%\n",
    "# 0: 3/8 split: 208 v 45: 82.2%\n",
    "# 0: common 4/8: 186 v 37: 83.4%\n",
    "#---\n",
    "# 1: 768 v 138, 84.8% (769 v 139 common code, 84.7%)\n",
    "#  --> common code! 779 v 128, 85.9%  (763 v 126 for common, 85.8%)\n",
    "#  --> 20/30 for all: 761 v 121, 86.3%\n",
    "#  --> common 4/8: 724 v 105, 87.3%\n",
    "#---\n",
    "# 2: 323 v 77, 80.8% (322 v 81 common code, 79.9%)\n",
    "#  --> common code! 328 v 71, 82.2%  (327 v 71 for common, 82.2%)\n",
    "#  --> 20/30 for all: 327 v 71, 82.2%\n",
    "#  --> common 4/8: 301 v 70, 81.1%\n",
    "#--------------------\n",
    "# OVERALL (5% cut-off): 1486 awesome, 271 crappy: 84.6%\n",
    "# 0.0% threshold to ignore: 2237 awesome, 752 crappy: 74.8% \n",
    "# 2.5% threshold to ignore: 1904 awesome, 396 crappy: 82.8%\n",
    "# 4.0% threshold to ignore: 1665 awesome, 319 crappy: 83.9%\n",
    "# 10.0% threshold to ignore: 664 awesome, 117 crappy: 85.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits['z_abs'] = hits.z.abs()\n",
    "hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "hits['zr'] = hits['z'] / hits['r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTENSION_ATTEMPT = 5\n",
    "EXTENSION_STANDARD_LIMITS = [0.02]#, 0.04, 0.06, 0.08, 0.10]\n",
    "\n",
    "\n",
    "l1 = np.copy(labels_helix1a)\n",
    "\n",
    "score_one_submission(event_id, hits, l1, truth)\n",
    "\n",
    "for ix, limit in enumerate(EXTENSION_STANDARD_LIMITS):\n",
    "    l1 = extend_labels(ix, l1, hits, truth=truth, do_swap=ix%2==1, limit=(limit))\n",
    "    score_one_submission(event_id, hits, l1, truth)\n",
    "\n",
    "score_one_submission(event_id, hits, l1, truth)\n",
    "# 0.54586603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTENSION_ATTEMPT = 5\n",
    "EXTENSION_STANDARD_LIMITS = [0.02]#, 0.04, 0.06, 0.08, 0.10]\n",
    "\n",
    "\n",
    "l2 = np.copy(labels_helix1a)\n",
    "\n",
    "score_one_submission(event_id, hits, l2, truth)\n",
    "\n",
    "for ix, limit in enumerate(EXTENSION_STANDARD_LIMITS):\n",
    "    l2 = extend_labels(ix, l2, hits, truth=truth, do_swap=ix%2==1, limit=(limit))\n",
    "    score_one_submission(event_id, hits, l2, truth)\n",
    "\n",
    "score_one_submission(event_id, hits, l2, truth)\n",
    "# 0.55049849\n",
    "# 0.55770785 for min track length of 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1 = merge.heuristic_merge_tracks(labels_helix2, labels_helix1, hits, overwrite_limit=6, print_summary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_merge_orig = merge.heuristic_merge_tracks(labels_helix2, labels_orig_allx_filter, hits, overwrite_limit=6, print_summary=False)\n",
    "labels_merge_orig = merge.heuristic_merge_tracks(labels_helix2, l3, hits, overwrite_limit=6, print_summary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_merge_new = merge.heuristic_merge_tracks(labels_helix2, labels_new_allx_filter, hits, overwrite_limit=6, print_summary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_one_submission(event_id, hits, labels1, truth)\n",
    "score_one_submission(event_id, hits, labels_merge_orig, truth)\n",
    "score_one_submission(event_id, hits, labels_merge_new, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_orig_allx = np.copy(l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_new_allx = np.copy(l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_orig_allx_filter = np.copy(labels_orig_allx)\n",
    "labels_orig_allx_filter = merge.remove_outliers(labels_orig_allx_filter, hits, print_counts=True)\n",
    "score_one_submission(event_id, hits, labels_orig_allx_filter, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_new_allx_filter = np.copy(labels_new_allx)\n",
    "labels_new_allx_filter = merge.remove_outliers(labels_new_allx_filter, hits, print_counts=True)\n",
    "score_one_submission(event_id, hits, labels_new_allx_filter, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3 = np.copy(labels_helix6)\n",
    "#score_one_submission(event_id, hits, l3, truth)\n",
    "\n",
    "EXTENSION_STANDARD_LIMITS = [0.02, 0.04, 0.06, 0.08, 0.10]\n",
    "#EXTENSION_STANDARD_LIMITS = [0.025, 0.05, 0.075, 0.10, 0.125]#1, 0.03, 0.05, 0.07, 0.09, 0.11]\n",
    "extensions = EXTENSION_STANDARD_LIMITS\n",
    "l3 = do_all_track_extensions(l3, hits, extensions, num_neighbours=18)\n",
    "\n",
    "score_one_submission(event_id, hits, l3, truth)\n",
    "# Baseline: 0.52818660\n",
    "# No ZR:\n",
    "# 0.02/4/6/8/10: 0.57544313\n",
    "# 0.03/6/9: 0.57472324\n",
    "# 0.02/3/4/5/6: 0.57176198\n",
    "# With ZR:\n",
    "# 0.02/4/6/8/10: 0.57804640\n",
    "# 0.03/6/9: 0.57757000\n",
    "# 0.03/4/5: 0.57152764\n",
    "# 0.03: 0.56004549\n",
    "# 0.03/3: 0.56305501\n",
    "# 0.02/25/3/35/4/45/5: 0.57085263\n",
    "# 0.01: 0.54440312\n",
    "# 0.01/3/5/7/9/11: 0.57874132\n",
    "# 0.05: 0.56770284\n",
    "# 0.07: 0.56946313\n",
    "# 0.09: 0.57157504\n",
    "# 0.11: 0.57248631\n",
    "# 0.07/85/1/115: 0.57666858\n",
    "# 0.09/1/11/12: 0.57531913\n",
    "# 0.025/5/75/1/125: 0.57861203\n",
    "### 18 neighbours\n",
    "# 0.02/4/6/8/10: 0.57364430\n",
    "# 0.025/5/75/1/125: 0.57521521\n",
    "### 25 neighbours (~225sec)\n",
    "# 0.02/4/6/8/10: 0.57679645\n",
    "# 0.025/5/75/1/125: 0.57800959\n",
    "### ZR checks, 25 neighbours (takes much longer - 450sec+)\n",
    "# 0.02/4/6/8/10: 0.57748932\n",
    "# 0.025/5/75/1/125: 0.57917625\n",
    "### ZR + Scoring, 25 neighbours (545 seconds/659 seconds)\n",
    "# 0.02/4/6/8/10: 0.58137913\n",
    "# 0.025/5/75/1/125: 0.57957819\n",
    "### Scoring, 25 neighbours\n",
    "# 0.02/4/6/8/10: 0.57486798  (303 seconds)\n",
    "# 0.025/5/75/1/125: 0.56984849 (379 seconds)\n",
    "### ZR + Scoring, 30 neighbours\n",
    "# 0.02/4/6/8/10: 0.57911902 (645 seconds)\n",
    "### ZR + Scoring, 18 neighbours\n",
    "# 0.02/4/6/8/10: 0.58033498\n",
    "# 0.02/4/6/8: 0.57894488 (361 seconds)\n",
    "### ZR + Scoring, 10 neighbours\n",
    "# 0.02/4/6/8/10: 0.57219654 (354 seconds)\n",
    "#########\n",
    "# base: 0.54537665\n",
    "# 18 neighbours, zr+score: 0.62097243\n",
    "# 18 neighbours, log(zr)+score: 0.61599901\n",
    "# 18 neighbours, log(zr), no score: 0.61497311\n",
    "# 18 neighbours, zr, no score: 0.61547195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_extend_single_hit_qqq(track, track_len, hit_ix, labels, hits, use_scoring):\n",
    "    use_scoring = True\n",
    "    trk_ix = np.where(labels == track)[0]\n",
    "    df = hits.loc[trk_ix]\n",
    "    (z) = df[['z']].values.astype(np.float32).T\n",
    "    hit_z = hits.loc[hit_ix].z\n",
    "    hit_zr = hits.loc[hit_ix].zr\n",
    "    hit_volume = hits.loc[hit_ix].volume_id\n",
    "    hit_layer = hits.loc[hit_ix].layer_id.astype(np.int)\n",
    "    #print('hit_layer: ' + str(hit_layer))\n",
    "    if hit_z in z:\n",
    "        #print('LIAM: Already have z: ' + str(hit_z) + ', ' + str(z))\n",
    "        return (labels, track_len)\n",
    "    df = df.loc[(hits['volume_id'] == hit_volume)]\n",
    "    df = df.sort_values('z')\n",
    "    (x, y, z, zr) = df[['x', 'y', 'z', 'zr']].values.astype(np.float32).T\n",
    "    layer = df.layer_id.values\n",
    "    lmap = [0,0,0,0,1,0,2,0,3,0,4,0,5,0,6]\n",
    "    #print('LIAM: ' + str(layer))\n",
    "    (xs, ys, zrs, counts) = strt.generate_zr_layer_data(x, y, zr, layer, lmap)\n",
    "    aix = lmap[hit_layer]\n",
    "    if zrs[aix] != 0 and abs(zrs[aix]) > 1.0: # FIXME: Evaluate ones <= 1.0....\n",
    "        #print('LIAM: existing mean zr: ' + str(zrs[aix]) + ', hit_zr: ' + str(hit_zr))\n",
    "        abs1 = abs(zrs[aix])\n",
    "        abs2 = abs(hit_zr)\n",
    "        if abs(zrs[aix]) < 1:\n",
    "            min_abs = abs1 * 0.8\n",
    "            max_abs = abs1 * 1.2\n",
    "        elif abs(zrs[aix]) < 10:\n",
    "            min_abs = abs1 * 0.9\n",
    "            max_abs = abs1 * 1.1\n",
    "        else:\n",
    "            min_abs = abs1 * 0.99\n",
    "            max_abs = abs1 * 1.01\n",
    "        if abs2 < min_abs or abs2 > max_abs or (zrs[aix] > 0 and hit_zr < 0) or (zrs[aix] < 0 and hit_zr > 0):\n",
    "            #print('LIAM: SKIP: existing mean zr: ' + str(zrs[aix]) + ', hit_zr: ' + str(hit_zr) + ', min: ' + str(min_abs) + ', max: ' + str(max_abs))\n",
    "            return (labels, track_len)\n",
    "    if use_scoring:\n",
    "        outlier_modifier = 0.75\n",
    "        orig_track = labels[hit_ix]\n",
    "        labels[hit_ix] = track\n",
    "        new_score = score2.calculate_track_score(track, labels, hits, outlier_modifier=outlier_modifier, outlier_ix=hit_ix)\n",
    "        labels[hit_ix] = orig_track\n",
    "        if orig_track != 0:\n",
    "            orig_score = score2.calculate_track_score(orig_track, labels, hits, outlier_modifier=outlier_modifier, outlier_ix=hit_ix)\n",
    "        else:\n",
    "            orig_score = 0\n",
    "\n",
    "        if new_score >= orig_score:\n",
    "            labels[hit_ix] = track\n",
    "            track_len = track_len + 1\n",
    "    else:\n",
    "        orig_track = labels[hit_ix]\n",
    "        if orig_track == 0:\n",
    "            labels[hit_ix] = track\n",
    "        else:\n",
    "            # If the hit is already occupied by another track, only take ownership\n",
    "            # of the hit if our track is longer than the current-occupying track.\n",
    "            orig_track_len = len(np.where(labels==orig_track)[0])\n",
    "            if track_len > orig_track_len:\n",
    "                labels[hit_ix] = track\n",
    "                track_len = track_len + 1\n",
    "                \n",
    "    return (labels, track_len)\n",
    "\n",
    "def try_extend_single_hit(track, track_len, hit_ix, labels, hits, use_scoring):\n",
    "    #use_scoring = True\n",
    "    if use_scoring:\n",
    "        outlier_modifier = 0.75\n",
    "        orig_track = labels[hit_ix]\n",
    "        labels[hit_ix] = track\n",
    "        new_score = score2.calculate_track_score(track, labels, hits, outlier_modifier=outlier_modifier, outlier_ix=hit_ix)\n",
    "        labels[hit_ix] = orig_track\n",
    "        if orig_track != 0:\n",
    "            orig_score = score2.calculate_track_score(orig_track, labels, hits, outlier_modifier=outlier_modifier, outlier_ix=hit_ix)\n",
    "        else:\n",
    "            orig_score = 0\n",
    "\n",
    "        if new_score >= orig_score:\n",
    "            labels[hit_ix] = track\n",
    "            track_len = track_len + 1\n",
    "    else:\n",
    "        orig_track = labels[hit_ix]\n",
    "        if orig_track == 0:\n",
    "            labels[hit_ix] = track\n",
    "        else:\n",
    "            # If the hit is already occupied by another track, only take ownership\n",
    "            # of the hit if our track is longer than the current-occupying track.\n",
    "            orig_track_len = len(np.where(labels==orig_track)[0])\n",
    "            if track_len > orig_track_len:\n",
    "                labels[hit_ix] = track\n",
    "                track_len = track_len + 1\n",
    "                \n",
    "    return (labels, track_len)\n",
    "\n",
    "\n",
    "def _one_cone_slice(df, df1, angle, delta_angle, limit=0.04, num_neighbours=18, use_scoring=False):\n",
    "\n",
    "    min_num_neighbours = len(df1)\n",
    "    if min_num_neighbours < 3: \n",
    "        return df\n",
    "\n",
    "    hit_ids = df1.hit_id.values\n",
    "    a,c,s,r,zr,z = df1[['a', 'c', 's', 'r_norm', 'zr', 'z']].values.T\n",
    "    tree = KDTree(np.column_stack([c,s,r,zr]), metric='euclidean')\n",
    "    #tree = KDTree(np.column_stack([c,s,r]), metric='euclidean')\n",
    "\n",
    "    track_ids = list(df1.track_id.unique())\n",
    "    num_track_ids = len(track_ids)\n",
    "    min_length=2\n",
    "\n",
    "    labels = df.track_id.values\n",
    "    label_track_counts = coll.Counter(df1.track_id.values).most_common(num_track_ids)\n",
    "    \n",
    "    for track_count in label_track_counts:\n",
    "        p = track_count[0]\n",
    "        if p == 0: continue\n",
    "\n",
    "        idx = np.where(df1.track_id==p)[0]\n",
    "        cur_track_len = len(idx)\n",
    "        if cur_track_len<min_length: continue\n",
    "\n",
    "        # Un-comment following code to find the truth particle ID for the track.\n",
    "        #truth_ix = eda.get_truth_for_track(p, labels, truth)\n",
    "        #tdf = truth.loc[truth_ix]\n",
    "        #truth_particle_id = tdf.particle_id.values[0]\n",
    "        #print('track: ' + str(p) + ', len: ' + str(len(idx)) + ', idx: ' + str(idx))\n",
    "        #print('truth particle: ' + str(truth_particle_id) + ', count:' + str(truth_count[0][1]))\n",
    "\n",
    "        if angle>0:\n",
    "            idx = idx[np.argsort( z[idx])]\n",
    "        else:\n",
    "            idx = idx[np.argsort(-z[idx])]\n",
    "\n",
    "## start and end points  ##\n",
    "        idx0,idx1 = idx[0],idx[-1]\n",
    "        a0 = a[idx0]\n",
    "        a1 = a[idx1]\n",
    "        r0 = r[idx0]\n",
    "        r1 = r[idx1]\n",
    "        c0 = c[idx0]\n",
    "        c1 = c[idx1]\n",
    "        s0 = s[idx0]\n",
    "        s1 = s[idx1]\n",
    "        zr0 = zr[idx0]\n",
    "        zr1 = zr[idx1]\n",
    "\n",
    "        da0 = a[idx[1]] - a[idx[0]]  #direction\n",
    "        dr0 = r[idx[1]] - r[idx[0]]\n",
    "        direction0 = np.arctan2(dr0,da0)\n",
    "\n",
    "        da1 = a[idx[-1]] - a[idx[-2]]\n",
    "        dr1 = r[idx[-1]] - r[idx[-2]]\n",
    "        direction1 = np.arctan2(dr1,da1)\n",
    "\n",
    "        ## extend start point\n",
    "        ns = tree.query([[c0, s0, r0, zr0]], k=min(num_neighbours, min_num_neighbours), return_distance=False)\n",
    "        #ns = tree.query([[c0, s0, r0]], k=min(num_neighbours, min_num_neighbours), return_distance=False)\n",
    "        ns = np.concatenate(ns)\n",
    "\n",
    "        direction = np.arctan2(r0 - r[ns], a0 - a[ns])\n",
    "        diff = 1 - np.cos(direction - direction0)\n",
    "        ns = ns[(r0 - r[ns] > 0.01) & (diff < (1 - np.cos(limit)))]\n",
    "        for n in ns:\n",
    "            df_ix = hit_ids[n] - 1\n",
    "            # Un-comment this to see if we are extending the track properly\n",
    "            #is_good = (truth.loc[df_ix, 'particle_id'] == truth_particle_id)\n",
    "\n",
    "            (labels, cur_track_len) = try_extend_single_hit(p, cur_track_len, df_ix, labels, df, use_scoring)\n",
    "\n",
    "        ## extend end point\n",
    "        ns = tree.query([[c1, s1, r1, zr1]], k=min(num_neighbours, min_num_neighbours), return_distance=False)\n",
    "        #ns = tree.query([[c1, s1, r1]], k=min(num_neighbours, min_num_neighbours), return_distance=False)\n",
    "        ns = np.concatenate(ns)\n",
    "\n",
    "        direction = np.arctan2(r[ns] - r1, a[ns] - a1)\n",
    "        diff = 1 - np.cos(direction - direction1)\n",
    "  \n",
    "        ns = ns[(r[ns] - r1 > 0.01) & (diff < (1 - np.cos(limit)))]\n",
    "        for n in ns:  \n",
    "            df_ix = hit_ids[n] - 1\n",
    "            # Un-comment this to see if we are extending the track properly\n",
    "            #is_good = (truth.loc[df_ix, 'particle_id'] == truth_particle_id)\n",
    "\n",
    "            (labels, cur_track_len) = try_extend_single_hit(p, cur_track_len, df_ix, labels, df, use_scoring)\n",
    "\n",
    "    df['track_id'] = labels\n",
    "\n",
    "    return df\n",
    "\n",
    "def do_all_track_extensions(labels, hits, track_extension_limits, num_neighbours=18):\n",
    "    time1 = time.time()\n",
    "    df = hits.copy(deep=True)\n",
    "    df['track_id'] = labels.tolist()\n",
    "    df['r'] = np.sqrt(df.x**2 + df.y**2)\n",
    "    df['r_norm'] = df.r / 1000\n",
    "    df['zr'] = df.z / df.r\n",
    "    #df['zr'] = np.log1p(np.absolute(df['z'] / df['r']))*np.sign(df['z'])\n",
    "    df['arctan2'] = np.arctan2(df.z, df.r)\n",
    "    for ix, limit in enumerate(track_extension_limits):\n",
    "        df = extend(ix, df, do_swap=(ix%2==1), limit=(limit), num_neighbours=num_neighbours)\n",
    "    time2 = time.time()\n",
    "    print('Track extension took {:.3f} ms'.format((time2-time1)*1000.0))\n",
    "    return df.track_id.values\n",
    "\n",
    "def extend(iter, df, do_swap=False, limit=0.04, num_neighbours=18):\n",
    "    if do_swap:\n",
    "        df = df.assign(x = -df.x)\n",
    "        df = df.assign(y = -df.y)\n",
    "\n",
    "    df['a'] = np.arctan2(df.y, df.x)\n",
    "    df['c'] = np.cos(df.a)\n",
    "    df['s'] = np.sin(df.a)\n",
    "\n",
    "    for angle in range(-90,90,1):\n",
    "\n",
    "        print ('\\r%d %f '%(iter,angle), end='',flush=True)\n",
    "        df1 = df.loc[(df.arctan2>(angle-1.0)/180*np.pi) & (df.arctan2<(angle+1.0)/180*np.pi)]\n",
    "\n",
    "        num_hits = len(df1)\n",
    "        # Dynamically adjust the delta based on how many hits are found\n",
    "        if num_hits > 2000:\n",
    "            df1 = df.loc[(df.arctan2>(angle - 0.6 - 0.4)/180*np.pi) & (df.arctan2<(angle -0.6 + 0.4)/180*np.pi)]\n",
    "            df = _one_cone_slice(df, df1, angle-0.6, 0.4, limit, num_neighbours)\n",
    "            df1 = df.loc[(df.arctan2>(angle - 0.2 - 0.4)/180*np.pi) & (df.arctan2<(angle -0.2 + 0.4)/180*np.pi)]\n",
    "            df = _one_cone_slice(df, df1, angle-0.2, 0.4, limit, num_neighbours)\n",
    "            df1 = df.loc[(df.arctan2>(angle + 0.2 - 0.4)/180*np.pi) & (df.arctan2<(angle +0.2 + 0.4)/180*np.pi)]\n",
    "            df = _one_cone_slice(df, df1, angle+0.2, 0.4, limit, num_neighbours)\n",
    "            df1 = df.loc[(df.arctan2>(angle + 0.6 - 0.4)/180*np.pi) & (df.arctan2<(angle +0.6 + 0.4)/180*np.pi)]\n",
    "            df = _one_cone_slice(df, df1, angle+0.6, 0.4, limit, num_neighbours)\n",
    "        else:\n",
    "            df = _one_cone_slice(df, df1, angle, 1, limit, num_neighbours)\n",
    "           \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_outlier_len(track_id, labels, hits, outlier_ix):\n",
    "    labels = np.copy(labels)\n",
    "    orig_len = len(np.where(labels == track_id)[0])\n",
    "    contains_outlier = False\n",
    "    outliers = find_track_outliers_zr(track_id, labels, hits, find_all=True)\n",
    "    if not contains_outlier:\n",
    "        contains_outlier = (outlier_ix in outliers)\n",
    "    for outx in outliers:\n",
    "        labels[outx] = 0\n",
    "    if len(outliers) > 0:\n",
    "        outliers = find_track_outliers_zr(track_id, labels, hits, find_all=True)\n",
    "        if not contains_outlier:\n",
    "            contains_outlier = (outlier_ix in outliers)\n",
    "        for outx in outliers:\n",
    "            labels[outx] = 0\n",
    "        if len(outliers) > 0:\n",
    "            outliers = find_track_outliers_zr(track_id, labels, hits, find_all=True)\n",
    "            if not contains_outlier:\n",
    "                contains_outlier = (outlier_ix in outliers)\n",
    "            for outx in outliers:\n",
    "                labels[outx] = 0\n",
    "            if len(outliers) > 0:\n",
    "                outliers = find_track_outliers_zr(track_id, labels, hits, find_all=True)\n",
    "                if not contains_outlier:\n",
    "                    contains_outlier = (outlier_ix in outliers)\n",
    "                for outx in outliers:\n",
    "                    labels[outx] = 0\n",
    "    new_len = len(np.where(labels == track_id)[0])\n",
    "    return (orig_len, new_len, contains_outlier)\n",
    "\n",
    "def calculate_track_score(track_id, labels, hits, outlier_modifier=0.75, outlier_ix=-1):\n",
    "    (cur_len, no_outlier_len, has_outlier) = non_outlier_len(track_id, labels, hits, outlier_ix)\n",
    "    modifier=1.0\n",
    "    if has_outlier:\n",
    "        modifier=outlier_modifier\n",
    "    score1 = no_outlier_len / cur_len\n",
    "    if cur_len < 4:\n",
    "        score2 = 0\n",
    "    else:\n",
    "        score2 = min(cur_len/20.0, 1.0)\n",
    "    return ((score1 + score2) * modifier)/2.0\n",
    "\n",
    "def _one_cone_slice_new(df, truth, angle, delta_angle, limit=0.04, num_neighbours=18):\n",
    "\n",
    "    df1 = df.loc[(df.arctan2>(angle - delta_angle)/180*np.pi) & (df.arctan2<(angle + delta_angle)/180*np.pi)]\n",
    "\n",
    "    min_num_neighbours = len(df1)\n",
    "    if min_num_neighbours < 3: \n",
    "        return df, 0, 0, 0, 0\n",
    "\n",
    "    hit_ids = df1.hit_id.values\n",
    "    x,y,z = df1.as_matrix(columns=['x', 'y', 'z']).T\n",
    "    r  = (x**2 + y**2)**0.5\n",
    "    r  = r/1000\n",
    "    a  = np.arctan2(y,x)\n",
    "    c = np.cos(a)\n",
    "    s = np.sin(a)\n",
    "    tree = KDTree(np.column_stack([c,s,r]), metric='euclidean')\n",
    "\n",
    "    track_ids = list(df1.track_id.unique())\n",
    "    num_track_ids = len(track_ids)\n",
    "    min_length=2\n",
    "    \n",
    "    labels = df.track_id.values\n",
    "    count_good = 0\n",
    "    count_bad = 0\n",
    "    count_outlier_correct = 0\n",
    "    count_outlier_wrong = 0\n",
    "\n",
    "    label_track_counts = coll.Counter(df1.track_id.values).most_common(num_track_ids)\n",
    "    \n",
    "    for track_count in label_track_counts:\n",
    "        p = track_count[0]\n",
    "        if p == 0: continue\n",
    "\n",
    "        idx = np.where(df1.track_id==p)[0]\n",
    "        cur_track_len = len(idx)\n",
    "\n",
    "        if cur_track_len<min_length: continue\n",
    "        label_idx = np.where(labels==p)[0]\n",
    "\n",
    "        # Un-comment following code to find the truth particle ID for the track.\n",
    "        #truth_ix = []\n",
    "        #for ii in idx:\n",
    "        #    truth_ix.append(hit_ids[ii] - 1)\n",
    "        #tdf = truth.loc[truth_ix]\n",
    "        #truth_count = coll.Counter(tdf.particle_id.values).most_common(1)\n",
    "        #truth_particle_id = truth_count[0][0]\n",
    "        #print('track: ' + str(p) + ', len: ' + str(len(idx)) + ', idx: ' + str(idx))\n",
    "        #print('truth particle: ' + str(truth_particle_id) + ', count:' + str(truth_count[0][1]))\n",
    "            \n",
    "        if angle>0:\n",
    "            idx = idx[np.argsort( z[idx])]\n",
    "        else:\n",
    "            idx = idx[np.argsort(-z[idx])]\n",
    "\n",
    "        ## start and end points  ##\n",
    "        idx0,idx1 = idx[0],idx[-1]\n",
    "        a0 = a[idx0]\n",
    "        a1 = a[idx1]\n",
    "        r0 = r[idx0]\n",
    "        r1 = r[idx1]\n",
    "        c0 = c[idx0]\n",
    "        c1 = c[idx1]\n",
    "        s0 = s[idx0]\n",
    "        s1 = s[idx1]\n",
    "\n",
    "        da0 = a[idx[1]] - a[idx[0]]  #direction\n",
    "        dr0 = r[idx[1]] - r[idx[0]]\n",
    "        direction0 = np.arctan2(dr0,da0)\n",
    "\n",
    "        da1 = a[idx[-1]] - a[idx[-2]]\n",
    "        dr1 = r[idx[-1]] - r[idx[-2]]\n",
    "        direction1 = np.arctan2(dr1,da1)\n",
    "\n",
    "        ## extend start point\n",
    "        ns = tree.query([[c0, s0, r0]], k=min(num_neighbours, min_num_neighbours), return_distance=False)\n",
    "        ns = np.concatenate(ns)\n",
    "\n",
    "        direction = np.arctan2(r0 - r[ns], a0 - a[ns])\n",
    "        diff = 1 - np.cos(direction - direction0)\n",
    "        ns = ns[(r0 - r[ns] > 0.01) & (diff < (1 - np.cos(limit)))]\n",
    "        for n in ns:\n",
    "            df_ix = hit_ids[n] - 1\n",
    "            # Un-comment this to see if we are extending the track properly\n",
    "            #is_good = (truth.loc[df_ix, 'particle_id'] == truth_particle_id)\n",
    "\n",
    "            outlier_modifier = 0.75\n",
    "            orig_label = labels[df_ix]\n",
    "            labels[df_ix] = p\n",
    "            new_score = score2.calculate_track_score(p, labels, hits, outlier_modifier=outlier_modifier, outlier_ix=df_ix)\n",
    "            labels[df_ix] = orig_label\n",
    "            if orig_label != 0:\n",
    "                orig_score = score2.calculate_track_score(orig_label, labels, hits, outlier_modifier=outlier_modifier, outlier_ix=df_ix)\n",
    "            else:\n",
    "                orig_score = 0\n",
    "\n",
    "            if new_score >= orig_score:\n",
    "                #df.loc[df_ix, 'track_id'] = p\n",
    "                labels[df_ix] = p\n",
    "                cur_track_len = cur_track_len + 1\n",
    "\n",
    "        ## extend end point\n",
    "        ns = tree.query([[c1, s1, r1]], k=min(num_neighbours, min_num_neighbours), return_distance=False)\n",
    "        ns = np.concatenate(ns)\n",
    "\n",
    "        direction = np.arctan2(r[ns] - r1, a[ns] - a1)\n",
    "        diff = 1 - np.cos(direction - direction1)\n",
    "  \n",
    "        ns = ns[(r[ns] - r1 > 0.01) & (diff < (1 - np.cos(limit)))]\n",
    "        for n in ns:  \n",
    "            df_ix = hit_ids[n] - 1\n",
    "            # Un-comment this to see if we are extending the track properly\n",
    "            #is_good = (truth.loc[df_ix, 'particle_id'] == truth_particle_id)\n",
    "\n",
    "            outlier_modifier=0.75\n",
    "            orig_label = labels[df_ix]\n",
    "            labels[df_ix] = p\n",
    "            new_score = score2.calculate_track_score(p, labels, hits, outlier_modifier=outlier_modifier, outlier_ix=df_ix)\n",
    "            labels[df_ix] = orig_label\n",
    "            if orig_label != 0:\n",
    "                orig_score = score2.calculate_track_score(orig_label, labels, hits, outlier_modifier=outlier_modifier, outlier_ix=df_ix)\n",
    "            else:\n",
    "                orig_score = 0\n",
    "\n",
    "            if new_score >= orig_score:\n",
    "                #df.loc[df_ix, 'track_id'] = p\n",
    "                labels[df_ix] = p\n",
    "                cur_track_len = cur_track_len + 1\n",
    "\n",
    "    df['track_id'] = labels\n",
    "\n",
    "    return df, count_good, count_bad, count_outlier_correct, count_outlier_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _one_cone_slice_orig(df, truth, angle, delta_angle, limit=0.04, num_neighbours=18):\n",
    "\n",
    "    df1 = df.loc[(df.arctan2>(angle - delta_angle)/180*np.pi) & (df.arctan2<(angle + delta_angle)/180*np.pi)]\n",
    "\n",
    "    min_num_neighbours = len(df1)\n",
    "    if min_num_neighbours < 3: \n",
    "        return df, 0, 0, 0, 0\n",
    "\n",
    "    hit_ids = df1.hit_id.values\n",
    "    x,y,z = df1.as_matrix(columns=['x', 'y', 'z']).T\n",
    "    r  = (x**2 + y**2)**0.5\n",
    "    r  = r/1000\n",
    "    a  = np.arctan2(y,x)\n",
    "    c = np.cos(a)\n",
    "    s = np.sin(a)\n",
    "    tree = KDTree(np.column_stack([c,s,r]), metric='euclidean')\n",
    "\n",
    "    track_ids = list(df1.track_id.unique())\n",
    "    num_track_ids = len(track_ids)\n",
    "    min_length=2\n",
    "    \n",
    "    labels = df.track_id.values\n",
    "    count_good = 0\n",
    "    count_bad = 0\n",
    "    count_outlier_correct = 0\n",
    "    count_outlier_wrong = 0\n",
    "\n",
    "    \n",
    "    for i in range(num_track_ids):\n",
    "        p = track_ids[i]\n",
    "        if p==0: continue\n",
    "\n",
    "        idx = np.where(df1.track_id==p)[0]\n",
    "        cur_track_len = len(idx)\n",
    "        if cur_track_len<min_length: continue\n",
    "\n",
    "        truth_ix = []\n",
    "        for ii in idx:\n",
    "            truth_ix.append(hit_ids[ii] - 1)\n",
    "        tdf = truth.loc[truth_ix]\n",
    "        truth_count = coll.Counter(tdf.particle_id.values).most_common(1)\n",
    "        truth_particle_id = truth_count[0][0]\n",
    "        #print('track: ' + str(p) + ', len: ' + str(len(idx)) + ', idx: ' + str(idx))\n",
    "        #print('truth particle: ' + str(truth_particle_id) + ', count:' + str(truth_count[0][1]))\n",
    "            \n",
    "        if angle>0:\n",
    "            idx = idx[np.argsort( z[idx])]\n",
    "        else:\n",
    "            idx = idx[np.argsort(-z[idx])]\n",
    "\n",
    "## start and end points  ##\n",
    "        idx0,idx1 = idx[0],idx[-1]\n",
    "        a0 = a[idx0]\n",
    "        a1 = a[idx1]\n",
    "        r0 = r[idx0]\n",
    "        r1 = r[idx1]\n",
    "        c0 = c[idx0]\n",
    "        c1 = c[idx1]\n",
    "        s0 = s[idx0]\n",
    "        s1 = s[idx1]\n",
    "\n",
    "        da0 = a[idx[1]] - a[idx[0]]  #direction\n",
    "        dr0 = r[idx[1]] - r[idx[0]]\n",
    "        direction0 = np.arctan2(dr0,da0)\n",
    "\n",
    "        da1 = a[idx[-1]] - a[idx[-2]]\n",
    "        dr1 = r[idx[-1]] - r[idx[-2]]\n",
    "        direction1 = np.arctan2(dr1,da1)\n",
    "\n",
    "        ## extend start point\n",
    "        ns = tree.query([[c0, s0, r0]], k=min(num_neighbours, min_num_neighbours), return_distance=False)\n",
    "        ns = np.concatenate(ns)\n",
    "\n",
    "        direction = np.arctan2(r0 - r[ns], a0 - a[ns])\n",
    "        diff = 1 - np.cos(direction - direction0)\n",
    "        ns = ns[(r0 - r[ns] > 0.01) & (diff < (1 - np.cos(limit)))]\n",
    "        for n in ns:\n",
    "            df_ix = hit_ids[n] - 1\n",
    "            is_good = (truth.loc[df_ix, 'particle_id'] == truth_particle_id)\n",
    "            #if truth.loc[df_ix, 'particle_id'] == truth_particle_id:\n",
    "            #    print('Found valid start extension!')\n",
    "            #else:\n",
    "            #    print('Found WRONG start extension!')\n",
    "                \n",
    "            #old_track = df.loc[df_ix, 'track_id']\n",
    "            old_track = labels[df_ix]\n",
    "            if old_track == 0:# and is_good:# and allow_overwrite:# and is_good:\n",
    "                #df.loc[df_ix, 'track_id'] = p\n",
    "                labels[df_ix] = p\n",
    "                if is_good:\n",
    "                    count_good = count_good + 1\n",
    "                else:\n",
    "                    count_bad = count_bad + 1\n",
    "                # increase cur_track_len?\n",
    "            elif old_track != 0:\n",
    "                # If the hit is already occupied by another track, only take ownership\n",
    "                # of the hit if our track is longer than the current-occupying track.\n",
    "                #existing_track_len = len(np.where(df.track_id==old_track)[0])\n",
    "                existing_track_len = len(np.where(labels==old_track)[0])\n",
    "                if cur_track_len > existing_track_len:\n",
    "                    #df.loc[df_ix, 'track_id'] = p\n",
    "                    labels[df_ix] = p\n",
    "                    if is_good:\n",
    "                        count_good = count_good + 1\n",
    "                    else:\n",
    "                        count_bad = count_bad + 1\n",
    "                else:\n",
    "                    if is_good:\n",
    "                        count_bad = count_bad + 1\n",
    "                    else:\n",
    "                        count_good = count_good + 1\n",
    "\n",
    "        ## extend end point\n",
    "        ns = tree.query([[c1, s1, r1]], k=min(num_neighbours, min_num_neighbours), return_distance=False)\n",
    "        ns = np.concatenate(ns)\n",
    "\n",
    "        direction = np.arctan2(r[ns] - r1, a[ns] - a1)\n",
    "        diff = 1 - np.cos(direction - direction1)\n",
    "  \n",
    "        ns = ns[(r[ns] - r1 > 0.01) & (diff < (1 - np.cos(limit)))]\n",
    "        for n in ns:  \n",
    "            df_ix = hit_ids[n] - 1\n",
    "            is_good = (truth.loc[df_ix, 'particle_id'] == truth_particle_id)\n",
    "            #if truth.loc[df_ix, 'particle_id'] == truth_particle_id:\n",
    "            #    print('Found valid end extension!')\n",
    "            #else:\n",
    "            #    print('Found WRONG end extension!')\n",
    "            #old_track = df.loc[df_ix, 'track_id']\n",
    "            old_track = labels[df_ix]\n",
    "            if old_track == 0:# and allow_overwrite:# and is_good:\n",
    "                #df.loc[df_ix, 'track_id'] = p\n",
    "                labels[df_ix] = p\n",
    "                if is_good:\n",
    "                    count_good = count_good + 1\n",
    "                else:\n",
    "                    count_bad = count_bad + 1\n",
    "            elif old_track != 0:\n",
    "                # If the hit is already occupied by another track, only take ownership\n",
    "                # of the hit if our track is longer than the current-occupying track.\n",
    "                #existing_track_len = len(np.where(df.track_id==old_track)[0])\n",
    "                existing_track_len = len(np.where(labels==old_track)[0])\n",
    "                if cur_track_len > existing_track_len:\n",
    "                    #df.loc[df_ix, 'track_id'] = p\n",
    "                    labels[df_ix] = p\n",
    "                    if is_good:\n",
    "                        count_good = count_good + 1\n",
    "                    else:\n",
    "                        count_bad = count_bad + 1\n",
    "                else:\n",
    "                    if is_good:\n",
    "                        count_bad = count_bad + 1\n",
    "                    else:\n",
    "                        count_good = count_good + 1\n",
    "                        \n",
    "    df['track_id'] = labels\n",
    "      \n",
    "    return df, count_good, count_bad, count_outlier_correct, count_outlier_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_proc = np.copy(labels_merged)\n",
    "labels_proc = perfect_inner_extension(labels_proc, hits, truth)\n",
    "one_submission = create_one_event_submission(event_id, hits, labels_proc)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Extension %d score for event %d: %.8f\" % (ix,event_id, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_i1 = pd.read_csv('../../best_csvs12/event_' + str(event_id) + '_labels_train_helix1_phase1_dbscan1.csv').label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_proc1 = np.copy(labels_i1)\n",
    "#track_extension_limits = [0.02, 0.04, 0.06, 0.08, 0.10]\n",
    "#for ix, limit in enumerate(track_extension_limits):\n",
    "#    labels_proc1 = extend_labelsi(ix, labels_proc1, hits, do_swap=(ix%2==1), limit=(limit))\n",
    "#    one_submission = create_one_event_submission(event_id, hits, labels_proc1)\n",
    "#    score = score_event(truth, one_submission)\n",
    "#    print(\"Extension1 %d score for event %d: %.8f\" % (ix, event_id, score))\n",
    "\n",
    "labels_proc2 = np.copy(labels_i1)\n",
    "track_extension_limits = [0.02, 0.04, 0.06, 0.08, 0.10]\n",
    "for ix, limit in enumerate(track_extension_limits):\n",
    "    labels_proc2 = extend_labelsi(ix, labels_proc2, hits, do_swap=(ix%2==1), limit=(limit))\n",
    "    one_submission = create_one_event_submission(event_id, hits, labels_proc2)\n",
    "    score = score_event(truth, one_submission)\n",
    "    print(\"Extension2 %d score for event %d: %.8f\" % (ix, event_id, score))\n",
    "# 0.56229910, 0.59463552, 0.60778460, 0.61425334, 0.61669246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_proc1 = np.copy(labels_proc2)\n",
    "track_extension_limits = [0.02, 0.04, 0.06, 0.08, 0.10]\n",
    "for ix, limit in enumerate(track_extension_limits):\n",
    "    labels_proc1 = extend_labelsi(ix, labels_proc1, hits, do_swap=(ix%2==1), limit=(limit))\n",
    "    one_submission = create_one_event_submission(event_id, hits, labels_proc1)\n",
    "    score = score_event(truth, one_submission)\n",
    "    print(\"Extension1 %d score for event %d: %.8f\" % (ix, event_id, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_proc = np.copy(labels_merged)\n",
    "track_extension_limits = [0.02, 0.04, 0.06, 0.08, 0.10]\n",
    "for ix, limit in enumerate(track_extension_limits):\n",
    "    labels_proc = extend_labelsi(ix, labels_proc, hits, do_swap=(ix%2==1), limit=(limit))\n",
    "    one_submission = create_one_event_submission(event_id, hits, labels_proc)\n",
    "    score = score_event(truth, one_submission)\n",
    "    print(\"Extension %d score for event %d: %.8f\" % (ix, event_id, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perfect_inner_extension(labels, hits, truth):\n",
    "    labels = np.copy(labels)\n",
    "    truth['z_abs'] = truth.tz.abs()\n",
    "    tracks = np.unique(labels)\n",
    "    count_extended = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 5:\n",
    "            tdf = truth.loc[track_hits]\n",
    "            tdf = tdf.sort_values('z_abs')\n",
    "            zs = tdf.z_abs.values\n",
    "            truth_count = coll.Counter(tdf.particle_id.values).most_common(1)\n",
    "            truth_particle_id = truth_count[0][0]\n",
    "            tr2_df = truth.loc[truth['particle_id'] == truth_particle_id]\n",
    "            tr2_df = tr2_df.sort_values('z_abs')\n",
    "            tr2_ixes = tr2_df.index.values\n",
    "            for tr2_ix in tr2_ixes:\n",
    "                if tr2_df.loc[tr2_ix].z_abs > zs[0] and tr2_df.loc[tr2_ix].z_abs < zs[-1]:\n",
    "                    count_extended = count_extended + 1\n",
    "                    labels[tr2_ix] = track\n",
    "\n",
    "    print('perfect count_extended: ' + str(count_extended))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_helix1_filter = merge.remove_outliers(labels_helix1, hits, smallest_track_size=2, print_counts=True)\n",
    "#labels_helix2_filter = merge.remove_outliers(labels_helix2, hits, smallest_track_size=2, print_counts=True)\n",
    "labels_merged = merge.heuristic_merge_tracks(labels_helix1_filter, labels_helix2_filter, overwrite_limit=3)\n",
    "#labels_merged = merge.heuristic_merge_tracks(labels_merged, labels_helix3_filter)\n",
    "one_submission = create_one_event_submission(event_id, hits, labels_merged)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merged score for event %d: %.8f\" % (event_id, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_submission = create_one_event_submission(1000, hits, labels_helix)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"helix score for event %d: %.8f\" % (1000, score))\n",
    "#one_submission = create_one_event_submission(1000, hits, labels_cone)\n",
    "#score = score_event(truth, one_submission)\n",
    "#print(\"cone score for event %d: %.8f\" % (1000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_simple_merge = merge.merge_tracks(labels_helix, labels_cone)\n",
    "one_submission = create_one_event_submission(1000, hits, labels_simple_merge)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Simple merged cone+helix score for event %d: %.8f\" % (1000, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Removing cone outliers')\n",
    "labels_c3 = np.copy(labels_cone)\n",
    "labels_c3 = remove_outliers(labels_c3, hits, print_counts=True)\n",
    "\n",
    "print('Removing helix outliers')\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "labels_h3 = remove_outliers(labels_h3, hits, print_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Merging tracks')\n",
    "tmp_labels = merge.heuristic_merge_tracks(labels_helix, labels_cone)\n",
    "one_submission = create_one_event_submission(1000, hits, tmp_labels)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"cone score for event %d: %.8f\" % (1000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Merging tracks')\n",
    "tmp_labels = merge.heuristic_merge_tracks(labels_h3, labels_c3)\n",
    "one_submission = create_one_event_submission(1000, hits, tmp_labels)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merged score for event %d: %.8f\" % (1000, score))\n",
    "# Baseline helix prior to outlier removal: 0.51217316\n",
    "# Score: 0.4766-->0.4799 after improved outlier removal\n",
    "# Score is 0.5037 if, after outlier removal,  we only add tracks from labels_c3 when no such track was recorded in labels_h3\n",
    "# Score is 0.51087490 if we only remove bad volumes, duplicatez, and singletons, and only add tracks from labels_c3 when no such track was recorded in labels_h3\n",
    "# Score is 0.51936460, remove badv, dupz, sings, selective track merging.\n",
    "# Score is 0.52234651, remove badv, dupz, sings, selective track merging.\n",
    "# Score is 0.52499622, remove badv, dupz, sings, selective track merging, overwrite smaller tracks of length <= 3.\n",
    "# Score is 0.52653574, same as above except overwrite tracks of length <= 4\n",
    "# Score is 0.52622554, same as above except overwrite tracks of length <= 5\n",
    "# Score is 0.52209245, full outlier removal, overwrite tracks of length <= 4\n",
    "# Score is 0.58658664 orig heuristic\n",
    "#  --> 0.58240360 with aggressive outlier removal\n",
    "#  --> 0.58621259 with aggressive cone removal, non-aggressive helix\n",
    "#  --> 0.58417970 with aggressive helix removal, non-aggressive cone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: Need to evaluate this better, seems to hurt!\n",
    "def find_invalid_volumes(track, labels, df):\n",
    "    invalid_ix = []\n",
    "\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "    df2 = df.loc[hit_ix]\n",
    "    df2 = df2.sort_values('z_abs')\n",
    "    hit_ix2 = df2.index.values\n",
    "    all_positive = np.all(df2.z.values >= 0)\n",
    "    all_negative = np.all(df2.z.values <= 0)\n",
    "    volumes = df2.volume_id.values\n",
    "    layers = df2.layer_id.values\n",
    "    last_volume = volumes[0]\n",
    "    last_layer = layers[0]\n",
    "    # Tracks with the first volume of 8, 13, and 17 are very odd, sometimes\n",
    "    # they hit in the negative way, sometimes the positive way,\n",
    "    # sometimes a mix of both. Ignore these.\n",
    "    if last_volume == 8 or last_volume == 13 or last_volume == 17:\n",
    "        all_negative = False\n",
    "        all_positive = False\n",
    "    for idx, cur_vol in enumerate(volumes):\n",
    "        cur_layer = layers[idx]\n",
    "        if all_positive:\n",
    "            # When we go from one volume to the next, we expect to see the\n",
    "            # layer drop from a large layer id to a smaller layer id.\n",
    "            # If we stay in the same volume, the layer id should not decrease.\n",
    "            #if (last_volume != cur_vol and (cur_layer > (last_layer - 4))) or (last_volume == cur_vol and cur_layer < last_layer):\n",
    "            if (last_volume == cur_vol and cur_layer < last_layer):\n",
    "                invalid_ix.append(hit_ix2[idx])\n",
    "            else:\n",
    "                last_volume = cur_vol\n",
    "                last_layer = cur_layer\n",
    "        elif all_negative:\n",
    "            # When we go from one volume to the next, we expect to see the\n",
    "            # layer increase from a small layer id to a larger layer id.\n",
    "            # If we stay in the same volume, the layer id should not increase.\n",
    "            #if (last_volume != cur_vol and (cur_layer < (last_layer + 4))) or (last_volume == cur_vol and cur_layer > last_layer):\n",
    "            if (last_volume == cur_vol and cur_layer > last_layer):\n",
    "                invalid_ix.append(hit_ix2[idx])\n",
    "            else:\n",
    "                last_volume = volumes[idx]\n",
    "                last_layer = layers[idx]\n",
    "        else:\n",
    "            last_volume = cur_vol\n",
    "            last_layer = cur_layer\n",
    "\n",
    "    return invalid_ix\n",
    "    \n",
    "def find_dimension_outlier(track, labels, df, dimension):\n",
    "    outlier_ix = []\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "\n",
    "    # Need at least 3 values to determine if any look like outliers\n",
    "    if len(hit_ix) < 3:\n",
    "        return outlier_ix\n",
    "\n",
    "    df2 = df.loc[hit_ix]        \n",
    "    df2 = df2.sort_values('z')\n",
    "    hit_ix2 = df2.index.values\n",
    "\n",
    "    # Note, diff[0] is diff between 0 and 1\n",
    "    diffs = np.diff(df2[dimension].values)\n",
    "\n",
    "    growing_trend = 0\n",
    "    shrinking_trend = 0\n",
    "    for idx, diff in enumerate(diffs):\n",
    "        if idx > 0 and diff > diffs[idx-1]:\n",
    "            growing_trend = growing_trend + 1\n",
    "        if idx > 0 and diff < diffs[idx-1]:\n",
    "            shrinking_trend = shrinking_trend + 1\n",
    "\n",
    "    check_largest_and_smallest = True\n",
    "    if growing_trend > math.ceil(0.6*len(diffs)) or shrinking_trend > math.ceil(0.6*len(diffs)):\n",
    "        check_largest_and_smallest = False\n",
    "\n",
    "    if check_largest_and_smallest:\n",
    "        # Find largest and smallest diffs, if largest is 20x larger than 2nd largest,\n",
    "        # or smallest is 20x smaller than 2nd smallest, consider them outliers.\n",
    "        top_two_ix = diffs.argsort()[-2:][::-1]\n",
    "        large1 = diffs[top_two_ix[0]]\n",
    "        large2 = diffs[top_two_ix[1]]\n",
    "        bot_two_ix = diffs.argsort()[:2]\n",
    "        small1 = diffs[bot_two_ix[0]]\n",
    "        small2 = diffs[bot_two_ix[1]]\n",
    "\n",
    "        largest_is_outlier = False\n",
    "        smallest_is_outlier = False\n",
    "        if large1 > 0 and large2 > 0 and large1 > 10.0 and large2 > 2.0 and (large2*7) < large1:\n",
    "            largest_is_outlier = True\n",
    "        if large1 < 0 and large2 < 0 and large1 < -10.0 and large2 < -2.0 and (large1*7) > large2:\n",
    "            largest_is_outlier = True\n",
    "        if small1 > 0 and small2 > 0 and small1 > 10.0 and small2 > 2.0 and (small2*7) < small1:\n",
    "            smallest_is_outlier = True\n",
    "        if small1 < 0 and small2 < 0 and small1 < -10.0 and small2 < -2.0 and (small1*7) > small2:\n",
    "            smallest_is_outlier = True\n",
    "\n",
    "        if largest_is_outlier or smallest_is_outlier:\n",
    "            hit_ix_list = hit_ix.tolist()\n",
    "            for idx, diff in enumerate(diffs):\n",
    "                if (largest_is_outlier and diff == large1) or (smallest_is_outlier and diff == small1):\n",
    "                    #print('Removing extreme outlier diff: ' + str(diff) + ', ix: ' + str(hit_ix2[idx + 1]) + ', from diffs: ' + str(diffs))\n",
    "                    outlier_ix.append(hit_ix2[idx + 1])\n",
    "                    hit_ix_list.remove(hit_ix2[idx + 1])\n",
    "\n",
    "            # Re-generate the diffs now that we've removed the extreme outliers\n",
    "            hit_ix = np.asarray(hit_ix_list)\n",
    "            if len(hit_ix) < 3:\n",
    "                return outlier_ix\n",
    "            df2 = df.loc[hit_ix]        \n",
    "            df2 = df2.sort_values('z')\n",
    "            hit_ix2 = df2.index.values\n",
    "            diffs = np.diff(df2[dimension].values)\n",
    "                \n",
    "    # Restrict to when the majority (75%+) of diffs are all in same direction\n",
    "    neg_diffs = np.where(diffs < 0)[0]\n",
    "    pos_diffs = np.where(diffs >= 0)[0]\n",
    "\n",
    "    #print(df2[dimension].values)\n",
    "    #print(hit_ix)\n",
    "    #print('trk: ' + str(track) + ', diffs: ' + str(diffs))\n",
    "    #print(neg_diffs)\n",
    "    #print(pos_diffs)\n",
    "    #print(df2)\n",
    "\n",
    "    # Restrict to when the majority of diffs are either positive or negative.\n",
    "    # (more difficult to detect outliers if diffs oscillate -ve and +ve)\n",
    "    dim_vals = df2[dimension].values\n",
    "    if len(neg_diffs) >= math.ceil(0.75*len(diffs)):\n",
    "        # remove large positive ones.\n",
    "        growing_trend = 0\n",
    "        previous_diff = 0\n",
    "        for idx, diff in enumerate(diffs):\n",
    "            # Some tracks trend from negative to positive diffs, don't eliminate\n",
    "            # positive values if it looks like we are trending that way.\n",
    "            if idx > 0 and diff > diffs[idx-1]:\n",
    "                growing_trend = growing_trend + 1\n",
    "                if growing_trend > 2:\n",
    "                    break\n",
    "            else:\n",
    "                growing_trend = 0\n",
    "            # Use absolute value > 1.0 in case there is small delta each time,\n",
    "            # we only try to find big jumps in the wrong direction.\n",
    "            #print('nidx-' + dimension + ': ' + str(idx) + ', diff: ' + str(diff) + ', df ix: ' + str(hit_ix2[idx+1]))\n",
    "            if diff > 1.0:\n",
    "                # We sometimes see cases like:\n",
    "                # diff[n-1] = -22\n",
    "                # diff[n] = 12\n",
    "                # diff[n+1] = -14\n",
    "                # In this case, we want to remove n-1 as the outlier, since if that\n",
    "                # was gone, diff[n] would be -10, which is more reasonable.\n",
    "                # In cases where we see:\n",
    "                # diff[0] = 23\n",
    "                # diff[1] = -5\n",
    "                # We want to check the dimension values directly instead of the diffs, it\n",
    "                # could be that val[0] is the outlier.\n",
    "                if idx == 0 and dim_vals[1] > dim_vals[2] and dim_vals[0] < dim_vals[2]:\n",
    "                    # The real outlier is the first entry in the list.\n",
    "                    outlier_ix.append(hit_ix2[0])\n",
    "                elif idx == 0 or idx == (len(diffs)-1) or ((diff + diffs[idx-1]) > 0) or diffs[idx+1] > 0:\n",
    "                    #print('Removing: ' + str(hit_ix2[idx+1]))\n",
    "                    outlier_ix.append(hit_ix2[idx + 1])\n",
    "                else:\n",
    "                    # The real outlier is the previous one (i.e. diff[n-1] in the example above!\n",
    "                    outlier_ix.append(hit_ix2[idx])\n",
    "    \n",
    "    elif len(pos_diffs) >= math.ceil(0.75*len(diffs)):\n",
    "        # remove large negative ones\n",
    "        shrinking_trend = 0\n",
    "        for idx, diff in enumerate(diffs):\n",
    "            # Some tracks trend from positive to negative diffs, don't eliminate\n",
    "            # negative values if it looks like we are trending that way.\n",
    "            if idx > 0 and diff < diffs[idx-1]:\n",
    "                shrinking_trend = shrinking_trend + 1\n",
    "                if shrinking_trend > 2:\n",
    "                    break\n",
    "            else:\n",
    "                shrinking_trend = 0\n",
    "            # Use absolute value > 1.0 in case there is small delta each time,\n",
    "            # we only try to find big jumps in the wrong direction.\n",
    "            #print('pidx-' + dimension + ': ' + str(idx) + ', diff: ' + str(diff) + ', df ix: ' + str(hit_ix2[idx+1]))\n",
    "            if diff < -1.0:\n",
    "                #print('Removing: ' + str(hit_ix2[idx+1]))\n",
    "                # Similar to the negative case above, make sure we remove the real\n",
    "                # outlier, in case the previous diff was misleading.\n",
    "                if idx == 0 and dim_vals[1] < dim_vals[2] and dim_vals[0] > dim_vals[2]:\n",
    "                    # The real outlier is the first entry in the list.\n",
    "                    outlier_ix.append(hit_ix2[0])\n",
    "                elif idx == 0 or idx == (len(diffs)-1) or ((diff + diffs[idx-1]) < 0) or diffs[idx+1] < 0:\n",
    "                    #print('Removing: ' + str(hit_ix2[idx+1]))\n",
    "                    outlier_ix.append(hit_ix2[idx + 1])\n",
    "                else:\n",
    "                    # The real outlier is the previous one (i.e. diff[n-1] in the example above!\n",
    "                    outlier_ix.append(hit_ix2[idx])\n",
    "\n",
    "\n",
    "\n",
    "    # Future ideas for patterns:\n",
    "    # - average positive jump + average negative jump, for values that oscillate +ve and -ve\n",
    "    # - absolute value of jump in same direction, this is hard since some tracks seem jumpy\n",
    "    #   i.e. small diffs followed by a bigger jump, then smaller diffs. May need to tie that\n",
    "    #   in with volume/layer/module ids, i.e. only allow bigger jumps between layers.\n",
    "    return outlier_ix                \n",
    "\n",
    "def find_duplicate_z(track, labels, df):\n",
    "    def number_is_between(a1, a2, a3):\n",
    "        return (a1 >= a2 and a2 >= a3) or (a1 <= a2 and a2 <= a3)\n",
    "\n",
    "    def numbers_are_between(a1, a2, a3, b1, b2, b3):\n",
    "        return number_is_between(a1, a2, a3) and number_is_between(b1, b2, b3)\n",
    "\n",
    "    duplicatez_ix = []\n",
    "    hit_ix = np.where(labels == track)[0]\n",
    "\n",
    "    # Need at least 4 values to be able to evaluate duplicate z-values.\n",
    "    if len(hit_ix) < 4:\n",
    "        return duplicatez_ix\n",
    "\n",
    "    df2 = df.loc[hit_ix]        \n",
    "    df2 = df2.sort_values('z_abs')\n",
    "    hit_ix2 = df2.index.values # remember new indexes after sorting\n",
    "    xs = df2.x.values\n",
    "    ys = df2.y.values\n",
    "    zs = df2.z.values\n",
    "    max_idx = len(zs) - 1\n",
    "\n",
    "    z_counts = coll.Counter(df2.z.values).most_common(len(df2.z.values))\n",
    "\n",
    "    if zs[0] == zs[1]:\n",
    "        # zs at the beginning\n",
    "        x1 = xs[2]\n",
    "        x2 = xs[3]\n",
    "        y1 = ys[2]\n",
    "        y2 = ys[3]\n",
    "        if numbers_are_between(xs[0], x1, x2, ys[0], y1, y2) and not numbers_are_between(xs[1], x1, x2, ys[1], y1, y2):\n",
    "            # The first one is more consistent, delete the 2nd duplicate value\n",
    "            duplicatez_ix.append(hit_ix2[1])\n",
    "            #print('xs[1] ' + str(xs[1]) + ' <= x1 ' + str(x1) + ' <= x2 ' + str(x2))\n",
    "            #print('ys[1] ' + str(ys[1]) + ' <= y1 ' + str(y1) + ' <= y2 ' + str(y2))\n",
    "        elif numbers_are_between(xs[1], x1, x2, ys[1], y1, y2) and not numbers_are_between(xs[0], x1, x2, ys[0], y1, y2):\n",
    "            # The second one is more consistent, delete the 1st duplicate value\n",
    "            duplicatez_ix.append(hit_ix2[0])\n",
    "            #print('b')\n",
    "        elif numbers_are_between(xs[0], x1, x2, ys[0], y1, y2) and numbers_are_between(xs[1], x1, x2, ys[1], y1, y2):\n",
    "            # Both z-values seem reasonable, need a tie-breaker to find out which is the right one.\n",
    "            add_code_here = True\n",
    "        # else, neither seem valid, unsure how to proceed, better off not rejecting any.\n",
    "\n",
    "    if zs[-1] == zs[-2]:\n",
    "        # zs at the end\n",
    "        x1 = xs[-4]\n",
    "        x2 = xs[-3]\n",
    "        y1 = ys[-4]\n",
    "        y2 = ys[-3]\n",
    "        if numbers_are_between(x1, x2, xs[-2], y1, y2, ys[-2]) and not numbers_are_between(x1, x2, xs[-1], y1, y2, ys[-1]):\n",
    "            # The first one is more consistent, delete the last duplicate value\n",
    "            duplicatez_ix.append(hit_ix2[-1])\n",
    "        elif numbers_are_between(x1, x2, xs[-1], y1, y2, ys[-1]) and not numbers_are_between(x1, x2, xs[-2], y1, y2, ys[-2]):\n",
    "            # The last one is more consistent, delete the 1st duplicate value\n",
    "            duplicatez_ix.append(hit_ix2[-2])\n",
    "        elif numbers_are_between(x1, x2, xs[-1], y1, y2, ys[-1]) and numbers_are_between(x1, x2, xs[-2], y1, y2, ys[-2]):\n",
    "            # Both z-values seem reasonable, need a tie-breaker to find out which is the right one.\n",
    "            add_code_here = True\n",
    "        # else, neither seem valid, unsure how to proceed, better off not rejecting any.\n",
    "        \n",
    "    # Idea: Find duplicate adjacent z-values. Remember x and y before and after the\n",
    "    # duplicates. Choose z that lies between the two. If z at beginning or end,\n",
    "    # need the two post (or pre-) x/y values to see the expected sign of the diff.\n",
    "\n",
    "    # Note max_idx is largest valid index, we already handled the case where the\n",
    "    # duplicate zs are at the beginning or end of the list.\n",
    "    for idx in range(0, max_idx):\n",
    "        if idx > 0 and (idx+2) <= max_idx and zs[idx] == zs[idx+1]:\n",
    "            x1 = xs[idx-1]\n",
    "            x2 = xs[idx+2]\n",
    "            y1 = ys[idx-1]\n",
    "            y2 = ys[idx+2]\n",
    "            # now, x1 <= z1 <= x2, and y1 <= z1 <= y2\n",
    "            if numbers_are_between(x1, xs[idx], x2, y1, ys[idx], y2) and not numbers_are_between(x1, xs[idx+1], x2, y1, ys[idx+1], y2):\n",
    "                # The first one is more consistent, delete the 2nd duplicate value\n",
    "                duplicatez_ix.append(hit_ix2[idx+1])\n",
    "            elif numbers_are_between(x1, xs[idx+1], x2, y1, ys[idx+1], y2) and not numbers_are_between(x1, xs[idx], x2, y1, ys[idx], y2):\n",
    "                # The second one is more consistent, delete the 1st duplicate value\n",
    "                duplicatez_ix.append(hit_ix2[idx])\n",
    "            elif numbers_are_between(x1, xs[idx], x2, y1, ys[idx], y2) and numbers_are_between(x1, xs[idx+1], x2, y1, ys[idx+1], y2):\n",
    "                # Both z-values seem reasonable, need a tie-breaker to find out which is the right one.\n",
    "                add_code_here = True\n",
    "            # else, neither seem valid, unsure how to proceed, better off not rejecting any.\n",
    "\n",
    "    #if z_counts[0][1] > 1:\n",
    "    #    print('Duplicatez found on track ' + str(track) + ', removed: ' + str(duplicatez_ix))\n",
    "\n",
    "    return duplicatez_ix\n",
    "\n",
    "# TODO pi, -pi discontinuity \n",
    "def remove_track_outliers_slope(track, labels, hits, debug=False):\n",
    "    final_outliers = []\n",
    "\n",
    "    hhh_ix = np.where(labels == track)\n",
    "    hhh_h = hits.loc[hhh_ix].sort_values('z')\n",
    "    \n",
    "    slopes_backward = []\n",
    "    slopes_forward = []\n",
    "\n",
    "    num_hits = len(hhh_h)\n",
    "    # Only reliable with tracks >= 5 hits\n",
    "    if num_hits < 5:\n",
    "        return final_outliers\n",
    "\n",
    "    if debug: print('backward:')\n",
    "    for i in np.arange(num_hits-1,0,-1):\n",
    "        a0 =  hhh_h.a0.values[i]\n",
    "        a1 =  hhh_h.a0.values[i-1]\n",
    "        r0 =  hhh_h.r.values[i]\n",
    "        r1 =  hhh_h.r.values[i-1]\n",
    "        if r0 == r1:\n",
    "            r0 = r0 + 1e-8\n",
    "        slope = (a0-a1)/(r0-r1) \n",
    "        slopes_backward.append(slope)\n",
    "        if debug: print(hhh_h.hit_id.values[i], slope, a0)\n",
    "        if i == 1:\n",
    "            a0 = hhh_h.a0.values[0]\n",
    "            a1 = hhh_h.a0.values[num_hits-1]\n",
    "            r0 =  hhh_h.r.values[0]\n",
    "            r1 =  hhh_h.r.values[num_hits-1]\n",
    "            if r0 == r1:\n",
    "                r0 = r0 + 1e-8\n",
    "            slope = (a0-a1)/(r0-r1)\n",
    "            slopes_backward.append(slope)\n",
    "            if debug: print(hhh_h.hit_id.values[0], slope, a1)\n",
    "\n",
    "    if debug: print('forward:')\n",
    "    for i in np.arange(0,num_hits-1,1):\n",
    "        a0 =  hhh_h.a0.values[i]\n",
    "        a1 =  hhh_h.a0.values[i+1]\n",
    "        r0 =  hhh_h.r.values[i]\n",
    "        r1 =  hhh_h.r.values[i+1]\n",
    "        if r0 == r1:\n",
    "            r1 = r1 + 1e-8\n",
    "        slope = (a1-a0)/(r1-r0) \n",
    "        slopes_forward.append(slope)\n",
    "        if debug: print(hhh_h.hit_id.values[i], slope, a0)\n",
    "\n",
    "        if i == num_hits-2:\n",
    "            a0 = hhh_h.a0.values[0]\n",
    "            a1 = hhh_h.a0.values[num_hits-1]\n",
    "            r0 =  hhh_h.r.values[0]\n",
    "            r1 =  hhh_h.r.values[num_hits-1]\n",
    "            if r0 == r1:\n",
    "                r1 = r1 + 1e-8\n",
    "            slope = (a1-a0)/(r1-r0) \n",
    "            slopes_forward.append(slope)\n",
    "            if debug: print(hhh_h.hit_id.values[num_hits-1], slope, a0)\n",
    "\n",
    "    slopes_backward = np.asarray(slopes_backward)\n",
    "    slopes_backward = np.reshape(slopes_backward, (-1, 1))\n",
    "    slopes_forward = np.asarray(slopes_forward)\n",
    "    slopes_forward = np.reshape(slopes_forward, (-1, 1))\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    X_back = ss.fit_transform(slopes_backward)\n",
    "    X_for = ss.fit_transform(slopes_forward)\n",
    "\n",
    "    cl = DBSCAN(eps=0.0033, min_samples=1)\n",
    "    outlier_labels_backward = cl.fit_predict(X_back)\n",
    "    outlier_labels_forward = cl.fit_predict(X_for)\n",
    "\n",
    "    if debug: print(outlier_labels_backward)\n",
    "    if debug: print(outlier_labels_forward)\n",
    "\n",
    "    track_counts = coll.Counter(outlier_labels_backward).most_common(1)\n",
    "    most_common_id = track_counts[0][0]\n",
    "    most_common_count = track_counts[0][1]\n",
    "\n",
    "    outlier_indices_backward = []\n",
    "    if most_common_count > 1 and len(np.unique(outlier_labels_forward)) < num_hits/2:\n",
    "        for i in np.arange(num_hits-1,-1,-1):\n",
    "            if outlier_labels_backward[i] != most_common_id:\n",
    "                if debug: print(hhh_h.index.values[num_hits-1-i])\n",
    "                outlier_indices_backward.append(hhh_h.index.values[num_hits-1-i])\n",
    "\n",
    "    track_counts = coll.Counter(outlier_labels_forward).most_common(1)\n",
    "    most_common_id = track_counts[0][0]\n",
    "    most_common_count = track_counts[0][1]\n",
    "\n",
    "\n",
    "    outlier_indices_forward = []\n",
    "    if most_common_count > 1 and len(np.unique(outlier_labels_forward)) < num_hits/2:\n",
    "        for i in np.arange(0,num_hits-1,1):\n",
    "            if outlier_labels_forward[i] != most_common_id:\n",
    "                if debug: print(hhh_h.index.values[i])\n",
    "                outlier_indices_forward.append(hhh_h.index.values[i])\n",
    "\n",
    "\n",
    "    outlier_candidates = list(set(outlier_indices_backward).intersection(outlier_indices_forward))\n",
    "\n",
    "\n",
    "    if debug: print('before removal:' + str(outlier_candidates))\n",
    "\n",
    "    for i in range(len(outlier_candidates)):\n",
    "        candidate = hhh_h.loc[outlier_candidates[i]]\n",
    "        found = False\n",
    "        for index, row in hhh_h.iterrows():\n",
    "            if np.absolute(candidate.z-row.z) == 0.5 and candidate.volume_id == row.volume_id \\\n",
    "            and candidate.layer_id == row.layer_id and candidate.module_id != row.module_id:\n",
    "                # true hits\n",
    "                if debug: print('true hit' + str(outlier_candidates[i]))\n",
    "                found = True\n",
    "        if found is False:\n",
    "            final_outliers.append(outlier_candidates[i])\n",
    "\n",
    "    if debug: print('new loutliers:' + str(final_outliers))\n",
    "\n",
    "    # If we determine that half (or more) of the hits need to be removed, we may have messed\n",
    "    # up, so do not return any outliers.\n",
    "    max_removal_threshold = math.floor(num_hits/2)\n",
    "    if len(final_outliers) >= max_removal_threshold:\n",
    "        final_outliers = []\n",
    "\n",
    "    return final_outliers\n",
    "\n",
    "    \n",
    "def remove_track_outliers(track, labels, hits, aggressive):\n",
    "    labels = np.copy(labels)\n",
    "    found_bad_volume = 0\n",
    "    found_bad_dimension = 0\n",
    "    found_bad_slope = 0\n",
    "    found_bad_z = 0\n",
    "\n",
    "    # Check if the sorted hits (on z-axis) go through the volumes\n",
    "    # and layers in the expected order\n",
    "    bad_volume_ix = find_invalid_volumes(track, labels, hits)\n",
    "    if len(bad_volume_ix) > 0:\n",
    "        #print('track ' + str(track) + ' bad volume: ' + str(bad_volume_ix))\n",
    "        found_bad_volume = found_bad_volume + len(bad_volume_ix)\n",
    "        for bvix in bad_volume_ix:\n",
    "            labels[bvix] = 0\n",
    "\n",
    "    if True:\n",
    "        # Check if the sorted hits (on z-axis) go through the volumes\n",
    "        # and layers in the expected order\n",
    "        duplicatez_ix = find_duplicate_z(track, labels, hits)\n",
    "        if len(duplicatez_ix) > 0:\n",
    "            #print('track ' + str(track) + ' duplicate z: ' + str(duplicatez_ix))\n",
    "            found_bad_z = found_bad_z + len(duplicatez_ix)\n",
    "            for bzix in duplicatez_ix:\n",
    "                labels[bzix] = 0\n",
    "\n",
    "    if True:\n",
    "        # Check the helix slope, discard hits that do not match\n",
    "        outlier_slope_ix = remove_track_outliers_slope(track, labels, hits)\n",
    "        if len(outlier_slope_ix) > 0:\n",
    "            #print('track ' + str(track) + ' slope outliers: ' + str(outlier_slope_ix))\n",
    "            found_bad_slope = found_bad_slope + len(outlier_slope_ix)\n",
    "            for oix in outlier_slope_ix:\n",
    "                labels[oix] = 0\n",
    "\n",
    "    if aggressive:\n",
    "        # Next analysis, from remaining hits, sort by 'z' (roughly time-based),\n",
    "        # check for anomolies in other dimensions.\n",
    "        outlier_ix = find_dimension_outlier(track, labels, hits, 'y')\n",
    "        if len(outlier_ix) > 0:\n",
    "            #print('track ' + str(track) + ' outlier dimension y: ' + str(outlier_ix))\n",
    "            found_bad_dimension = found_bad_dimension + len(outlier_ix)\n",
    "            for oix in outlier_ix:\n",
    "                labels[oix] = 0\n",
    "\n",
    "        # Next analysis, from remaining hits, sort by 'z' (roughly time-based),\n",
    "        # check for anomolies in z dimensions (i.e. outliers at beginning/end)\n",
    "        outlier_ix = find_dimension_outlier(track, labels, hits, 'z')\n",
    "        if len(outlier_ix) > 0:\n",
    "            #print('track ' + str(track) + ' outlier dimension z: ' + str(outlier_ix))\n",
    "            found_bad_dimension = found_bad_dimension + len(outlier_ix)\n",
    "            for oix in outlier_ix:\n",
    "                labels[oix] = 0\n",
    "            \n",
    "    return (labels, found_bad_volume, found_bad_dimension, found_bad_z, found_bad_slope)\n",
    "\n",
    "def remove_outliers(labels, hits, aggressive=False, print_counts=True):\n",
    "    tracks = np.unique(labels)\n",
    "    hits['z_abs'] = hits.z.abs()\n",
    "    hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "    hits['a0'] = np.arctan2(hits.y,hits.x)\n",
    "    count_rem_volume = 0\n",
    "    count_rem_dimension = 0\n",
    "    count_duplicatez = 0\n",
    "    count_rem_slope = 0\n",
    "    count_singletons = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 3:\n",
    "            (labels, c1, c2, c3, c4) = remove_track_outliers(track, labels, hits, aggressive)\n",
    "            count_rem_volume = count_rem_volume + c1\n",
    "            count_rem_dimension = count_rem_dimension + c2\n",
    "            count_duplicatez = count_duplicatez + c3\n",
    "            count_rem_slope = count_rem_slope + c4\n",
    "\n",
    "    # Remove singletons, we do not get any score for those. This is done\n",
    "    # last, in case removing the outliers (above) removed enough hits\n",
    "    # from a track to create a new singleton.\n",
    "    tracks = np.unique(labels)\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) == 1:\n",
    "            count_singletons = count_singletons + 1\n",
    "            labels[track_hits[0]] = 0\n",
    "\n",
    "    if print_counts:\n",
    "        print('Total removed due to bad volumes: ' + str(count_rem_volume))\n",
    "        print('Total removed due to bad dimensions: ' + str(count_rem_dimension))\n",
    "        print('Total removed due to duplicate zs: ' + str(count_duplicatez))\n",
    "        print('Total removed due to bad slopes: ' + str(count_rem_slope))\n",
    "        print('Total removed singleton hits: ' + str(count_singletons))\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_h3 = np.copy(labels_helix)\n",
    "\n",
    "#one_submission = create_one_event_submission(1000, hits, labels_h3)\n",
    "#score = score_event(truth, one_submission)\n",
    "#print(\"Initial score before outlier removal for event %d: %.8f\" % (1000, score))\n",
    "\n",
    "labels_h3 = remove_outliers(labels_h3, hits)\n",
    "one_submission = create_one_event_submission(1000, hits, labels_h3)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Initial score after outlier removal for event %d: %.8f\" % (1000, score))\n",
    "\n",
    "# before score: 0.57485796\n",
    "# after old outlier removal: 0.57471551\n",
    "# after slope removal: 0.56906582 (1006 removed due to bad slopes)\n",
    "# after slope removal + threshold: 0.56966146 (962 removed due to bad slopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with the track number to find a track that contains multiple particles\n",
    "hhh_ix = np.where(labels_helix == 5944)\n",
    "print(hhh_ix)\n",
    "hhh_t = truth.loc[hhh_ix]\n",
    "hhh_t = hhh_t.sort_values('tz')\n",
    "print(hhh_t)\n",
    "\n",
    "# Now add r/a0 as columns to the hits, and see if there's a way to detect\n",
    "# which hits belong to the track, and which should be considered outliers.\n",
    "hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "hits['a0'] = np.arctan2(hits.y,hits.x)\n",
    "hhh_h = hits.loc[hhh_ix]\n",
    "\n",
    "\n",
    "#hhh_h = hhh_h[(hhh_h.hit_id != 15459) & (hhh_h.hit_id != 16190) & (hhh_h.hit_id != 16155) ]\n",
    "\n",
    "hhh_h = hhh_h.sort_values('z')\n",
    "print(hhh_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dupz -> only remove when value ends in .0000 or .5000?\n",
    "# 59828: a: 0,025628, r: 13,745506, slope=a/r=0,001864463920062, 59825: a: 0,025181, r: 13,590172, slope=0,001852883098168\n",
    "# 67285: r: -35,053009, a: -0,072102, slope=0,002056941816322, 67284: r: -46,909088, a: -0,088433, slope=0,001885199729315\n",
    "# 59036: r: 25,316536, a: 0,041827, slope=0,001652161259344, 59007: r: 11,899605, a: 0,019226, 0,001615683881944\n",
    "# Ones that we had trouble with for slope outlier detection:\n",
    "#x 11/6 hits - track 1373 outliers: [24207, 39572, 39574, 48338, 78104, 78108] should remove [24207, 39572, 39574]\n",
    "#y 17/7 hits - track 1644 outliers: [21896, 29838, 36816, 89394, 89395, 111614, 95550] should remove: [89393, 89395]\n",
    "# 19/7 hits - track 1698 outliers: [80388, 73708, 42159, 73717, 86837, 86838, 41657] should remove: [86838, 86387, 80388, 73708, 42159]\n",
    "# 14/5 hits - track 1902 outliers: [23136, 37256, 116299, 30509, 116560] should remove: [116561]\n",
    "#x 15/9 hits - track 1963 outliers: [30432, 37195, 43502, 23025, 75475, 111219, 111222, 37180, 30367] should remove: [111222]\n",
    "#y 13/5 hits - track 2390 outliers: [76962, 84260, 84264, 45130, 45486] should remove: [45128, 45130, 45486, 76962, 84260, 84264, 91275]\n",
    "# --> actually, really good! this was a very complicated case.\n",
    "#x 20/11 hits - track 2445 outliers: [70818, 65315, 69259, 67852, 67858, 69236, 105080, 70776, 66589, 69246, 69247] should remove: [67852, 69246, 69236, 69259, 70776, 71464]\n",
    "# 16/6 hits - track 2522 outliers: [44685, 44686, 83245, 76114, 21534, 76126] should remove:  [21534, 21672, 44209, 44686, 76114, 83245, 90582]\n",
    "# 15/5 hits - track 3417 outliers: [28458, 81457, 81464, 80985, 35775] should remove: [81457, 80985]\n",
    "#x 15/8 hits - track 3427 outliers: [101154, 39335, 98506, 120427, 24496, 119921, 31986, 84925] should remove: []\n",
    "#y 15/6 hits - track 4225 outliers: [119013, 118535, 31339, 45003, 76853, 38453] should remove: [118535]\n",
    "#y 13/5 hits - track 4318 outliers: [84482, 45732, 77481, 77200, 77203] should remove: [77200, 77481, 84482, 84707, 119929]\n",
    "# 18/5 hits - track 5047 outliers: [44323, 44330, 44331, 30926, 37591] should remove: [22142, 30926, 37591, 44319, 44331, 44323, 44330]\n",
    "# 13/5 hits - track 5101 outliers: [117442, 22595, 22833, 111923, 95867] should remove   [117444]\n",
    "#xx 13/6 hits - track 5420 outliers: [86917, 92518, 86918, 92523, 108468, 108469] should remove [86918]\n",
    "#y 23/10 hits - track 5422 outliers: [94144, 81504, 88577, 94531, 74492, 88165, 43052, 110419, 21500, 110718] should remove [21650, 21512, 43052, 43060, 74492, 81504, 88165, 110718, 88577, 94531]\n",
    "# 17/5 hits - track 5826 outliers: [43176, 89005, 94905, 88988, 88991] should remove [43170, 43176, 89005, 88991, 88988, 94905, 94878, 111049]\n",
    "# 14/5 hits - track 6244 outliers: [34257, 34706, 34708, 19508, 34264] should remove [79229, 72675, 34708, 34264, 34706, 34697, 19718, 19508]\n",
    "#y 12/5 hits - track 6409 outliers: [35969, 73987, 28709, 73990, 42508] should remove [109342, 109340, 93542, 73987]\n",
    "# 15/6 hits - track 6740 outliers: [38990, 32046, 39374, 23633, 23666, 38997] should remove [23633, 23666, 32046, 38997, 39374]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_outlier_removal(labels, hits, truth):\n",
    "    tracks = np.unique(labels)\n",
    "    hits['z_abs'] = hits.z.abs()\n",
    "    hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "    hits['a0'] = np.arctan2(hits.y,hits.x)\n",
    "    count_removed = 0\n",
    "    count_not_removed = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 3:\n",
    "            outlier_slope_ix = remove_track_outliers_slope(track, labels, hits)\n",
    "            if len(outlier_slope_ix) > 0:\n",
    "                tdf = truth.loc[track_hits]\n",
    "                truth_count = coll.Counter(tdf.particle_id.values).most_common(1)\n",
    "                truth_particle_id = truth_count[0][0]\n",
    "                for out_ix in outlier_slope_ix:\n",
    "                    if tdf.loc[out_ix].particle_id != truth_particle_id:\n",
    "                        labels[out_ix] = 0\n",
    "                        count_removed = count_removed + 1\n",
    "                    else:\n",
    "                        count_not_removed = count_not_removed + 1\n",
    "\n",
    "    print('safe count_removed: ' + str(count_removed))\n",
    "    print('safe count_not_removed: ' + str(count_not_removed))\n",
    "    return labels\n",
    "\n",
    "def perfect_outlier_removal(labels, hits, truth):\n",
    "    tracks = np.unique(labels)\n",
    "    count_removed = 0\n",
    "    count_not_removed = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 3:\n",
    "            tdf = truth.loc[track_hits]\n",
    "            truth_count = coll.Counter(tdf.particle_id.values).most_common(1)\n",
    "            truth_particle_id = truth_count[0][0]\n",
    "            for hit_ix in track_hits:\n",
    "                if tdf.loc[hit_ix].particle_id != truth_particle_id:\n",
    "                    labels[hit_ix] = 0\n",
    "                    count_removed = count_removed + 1\n",
    "                else:\n",
    "                    count_not_removed = count_not_removed + 1\n",
    "\n",
    "    print('perfect count_removed: ' + str(count_removed))\n",
    "    print('perfect count_not_removed: ' + str(count_not_removed))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_h2 = np.copy(labels_helix)\n",
    "labels_h2 = remove_outliers(labels_h2, hits)\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "labels_h3 = safe_outlier_removal(labels_h3, hits, truth)\n",
    "labels_h4 = np.copy(labels_helix)\n",
    "labels_h4 = perfect_outlier_removal(labels_h4, hits, truth)\n",
    "\n",
    "one_submission = create_one_event_submission(1000, hits, labels_helix)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Initial score for event %d: %.8f\" % (1000, score))\n",
    "\n",
    "one_submission = create_one_event_submission(1000, hits, labels_h2)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Score after normal outlier removal for event %d: %.8f\" % (1000, score))\n",
    "\n",
    "one_submission = create_one_event_submission(1000, hits, labels_h3)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Score after safe outlier removal score for event %d: %.8f\" % (1000, score))\n",
    "\n",
    "one_submission = create_one_event_submission(1000, hits, labels_h4)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Score after perfect outlier removal for event %d: %.8f\" % (1000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_hh2 = np.copy(labels_hh)\n",
    "labels_hh2 = remove_outliers(labels_hh2, hits)\n",
    "labels_hh3 = np.copy(labels_hh)\n",
    "labels_hh3 = safe_outlier_removal(labels_hh3, hits, truth)\n",
    "labels_hh4 = np.copy(labels_hh)\n",
    "labels_hh4 = perfect_outlier_removal(labels_hh4, hits, truth)\n",
    "\n",
    "one_submission = create_one_event_submission(1003, hits, labels_hh)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Initial score for event %d: %.8f\" % (1003, score))\n",
    "\n",
    "one_submission = create_one_event_submission(1003, hits, labels_hh2)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Score after normal outlier removal for event %d: %.8f\" % (1003, score))\n",
    "\n",
    "one_submission = create_one_event_submission(1003, hits, labels_hh3)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Score after safe outlier removal score for event %d: %.8f\" % (1003, score))\n",
    "\n",
    "one_submission = create_one_event_submission(1003, hits, labels_hh4)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Score after perfect outlier removal for event %d: %.8f\" % (1003, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_h1 = np.copy(labels_helix1)\n",
    "labels_h2 = np.copy(labels_helix2)\n",
    "labels_h3 = np.copy(labels_helix3)\n",
    "labels_merged = merge.heuristic_merge_tracks(labels_h1, labels_h2)\n",
    "labels_merged = merge.heuristic_merge_tracks(labels_merged, labels_h3)\n",
    "one_submission = create_one_event_submission(1003, hits, labels_merged)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merge score no removal for event %d: %.8f\" % (1003, score))\n",
    "\n",
    "labels_hh1 = np.copy(labels_helix1)\n",
    "labels_hh2 = np.copy(labels_helix2)\n",
    "labels_hh3 = np.copy(labels_helix3)\n",
    "labels_hh1 = remove_outliers(labels_hh1, hits)\n",
    "labels_hh2 = remove_outliers(labels_hh2, hits)\n",
    "labels_hh3 = remove_outliers(labels_hh3, hits)\n",
    "labels_merged2 = merge.heuristic_merge_tracks(labels_hh1, labels_hh2)\n",
    "labels_merged = merge.heuristic_merge_tracks(labels_merged2, labels_hh3)\n",
    "one_submission = create_one_event_submission(1003, hits, labels_merged2)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merge score outlier removal for event %d: %.8f\" % (1003, score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_merge1 = merge.heuristic_merge_tracks(labels_helix, labels_hh)\n",
    "one_submission = create_one_event_submission(1003, hits, labels_merge1)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merge score no removal for event %d: %.8f\" % (1003, score))\n",
    "\n",
    "labels_merge2 = merge.heuristic_merge_tracks(labels_h2, labels_hh2)\n",
    "one_submission = create_one_event_submission(1003, hits, labels_merge2)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merge score normal outlier removal for event %d: %.8f\" % (1003, score))\n",
    "\n",
    "labels_merge3 = merge.heuristic_merge_tracks(labels_h3, labels_hh3)\n",
    "one_submission = create_one_event_submission(1003, hits, labels_merge3)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merge score safe removal for event %d: %.8f\" % (1003, score))\n",
    "\n",
    "labels_merge4 = merge.heuristic_merge_tracks(labels_h4, labels_hh4)\n",
    "one_submission = create_one_event_submission(1003, hits, labels_merge4)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merge score perfect removal for event %d: %.8f\" % (1003, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_q1 = np.copy(labels_helix)\n",
    "labels_q1 = remove_outliers(labels_q1, hits)\n",
    "\n",
    "labels_q2 = np.copy(labels_hh)\n",
    "labels_q2 = remove_outliers(labels_q2, hits)\n",
    "\n",
    "labels_mergeq1 = merge.heuristic_merge_tracks(labels_q1, labels_q2)\n",
    "one_submission = create_one_event_submission(1003, hits, labels_mergeq1)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merge score normal no-slope removal for event %d: %.8f\" % (1003, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO pi, -pi discontinuity \n",
    "def remove_track_outliers_slope(track, labels, hits, debug=False):\n",
    "    hhh_ix = np.where(labels == track)\n",
    "    hhh_h = hits.loc[hhh_ix].sort_values('z')\n",
    "    \n",
    "    slopes_backward = []\n",
    "    slopes_forward = []\n",
    "\n",
    "\n",
    "    num_hits = len(hhh_h)\n",
    "    if debug: print('backward:')\n",
    "    for i in np.arange(num_hits-1,0,-1):\n",
    "        a0 =  hhh_h.a0.values[i]\n",
    "        a1 =  hhh_h.a0.values[i-1]\n",
    "        r0 =  hhh_h.r.values[i]\n",
    "        r1 =  hhh_h.r.values[i-1]\n",
    "        if r0 == r1:\n",
    "            r0 = r0 + 1e-8\n",
    "        slope = (a0-a1)/(r0-r1) \n",
    "        slopes_backward.append(slope)\n",
    "        if debug: print(hhh_h.hit_id.values[i], slope, a0)\n",
    "        if i == 1:\n",
    "            a0 = hhh_h.a0.values[0]\n",
    "            a1 = hhh_h.a0.values[num_hits-1]\n",
    "            r0 =  hhh_h.r.values[0]\n",
    "            r1 =  hhh_h.r.values[num_hits-1]\n",
    "            if r0 == r1:\n",
    "                r0 = r0 + 1e-8\n",
    "            slope = (a0-a1)/(r0-r1)\n",
    "            slopes_backward.append(slope)\n",
    "            if debug: print(hhh_h.hit_id.values[0], slope, a1)\n",
    "\n",
    "    if debug: print('forward:')\n",
    "    for i in np.arange(0,num_hits-1,1):\n",
    "        a0 =  hhh_h.a0.values[i]\n",
    "        a1 =  hhh_h.a0.values[i+1]\n",
    "        r0 =  hhh_h.r.values[i]\n",
    "        r1 =  hhh_h.r.values[i+1]\n",
    "        if r0 == r1:\n",
    "            r1 = r1 + 1e-8\n",
    "        slope = (a1-a0)/(r1-r0) \n",
    "        slopes_forward.append(slope)\n",
    "        if debug: print(hhh_h.hit_id.values[i], slope, a0)\n",
    "\n",
    "        if i == num_hits-2:\n",
    "            a0 = hhh_h.a0.values[0]\n",
    "            a1 = hhh_h.a0.values[num_hits-1]\n",
    "            r0 =  hhh_h.r.values[0]\n",
    "            r1 =  hhh_h.r.values[num_hits-1]\n",
    "            if r0 == r1:\n",
    "                r1 = r1 + 1e-8\n",
    "            slope = (a1-a0)/(r1-r0) \n",
    "            slopes_forward.append(slope)\n",
    "            if debug: print(hhh_h.hit_id.values[num_hits-1], slope, a0)\n",
    "\n",
    "    slopes_backward = np.asarray(slopes_backward)\n",
    "    slopes_backward = np.reshape(slopes_backward, (-1, 1))\n",
    "    slopes_forward = np.asarray(slopes_forward)\n",
    "    slopes_forward = np.reshape(slopes_forward, (-1, 1))\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    X_back = ss.fit_transform(slopes_backward)\n",
    "    X_for = ss.fit_transform(slopes_forward)\n",
    "\n",
    "    cl = DBSCAN(eps=0.0033, min_samples=1)\n",
    "    outlier_labels_backward = cl.fit_predict(X_back)\n",
    "    outlier_labels_forward = cl.fit_predict(X_for)\n",
    "\n",
    "    if debug: print(outlier_labels_backward)\n",
    "    if debug: print(outlier_labels_forward)\n",
    "\n",
    "    track_counts = coll.Counter(outlier_labels_backward).most_common(1)\n",
    "    most_common_id = track_counts[0][0]\n",
    "    most_common_count = track_counts[0][1]\n",
    "\n",
    "    outlier_indices_backward = []\n",
    "    if most_common_count > 1 and len(np.unique(outlier_labels_forward)) < num_hits/2:\n",
    "        for i in np.arange(num_hits-1,-1,-1):\n",
    "            if outlier_labels_backward[i] != most_common_id:\n",
    "                if debug: print(hhh_h.index.values[num_hits-1-i])\n",
    "                outlier_indices_backward.append(hhh_h.index.values[num_hits-1-i])\n",
    "\n",
    "    track_counts = coll.Counter(outlier_labels_forward).most_common(1)\n",
    "    most_common_id = track_counts[0][0]\n",
    "    most_common_count = track_counts[0][1]\n",
    "\n",
    "\n",
    "    outlier_indices_forward = []\n",
    "    if most_common_count > 1 and len(np.unique(outlier_labels_forward)) < num_hits/2:\n",
    "        for i in np.arange(0,num_hits-1,1):\n",
    "            if outlier_labels_forward[i] != most_common_id:\n",
    "                if debug: print(hhh_h.index.values[i])\n",
    "                outlier_indices_forward.append(hhh_h.index.values[i])\n",
    "\n",
    "\n",
    "    outlier_candidates = list(set(outlier_indices_backward).intersection(outlier_indices_forward))\n",
    "    final_outliers = []\n",
    "\n",
    "    if debug: print('before removal:' + str(outlier_candidates))\n",
    "\n",
    "    for i in range(len(outlier_candidates)):\n",
    "        candidate = hhh_h.loc[outlier_candidates[i]]\n",
    "        found = False\n",
    "        for index, row in hhh_h.iterrows():\n",
    "            if np.absolute(candidate.z-row.z) == 0.5 and candidate.volume_id == row.volume_id \\\n",
    "            and candidate.layer_id == row.layer_id and candidate.module_id != row.module_id:\n",
    "                # true hits\n",
    "                if debug: print('true hit' + str(outlier_candidates[i]))\n",
    "                found = True\n",
    "        if found is False:\n",
    "            final_outliers.append(outlier_candidates[i])\n",
    "\n",
    "    if debug: print('new loutliers:' + str(final_outliers))\n",
    "\n",
    "    return final_outliers\n",
    "\n",
    "\n",
    "def remove_outliers_slope(labels, hits):\n",
    "    tracks = np.unique(labels)\n",
    "    count_rem_volume = 0\n",
    "    count_rem_dimension = 0\n",
    "    count_duplicatez = 0\n",
    "    count_singletons = 0\n",
    "    for track in tracks:\n",
    "        if track == 0:\n",
    "            continue\n",
    "        track_hits = np.where(labels == track)[0]\n",
    "        if len(track_hits) > 4:\n",
    "            outliers = remove_track_outliers_slope(track, labels, hits)\n",
    "            if len(outliers) > 0:\n",
    "                do_something = True\n",
    "                # filter out the outliers\n",
    "            \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_h3 = np.copy(labels_helix)\n",
    "outliers = remove_track_outliers_slope(14, labels_h3, hits)\n",
    "print('Slope outliers: ' + str(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Removing helix outliers')\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "labels_h3 = remove_outliers_slope(labels_h3, hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with the track number to find a track that contains multiple particles\n",
    "hhh_ix = np.where(labels_helix == 14)\n",
    "print(hhh_ix)\n",
    "hhh_t = truth.loc[hhh_ix]\n",
    "hhh_t = hhh_t.sort_values('tz')\n",
    "print(hhh_t)\n",
    "\n",
    "# Now add r/a0 as columns to the hits, and see if there's a way to detect\n",
    "# which hits belong to the track, and which should be considered outliers.\n",
    "hits['r'] = np.sqrt(hits.x**2+hits.y**2)\n",
    "hits['a0'] = np.arctan2(hits.y,hits.x)\n",
    "hhh_h = hits.loc[hhh_ix]\n",
    "\n",
    "\n",
    "#hhh_h = hhh_h[(hhh_h.hit_id != 15459) & (hhh_h.hit_id != 16190) & (hhh_h.hit_id != 16155) ]\n",
    "\n",
    "hhh_h = hhh_h.sort_values('z')\n",
    "print(hhh_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Removing cone outliers')\n",
    "labels_c3 = np.copy(labels_cone)\n",
    "labels_c3 = remove_outliers_slope(labels_c3, hits)\n",
    "\n",
    "print('Removing helix outliers')\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "labels_h3 = remove_outliers_slope(labels_h3, hits)\n",
    "\n",
    "print('Merging tracks')\n",
    "tmp_labels = merge.heuristic_merge_tracks(labels_h3, labels_c3)\n",
    "one_submission = create_one_event_submission(1000, hits, tmp_labels)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Merged score for event %d: %.8f\" % (1000, score))\n",
    "\n",
    "#Merged score for event 1000: 0.58701361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_c2 = np.copy(labels_cone)\n",
    "labels_c2 = remove_outliers(labels_c2, hits)\n",
    "\n",
    "labels_simple_merge = merge.merge_tracks(labels_helix, labels_c2)\n",
    "one_submission = create_one_event_submission(1000, hits, labels_simple_merge)\n",
    "#one_submission = create_one_event_submission(1000, hits, labels_c2)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Outlier removal score for cone event %d: %.8f\" % (1000, score))\n",
    "# Orig Score no removals: 0.26152679\n",
    "# With outlier removal:\n",
    "# volume: 1231\n",
    "# bad dim: 6733\n",
    "# score: 0.18735180\n",
    "# With volume removal: 3520, score: 0.23268937\n",
    "# With volume removal, treat -ve+8--> +ve: 2841, score: 0.23394451\n",
    "# With ignore of 8, otherwise full checks: 1056, score: 0.25499520\n",
    "# With ignore of 8, light checks: 1, score: 0.26152679\n",
    "# Bad volume removal: 1, Bad dims: 222, score: 0.25587569, merged: 0.48291748\n",
    "# Bad volume removal: 1, Bad dims: 220, duplicatez: 1675, score: 0.25657711, merged: 0.48742414\n",
    "# HELIX: Bad vol: 15, score: 0.51204521\n",
    "# HELIX: Bad vol: 15, duplicatez: 399, score: 0.51065321\n",
    "# HELIX: Bad vol: 15, duplicatez: 399, bad dim: 286, score: 0.49635631\n",
    "# HELIX: Bad vol: 15, duplicatez: 399, bad dim: 1518, score: 0.50364853\n",
    "# CONE: Bad vol: 0, duplicatez: 1675, bad dim: 1239, score: 0.25871803, merged: 0.48569633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_labels = merge.heuristic_merge_tracks(labels_helix, labels_c2)\n",
    "one_submission = create_one_event_submission(1000, hits, tmp_labels)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"cone score for event %d: %.8f\" % (1000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_h2 = np.copy(labels_helix)\n",
    "labels_h2 = remove_outliers(labels_h2, hits)#, aggressive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_to_remove = 157\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "xxx_ix = np.where(labels_h3 == track_to_remove)[0]\n",
    "print('xxx_ix: ' + str(xxx_ix))\n",
    "(labels_h3x, _, _, _) = remove_track_outliers(track_to_remove, labels_h3, hits, False)#, True)\n",
    "xxx_ixx = np.where(labels_h3x == track_to_remove)[0]\n",
    "print('xxx_ixx: ' + str(xxx_ixx))\n",
    "#(labels_h3, cx, cy, cz) = remove_track_outliers(track_to_remove, labels_c3, hits)\n",
    "#labels_c3x = np.copy(labels_c3)\n",
    "#one_submission = create_one_event_submission(1000, hits, labels_c3)\n",
    "#score = score_event(truth, one_submission)\n",
    "#print(\"Outlier removal score for cone event %d: %.8f\" % (1000, score))\n",
    "# rem 0,0, score 0.26152679\n",
    "#xxx_ix = np.where(labels_c3 == track_to_remove)[0]\n",
    "#print('xxx_ix: ' + str(xxx_ix))\n",
    "xxx_df = truth.loc[xxx_ix]\n",
    "xxx_df = xxx_df.sort_values('tz')\n",
    "xxx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_to_remove = 157\n",
    "labels_h3 = np.copy(labels_helix)\n",
    "xxx_ix = np.where(labels_h3 == track_to_remove)[0]\n",
    "print('xxx_ix: ' + str(xxx_ix))\n",
    "(labels_h3x, _, _, _) = remove_track_outliers(track_to_remove, labels_h3, hits, False)#, True)\n",
    "xxx_ixx = np.where(labels_h3x == track_to_remove)[0]\n",
    "print('xxx_ixx: ' + str(xxx_ixx))\n",
    "#(labels_h3, cx, cy, cz) = remove_track_outliers(track_to_remove, labels_c3, hits)\n",
    "#labels_c3x = np.copy(labels_c3)\n",
    "#one_submission = create_one_event_submission(1000, hits, labels_c3)\n",
    "#score = score_event(truth, one_submission)\n",
    "#print(\"Outlier removal score for cone event %d: %.8f\" % (1000, score))\n",
    "# rem 0,0, score 0.26152679\n",
    "#xxx_ix = np.where(labels_c3 == track_to_remove)[0]\n",
    "#print('xxx_ix: ' + str(xxx_ix))\n",
    "xxx_df = hits.loc[xxx_ix]\n",
    "xxx_df = xxx_df.sort_values('z')\n",
    "xxx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr2_df = truth.loc[truth['particle_id'] == 306250134780379136]\n",
    "tr2_df = tr2_df.sort_values('tz')\n",
    "tr2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr2_df = truth.loc[truth['particle_id'] == 49542619558051840]\n",
    "tr2_df = tr2_df.sort_values('tz')\n",
    "tr2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helidx = np.where(labels_helix==7095)[0]\n",
    "print(helidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_submission = create_one_event_submission(1000, hits, labels_cone)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Outlier removal score for cone event %d: %.8f\" % (1000, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_c2 = np.copy(labels_cone)\n",
    "#trks2 = np.unique(labels_c2)\n",
    "#count_rem_volume = 0\n",
    "#count_rem_dimension = 0\n",
    "#for trk2 in trks2:\n",
    "#    if trk2 == 0:\n",
    "#        continue\n",
    "#    trk2_hits = np.where(labels_c2 == trk2)[0]\n",
    "#    if len(trk2_hits) > 3:\n",
    "#        (labels_c2, c1, c2) = remove_track_outliers(trk2, labels_c2, hits)\n",
    "#        count_rem_volume = count_rem_volume + c1\n",
    "#        count_rem_dimension = count_rem_dimension + c2\n",
    "\n",
    "#print('Total removed due to bad volumes: ' + str(count_rem_volume))\n",
    "#print('Total removed due to bad dimensions: ' + str(count_rem_dimension))\n",
    "\n",
    "track_to_remove = 63542\n",
    "labels_c3 = np.copy(labels_cone)\n",
    "xxx_ix = np.where(labels_c3 == track_to_remove)[0]\n",
    "print('xxx_ix: ' + str(xxx_ix))\n",
    "(labels_c3, cx, cy, cz) = remove_track_outliers(track_to_remove, labels_c3, hits)\n",
    "labels_c3x = np.copy(labels_c3)\n",
    "one_submission = create_one_event_submission(1000, hits, labels_c3)\n",
    "score = score_event(truth, one_submission)\n",
    "print(\"Outlier removal score for cone event %d: %.8f\" % (1000, score))\n",
    "# rem 0,0, score 0.26152679\n",
    "xxx_ix = np.where(labels_c3 == track_to_remove)[0]\n",
    "print('xxx_ix: ' + str(xxx_ix))\n",
    "xxx_df = hits.loc[xxx_ix]\n",
    "xxx_df = xxx_df.sort_values('z')\n",
    "# Bad indexes: 59855, 61697\n",
    "print(xxx_df)\n",
    "xxx_ix = np.where(labels_cone == 63542)\n",
    "xxx_df = hits.loc[xxx_ix]\n",
    "xxx_df = xxx_df.sort_values('z')\n",
    "# indexes in question: ???\n",
    "xxx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_helix = merge.renumber_labels(labels_helix)\n",
    "max_track = np.amax(labels_helix)\n",
    "labels_cone[labels_cone != 0] = labels_cone[labels_cone != 0] + max_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsx = np.copy(labels_cone)\n",
    "trackx = 63949\n",
    "outx = find_dimension_outlier(trackx, labelsx, hits, 'y')\n",
    "print('outx: ' + str(outx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx_ix = np.where(labels_cone == 63542)\n",
    "xxx_df = hits.loc[xxx_ix]\n",
    "xxx_df = xxx_df.sort_values('z')\n",
    "# indexes in question: ???\n",
    "xxx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = truth.loc[xxx_ix]\n",
    "tr_df = tr_df.sort_values('tz')\n",
    "# BAD: 59855, 61697\n",
    "# sorted z, see steps for 'y', ensure they are consistent (good for 59855, ok for 61697)\n",
    "# --> if most in same direction except one/two outliers, and outlier magnitude is large....\n",
    "all_y1 = tr_df.ty.values\n",
    "all_y = np.diff(all_y1)\n",
    "print('mean: ' + str(all_y.mean()))\n",
    "print('max: ' + str(all_y.max()))\n",
    "print('min: ' + str(all_y.min()))\n",
    "#[49193, 59886, 61710]\n",
    "tr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr2_df = truth.loc[truth['particle_id'] == 166642325903114240]\n",
    "tr2_df = tr2_df.sort_values('tz')\n",
    "tr2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track 150 outlier dimension y: [100706]\n",
    "trk large -: 160, large1: -1.592, large2: -61.533\n",
    "track 160 outlier dimension y: [41261]\n",
    "track 169 outlier dimension y: [17867]\n",
    "track 181 outlier dimension y: [103556]\n",
    "track 188 outlier dimension y: [661]\n",
    "track 194 outlier dimension y: [40343]\n",
    "trk large -: 208, large1: -1.2258, large2: -30.6159\n",
    "track 208 outlier dimension y: [23693]\n",
    "trk large -: 223, large1: -1.3697, large2: -36.1324\n",
    "track 223 outlier dimension y: [29081]\n",
    "trk large +: 263, large1: 38.0123, large2: 1.7837\n",
    "track 263 outlier dimension y: [19765]\n",
    "trk large +: 281, large1: 102.727, large2: 3.78752\n",
    "track 281 outlier dimension y: [100657]\n",
    "track 286 outlier dimension y: [62478]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhh_ix = np.where(labels_helix == 5549)\n",
    "hhh_df = truth.loc[hhh_ix]\n",
    "hhh_df = hhh_df.sort_values('tz')\n",
    "# indexes in question: ???\n",
    "hhh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = truth.loc[hhh_ix]\n",
    "tr_df = tr_df.sort_values('tz')\n",
    "# BAD: 59855, 61697\n",
    "# sorted z, see steps for 'y', ensure they are consistent (good for 59855, ok for 61697)\n",
    "# --> if most in same direction except one/two outliers, and outlier magnitude is large....\n",
    "all_y1 = tr_df.ty.values\n",
    "all_y = np.diff(all_y1)\n",
    "print('mean: ' + str(all_y.mean()))\n",
    "print('max: ' + str(all_y.max()))\n",
    "print('min: ' + str(all_y.min()))\n",
    "#[48366, 48517, 51460, 54360, 54471, 57091, 57133]\n",
    "#[6596, 6539, 4232, 4180]\n",
    "tr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_x = np.array([\n",
    "    [\n",
    "        [-12.166300, -156.817993, 962.000],\n",
    "        [20.349701, -242.162003, 1498.500],\n",
    "        [48.221401, -284.333008, 1795.500],\n",
    "        [48.529099, -284.743988, 1798.500],\n",
    "        [122.458000, -404.139008, 2554.500]\n",
    "    ],\n",
    "    [\n",
    "        [-7.17245, -44.973099, -1098.0],\n",
    "        [-6.67366, -39.278999, -962.5],\n",
    "        [-6.65474, -39.091202, -958.0],\n",
    "        [-6.01885, -33.464001, -822.5],\n",
    "        [-5.99555, -33.277901, -818.0]\n",
    "    ],\n",
    "    [\n",
    "        [-6.829120, 30.917900, 40.626202],\n",
    "        [-28.525200, 169.742004, 212.766998],\n",
    "        [-34.125198, 258.694000, 325.647003],\n",
    "        [-31.138201, 361.954987, 453.306000],\n",
    "        [-12.355800, 503.367004, 629.344971]\n",
    "    ]\n",
    "])\n",
    "\n",
    "track_y = np.array([\n",
    "    [1, 1, 1, 1, 0],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [0, 0, 1, 1, 1]\n",
    "])\n",
    "print(track_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = np.unique(labels_h3)\n",
    "track_x = np.zeros((len(tracks), 10, 3))\n",
    "track_y = np.zeros((len(tracks), 10, 1))\n",
    "max_xval = np.amax(hits.x.values)\n",
    "print(max_xval)\n",
    "max_yval = np.amax(hits.y.values)\n",
    "print(max_yval)\n",
    "max_zval = np.amax(hits.z.values)\n",
    "print(max_zval)\n",
    "for idx, track in enumerate(tracks):\n",
    "    if track == 0:\n",
    "        continue\n",
    "    hit_ix = np.where(labels_h3 == track)[0]\n",
    "    df2 = hits.loc[hit_ix]\n",
    "    df2 = df2.sort_values('z_abs')\n",
    "    hit_ix2 = df2.index.values # remember new indexes after sorting\n",
    "    #print(df2)\n",
    "    xs = df2.x.values\n",
    "    ys = df2.y.values\n",
    "    zs = df2.z.values\n",
    "\n",
    "    # From this track, figure out the most common particle ID from the truth.\n",
    "    # Any hits from our track that belong to that particle will be set to '1'\n",
    "    # (correctly predicted hit), otherwise '0' (outlier).\n",
    "    tf2 = truth.loc[hit_ix]\n",
    "    counters = coll.Counter\n",
    "    track_counts = coll.Counter(tf2.particle_id.values).most_common(len(hit_ix2))\n",
    "    track_particle_id = track_counts[0][0]\n",
    "\n",
    "    for i in range(len(hit_ix2)):\n",
    "        if i < 10:\n",
    "            track_x[idx][i][0] = xs[i] / max_xval\n",
    "            track_x[idx][i][1] = ys[i] / max_yval\n",
    "            track_x[idx][i][2] = zs[i] / max_zval\n",
    "            if (truth.loc[hit_ix2[i]].particle_id == track_particle_id):\n",
    "                track_y[idx][i][0] = 3\n",
    "            else:\n",
    "                track_y[idx][i][0] = 30\n",
    "            #track_y[idx][i][0] = (truth.loc[hit_ix2[i]].particle_id == track_particle_id)\n",
    "            \n",
    "    #if idx < 10:\n",
    "    #    ignore_it = True\n",
    "    #elif idx < 20:\n",
    "    #    print(tf2)\n",
    "    #    print(df2)\n",
    "    #    print(track_x[idx])\n",
    "    #    print(track_y[idx])\n",
    "    #else:\n",
    "    #    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as L\n",
    "import keras.models as M\n",
    "\n",
    "# The inputs to the model.\n",
    "# We will create two data points, just for the example.\n",
    "data_x = np.array([\n",
    "    # Datapoint 1\n",
    "#    [\n",
    "#        # Input features at timestep 1\n",
    "#        [1, 2, 3],\n",
    "#        # Input features at timestep 2\n",
    "#        [4, 5, 6]\n",
    "#    ],\n",
    "    # Datapoint 2\n",
    "    [\n",
    "        # Features at timestep 1\n",
    "        [7, 8, 9],\n",
    "        # Features at timestep 2\n",
    "        [10, 11, 12]\n",
    "    ]\n",
    "])\n",
    "\n",
    "# The desired model outputs.\n",
    "# We will create two data points, just for the example.\n",
    "data_y = np.array([\n",
    "    # Datapoint 1\n",
    "    # Target features at timestep 2\n",
    "#    [105, 106, 107, 108, 109],\n",
    "    # Datapoint 2\n",
    "    # Target features at timestep 2\n",
    "    [205, 206, 207, 208, 209]\n",
    "])\n",
    "\n",
    "# Each input data point has 2 timesteps, each with 3 features.\n",
    "# So the input shape (excluding batch_size) is (2, 3), which\n",
    "# matches the shape of each data point in data_x above.\n",
    "model_input = L.Input(shape=(10, 3))\n",
    "\n",
    "# This RNN will return timesteps with 4 features each.\n",
    "# Because return_sequences=True, it will output 2 timesteps, each\n",
    "# with 4 features. So the output shape (excluding batch size) is\n",
    "# (2, 4), which matches the shape of each data point in data_y above.\n",
    "#model_output = L.LSTM(4, return_sequences=False)(model_input)\n",
    "#model_output = L.LSTM(100, return_sequences=True)(model_input)\n",
    "#model_output = L.Dense(100, activation='linear')(model_output)\n",
    "#model_output = L.Dense(4, activation='linear')(model_output)\n",
    "#model_output = L.LSTM(4, return_sequences=False)(model_output)\n",
    "model_output = L.Bidirectional(L.LSTM(10, return_sequences=True, stateful=False))(model_input)\n",
    "#model_output = L.LSTM(30, return_sequences=True, stateful=False)(model_output)\n",
    "model_output = L.Bidirectional(L.LSTM(10, return_sequences=True, stateful=False))(model_output)\n",
    "model_output = L.TimeDistributed(L.Dense(1, activation='linear'))(model_output)\n",
    "#model_output = L.Dense(100, activation='linear')(model_output)\n",
    "#model_output = L.Dense(30, activation='linear')(model_output)\n",
    "# Create the model.\n",
    "model = M.Model(inputs=model_input, outputs=model_output)\n",
    "\n",
    "# You need to pick appropriate loss/optimizers for your problem.\n",
    "# I'm just using these to make the example compile.\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train\n",
    "#model.fit(data_x, data_y)\n",
    "#model.fit(track_x, track_y)\n",
    "\n",
    "# batch_size=3\n",
    "# num_steps=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for e in range(100):\n",
    "#    #for i in range(track_x.shape[0]):\n",
    "#    #    tx = np.expand_dims(track_x[i], axis=0)\n",
    "#    #    ty = np.expand_dims(track_y[i], axis=0)\n",
    "model.fit(track_x, track_y, batch_size=1, epochs=5, verbose=1)\n",
    "#for i in range(track_x.shape[0]):\n",
    "#        tx = np.expand_dims(track_x[i], axis=0)\n",
    "\n",
    "for i in range(track_x.shape[0]):\n",
    "    tx = np.expand_dims(track_x[i], axis=0)\n",
    "    yhat = model.predict(tx)\n",
    "    ty = track_y[i]\n",
    "    print(ty)\n",
    "    print(yhat)\n",
    "    if i > 10:\n",
    "        break\n",
    "# old - (3 LSTM Layers): Loss: 0.0810\n",
    "# old - (2 LSTM, 2 Dense (100, 30)): Loss ~ 0.08\n",
    "# New TimeDistributed Loss: 0.2072\n",
    "# New normalized TimeDistributed loss: 0.3196\n",
    "# New normalized Bidi-LSTM TimeDistributed loss: 0.3100, 0.1920, 0.1362, 0.1273, 0.1193\n",
    "#  -> same, but with 10-hit track input: 0.5413, 0.4646, 0.4242, 0.3943, 0.3550\n",
    "#  -> 3 for right hit, 10 for outlier, 0 for ignore: 5.1141, 5.0738, 5.0602, 5.0038, 4.6757,\n",
    "#                                                    4.1964, 3.9969, 3.8963, 3.8313, 3.7737\n",
    "#  -> 3 for right hit, 30 for outlier, 0 for ignore: 62.5914"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(track_x.shape[0]):\n",
    "    if i < 1000:\n",
    "        continue\n",
    "    tx = np.expand_dims(track_x[i], axis=0)\n",
    "    yhat = model.predict(tx)\n",
    "    ty = track_y[i]\n",
    "    print('Prediction: ' + str(i))\n",
    "    print(ty)\n",
    "    print(yhat)\n",
    "    if i > 1020:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hits.head())\n",
    "print(len(hits))\n",
    "\n",
    "print(particles.head())\n",
    "print(len(particles))\n",
    "\n",
    "print(cells.head())\n",
    "print(len(cells))\n",
    "\n",
    "print(truth.head())\n",
    "print(len(truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the XY plane\n",
    "g = sns.jointplot(hits.x, hits.y, size=12)\n",
    "\n",
    "#Clear the axes containing the scatter plot\n",
    "g.ax_joint.cla()\n",
    "# Set the current axis to the parent of ax\n",
    "plt.sca(g.ax_joint)\n",
    "\n",
    "volumes = hits.volume_id.unique()\n",
    "for volume in volumes:\n",
    "    v = hits[hits.volume_id == volume]\n",
    "    # scattering the hit coordinates with the particle size = 1\n",
    "    plt.scatter(v.x, v.y, s=1, label='volume {}'.format(volume))\n",
    "\n",
    "plt.xlabel('X (mm)')\n",
    "plt.ylabel('Y (mm)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the YZ plane\n",
    "g = sns.jointplot(hits.z, hits.y, s=1, size=12)\n",
    "g.ax_joint.cla()\n",
    "plt.sca(g.ax_joint)\n",
    "\n",
    "volumes = hits.volume_id.unique()\n",
    "for volume in volumes:\n",
    "    v = hits[hits.volume_id == volume]\n",
    "    plt.scatter(v.z, v.y, s=1, label='volume {}'.format(volume))\n",
    "\n",
    "plt.xlabel('Z (mm)')\n",
    "plt.ylabel('Y (mm)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From XYZ 3D perspective\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for volume in volumes:\n",
    "    v = hits[hits.volume_id == volume]\n",
    "    ax.scatter(v.z, v.x, v.y, s=1, label='volume {}'.format(volume), alpha=0.5)\n",
    "ax.set_title('SHit Locations')\n",
    "ax.set_xlabel('Z (millimeters)')\n",
    "ax.set_ylabel('X (millimeters)')\n",
    "ax.set_zlabel('Y (millimeters)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(particles.nhits.values, axlabel='Hits/Particle', bins=50)\n",
    "plt.title('Distribution of number of hits per particle for event 1000.')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(particles.groupby('q')['vx'].count(),\n",
    "        labels=['negative', 'positive'],\n",
    "        autopct='%.0f%%',\n",
    "        shadow=True,\n",
    "        radius=1)\n",
    "plt.title('Distribution of particle charges.')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the original particles of tracks, most particle collisions are generated from the origin\n",
    "\n",
    "g = sns.jointplot(particles.vz, particles.vy,  s=3, size=12)\n",
    "g.ax_joint.cla()\n",
    "plt.sca(g.ax_joint)\n",
    "\n",
    "n_hits = particles.nhits.unique()\n",
    "for n_hit in n_hits:\n",
    "    p = particles[particles.nhits == n_hit]\n",
    "    plt.scatter(p.vz, p.vy, s=1, label='Hits {}'.format(n_hit))\n",
    "\n",
    "plt.xlabel('Z (mm)')\n",
    "plt.ylabel('Y (mm)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "\n",
    "for charge in [-1, 1]:\n",
    "    q = particles[particles.q == charge]\n",
    "    ax.scatter(q.vz, q.vx, q.vy, s=1, label='Charge {}'.format(charge), alpha=0.5)\n",
    "ax.set_title('Sample of 1000 Particle initial location')\n",
    "ax.set_xlabel('Z (millimeters)')\n",
    "ax.set_ylabel('X (millimeters)')\n",
    "ax.set_zlabel('Y (millimeters)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIT_COUNT = 12\n",
    "particle1 = particles.loc[particles.nhits == HIT_COUNT].iloc[0]\n",
    "particle2 = particles.loc[particles.nhits == HIT_COUNT].iloc[1]\n",
    "particle3 = particles.loc[particles.nhits == HIT_COUNT].iloc[2]\n",
    "\n",
    "\n",
    "p_traj_surface1 = truth[truth.particle_id == particle1.particle_id][['tx', 'ty', 'tz']]\n",
    "p_traj_surface2 = truth[truth.particle_id == particle2.particle_id][['tx', 'ty', 'tz']]\n",
    "p_traj_surface3 = truth[truth.particle_id == particle3.particle_id][['tx', 'ty', 'tz']]\n",
    "\n",
    "\n",
    "\n",
    "p_traj1 = (p_traj_surface1\n",
    "          .append({'tx': particle1.vx, 'ty': particle1.vy, 'tz': particle1.vz}, ignore_index=True)\n",
    "          .sort_values(by='tz'))\n",
    "\n",
    "p_traj2 = (p_traj_surface2\n",
    "          .append({'tx': particle2.vx, 'ty': particle2.vy, 'tz': particle2.vz}, ignore_index=True)\n",
    "          .sort_values(by='tz'))\n",
    "\n",
    "p_traj3 = (p_traj_surface3\n",
    "          .append({'tx': particle3.vx, 'ty': particle3.vy, 'tz': particle3.vz}, ignore_index=True)\n",
    "          .sort_values(by='tz'))\n",
    "\n",
    "\n",
    "# Visualize XY projection to the Z-axis\n",
    "\n",
    "plt.plot(p_traj1.tz, p_traj1.ty, '-o', label='hits')\n",
    "plt.plot(p_traj2.tz, p_traj2.ty, '-o', label='hits')\n",
    "plt.plot(p_traj3.tz, p_traj3.ty, '-o', label='hits')\n",
    "plt.xlabel('Z (mm)')\n",
    "plt.ylabel('Y (mm)')\n",
    "plt.title('ZY projection to the X-axis')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot(\n",
    "    xs=p_traj1.tx,\n",
    "    ys=p_traj1.ty,\n",
    "    zs=p_traj1.tz, marker='o')\n",
    "ax.plot(\n",
    "    xs=p_traj2.tx,\n",
    "    ys=p_traj2.ty,\n",
    "    zs=p_traj2.tz, marker='o')\n",
    "ax.plot(\n",
    "    xs=p_traj3.tx,\n",
    "    ys=p_traj3.ty,\n",
    "    zs=p_traj3.tz, marker='o')\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xlabel('X (mm)')\n",
    "ax.set_ylabel('Y (mm)')\n",
    "ax.set_zlabel('Z  (mm) -- Detection layers')\n",
    "plt.title('Trajectories of two particles as they cross the detection surface ($Z$ axis).')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
